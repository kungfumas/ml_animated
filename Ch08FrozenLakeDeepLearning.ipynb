{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 8: Deep Learning Game Strategies\n",
    "\n",
    "In the next five chapters, you’ll learn how to apply deep neural network to the real life situations. In particular, you'll use deep learning to train intelligent game strategies in various games. You’ll learn from A to Z on how to train a game strategy in this chapter. \n",
    "\n",
    "In this chapter, you'll learn how to play games in the OpenAI Gym environment. Even though games in the OpenAI Gym environment are designed for reinforcement learning, you'll learn how to creatively design a deep learning game strategy and win the game. \n",
    "\n",
    "We'll use the Fozen Lake game in the OpenAI Gym environment as the example in this chapter. You’ll learn how to generate data for training purpose. Once you have a trained model, you’ll learn how to use the model to design a best move and play against the computer. Finally, you’ll test the effectiveness of the game.\n",
    "\n",
    "At the end of this chapter, you'll create an animation to show how the agent uses the trained model to make decisions on what's the best next move. We'll first draw a game board with the current position for the agent. The agent then hypothetically plays all four next moves, and let the trained DNN model predicts the probability of winning if the agent were to take that action. The agent will pick the action with the highest probability of winning. We'll highlight the best action in the animation in each stage of the game, like so:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_stages.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 8}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 8 in a subfolder /files/ch08. The code in the cell below will create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch08\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. Get Started with the OpenAI Gym Environment\n",
    "OpenAI Gym provides the needed working environment for many simple games. Many machine learning enthusiasts use games in OpenAI Gym to test their algorithms. In this section, you’ll learn how to install the libraries needed in order to access games that we’ll use in this book. After that, you’ll learn how to play a simple game, the Frozen Lake, in this environment. \n",
    "\n",
    "Before you get started, install the OpenAI Gym library as follows with your virtual environment activated:\n",
    "\n",
    "`pip install gym==0.15.7`\n",
    "\n",
    "Or you can simply use the shortcut and run the following line of code in a new cell in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7dcc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.15.7 in c:\\users\\hlliu2\\anaconda3\\envs\\animatedml\\lib\\site-packages (0.15.7)\n",
      "Requirement already satisfied: six in c:\\users\\hlliu2\\anaconda3\\envs\\animatedml\\lib\\site-packages (from gym==0.15.7) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\hlliu2\\anaconda3\\envs\\animatedml\\lib\\site-packages (from gym==0.15.7) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\hlliu2\\anaconda3\\envs\\animatedml\\lib\\site-packages (from gym==0.15.7) (1.22.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\hlliu2\\anaconda3\\envs\\animatedml\\lib\\site-packages (from gym==0.15.7) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hlliu2\\anaconda3\\envs\\animatedml\\lib\\site-packages (from gym==0.15.7) (1.8.1)\n",
      "Requirement already satisfied: future in c:\\users\\hlliu2\\anaconda3\\envs\\animatedml\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.15.7) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.15.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539484a",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Python package version control}}$<br>\n",
    "***\n",
    "There is a newer version of OpenAI gym, but the newer verion is not compatible with Baselines, a package that we need to train the Breakout game.\n",
    "\n",
    "In case you accidentally installed a newer version, run the following lines of code to correct it.\n",
    "\n",
    "`pip uninstall gym`\n",
    "\n",
    "`pip install gym==0.15.7`\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "### 1.1. Basic Elements of A Game Environment\n",
    "The OpenAI Gym game environments are designed mainly for testing reinforcement learning (RL) alorithms. But we'll use them to test deep learning game strategies first before testing RL algorithms in later chapters. \n",
    "\n",
    "Let’s first discuss a few basic concepts related to the game environment: \n",
    "* Environment: the world in which agent(s) live and interact with each other or with nature. More important, an environment is where the agent(s) can explore and learn the best strategies. Examples include the Frozen Lake game we’ll discuss in this chapter, or the popular Breakout Atariz game, or a real-world problem that we need to solve. You’ll learn to use pre-created environments from OpenAI, and you’ll also learn to create your own environment later in this book. \n",
    "* Agent: the player of the game. In most games, there is one player and the opponent is enbedded into the environment. But we'll also discuss two-player games such as Tic Tac Toe and Connect Four later in this book. \n",
    "* State: the current situation of the game. The current game board in the Connect Four game, for example, is the current state of the game. We'll explain more as we go along.\n",
    "* Action: what the player decides to do given the current game situation. In a Tic Tac Toe game, your action is to choose which cell to place your piece, for example. \n",
    "* Reward: the payoff from the game. You can assign a numerical value to each game outcome. For example, in a Tic Tac Toe game, we can assign a reward of 0 to all situations except when the game ends, at which point you can assign a reward of 1 if you win, -1 if you lose the game. \n",
    "\n",
    "These concepts will become clearer as we move along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "### 1.2. The Frozen Lake Game "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "If you go to the official Frozen Lake game website https://gym.openai.com/envs/FrozenLake-v0/, you'll see see an explanation similar to the following:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozenlake.png\" />\n",
    "\n",
    "The agent moves on the surface of a frozen lake, which is simplified as a 4 by 4 grid. The agent starts at the top left corner and tries to get to the lower right corner, without falling into one of the four holes on the frozen lake. The conditions of the lake surface are as follows:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_t.png\" />\n",
    "\n",
    "There are four holes on the lake surface, indicated by the four gray circles in the picture above.\n",
    "We’ll write a Python script to access the OpenAI Gym environment and learn its features.  \n",
    "\n",
    "\n",
    "The code in the cell below will get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    " \n",
    "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "The *make()* method creates the game environment for us. We set the *is_slippery* argument to *False* so that the game is deterministic. Meaning the game will always use the action that you choose. That is, we choose the nonslippery version of the game. The default setting is *is_slippery=True* and this means that you may not go to your intended location since the frozen lake surface is slippery. For example, when you choose to go left on the surface, you may end up going to the right. \n",
    "\n",
    "The *reset()* method starts the game and puts the player at the starting position. The *render()* method shows the current game state. Note that the Frozen Lake game doesn't provide a graphical interface. Instead, it uses letters to indicate the game environment. The 16 letters form a 4 by 4 grid, which reprents the lake surface. The letters have the following meanings:\n",
    "* S: starting position;\n",
    "* F: frozen, meaning it's safe to ski on;\n",
    "* H: hole, the player will fall into the hole and lose the game at this position;\n",
    "* G: goal, the player wins the game if reaching this point. \n",
    "The current position is highlighted in red. The above output shows that the player is at the top left corner of the lake. \n",
    "\n",
    "We can also print out all possible actions and states of the game as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052e7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space in the Frozenlake game is Discrete(4)\n",
      "The state space in the Frozenlake game is Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# Print out all possible actions in this game\n",
    "actions = env.action_space\n",
    "print(f\"The action space in the Frozenlake game is {actions}\")\n",
    "\n",
    "# Print out all possible states in this game\n",
    "states = env.observation_space\n",
    "print(f\"The state space in the Frozenlake game is {states}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The action space in the Frozenlake game has four values: 0, 1, 2, and 3, with the following meanings:\n",
    "* 0: Going left\n",
    "* 1: Going down\n",
    "* 2: Going right\n",
    "* 3: Going up\n",
    "The state space in the Frozenlake game has 16 values: 0, 1, 2, …, 15. The top left square is state 0, the top right is state 3, and the bottom right corner is 15, as shown in the following picture:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/lakesurface.png\" />\n",
    "\n",
    "You can play a complete game as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629df3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0 0.0 False {'prob': 1.0}\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0 0.0 False {'prob': 1.0}\n",
      "2\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1 0.0 False {'prob': 1.0}\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0 0.0 False {'prob': 1.0}\n",
      "3\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0 0.0 False {'prob': 1.0}\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4 0.0 False {'prob': 1.0}\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 1.0}\n",
      "3\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4 0.0 False {'prob': 1.0}\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4 0.0 False {'prob': 1.0}\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 1.0}\n",
      "3\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4 0.0 False {'prob': 1.0}\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 1.0}\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "12 0.0 True {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()   \n",
    "\n",
    "while True:\n",
    "    action = actions.sample()\n",
    "    print(action)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(new_state, reward, done, info)    \n",
    "    if done == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "The script above uses several methods in the game environment:\n",
    "* The sample() method randomly selects an action from the action space. That is, it will return one of the values among {0, 1, 2, 3}. \n",
    "* The step() method is where the agent is interacting with the environment, and it takes the agent’s action as input. The output are four values: the new state, the reward, a variable *done* indicating whether the game has ended. The *info* variable provides some information about the game. In this case, it provides the probability that the agent reaches the intended state. Since we are using the nonslippery version of the game, the probability is always 100%. \n",
    "* The render() method shows a diagram of the resulting state. \n",
    "\n",
    "The game loop is an infinite *while* loop. If the *done* variable returns a value *True*, the game ends, and we stop the infinite while loop.\n",
    "\n",
    "Note that since the actions are chosen randomly, when you run the script, you’ll most likely get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064889bb",
   "metadata": {},
   "source": [
    "### 1.3. Play the Frozen Lake Game Manually\n",
    "Next, you’ll learn how to manually interact with the Frozen Lake game, so that you have a better understanding of the game environment. This will prepare you to design winning strategies for the Frozen Lake game.\n",
    "\n",
    "The following lines of code show you how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac17214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "enter 0 for left, 1 for down\n",
      "2 for right, and 3 for up\n",
      "\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "how do you want to move?\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "how do you want to move?\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "how do you want to move?\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "how do you want to move?\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "how do you want to move?\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "how do you want to move?\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Congrats, you have made to the destination!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()   \n",
    "\n",
    "print('''\n",
    "enter 0 for left, 1 for down\n",
    "2 for right, and 3 for up\n",
    "''')\n",
    "env.reset()                    \n",
    "env.render()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        action = int(input('how do you want to move?\\n'))\n",
    "    except:\n",
    "        print('please enter 0, 1, 2, or 3')\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    if done==True:\n",
    "        if new_state==15:\n",
    "            print(\"Congrats, you have made to the destination!\")\n",
    "        else:\n",
    "            print(\"Game over. Better luck next time!\")\n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "I chose the following actions: 1, 1, 2, 1, 2, 2 (meaning down, down, right, down, right, right sequentially) and successfully reached the destination. This is one of the shortest paths that you can win the game.\n",
    "\n",
    "Now, the question is: can you train your computer to win the game by itself? \n",
    "\n",
    "The answer is yes, and you’ll learn how to do that by using the deep learning method via deep neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa3a085",
   "metadata": {},
   "source": [
    "## 2. Deep Learning Game Strategies: Generating Data\n",
    "\n",
    "In the next few sections, you’ll learn how to use deep neural networks to train intelligent game strategies.\n",
    "\n",
    "You’ll learn from A to Z on how to train a game strategy using the Frozen Lake game as an example. You'll apply the strategies to other games as well later in the book. \n",
    "\n",
    "First, you’ll learn how to generate simulated game data for training purpose. Once you have a trained model, you’ll learn how to use the model to design a best-move game strategy and play against the computer. Finally, you’ll test the effectiveness of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "### 2.2. Prepare Data for the Neural Network\n",
    "How to use neural network to train a game strategy in this case? Here is a summary of what we’ll do to train the game strategy:\n",
    "1. We’ll let the player randomly choose actions and complete a game and record the whole game history. The game history will contain all the intermediate states and actions from the very first move to the very last move.\n",
    "2. We then associate each state-action pair with a game outcome (win or lose). The state-action pair is similar to X (i.e., image pixels) in our image classification problem, and the outcome is similar to y (i.e., image labels such as horse, deer, airplane and so on) in the image classification problem.\n",
    "3. We’ll simulate a large number of games, say, 10000 of them. Use the histories of the games and the corresponding outcome as X and y pairs to feed into a Deep Neural Networks model. After training is done, we have a trained model.\n",
    "4. At each move of the game, we look at all possible next moves, and feed the hypothetical state-action pair into the pretained model. The model will tell you the probability of winning the game if that state-action pair were chosen.\n",
    "5. You select the move with the highest chance of winning based on the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2dd43",
   "metadata": {},
   "source": [
    "#### Simulate One Game\n",
    "\n",
    "First we’ll simulate one game and record the whole game history and the game outcome. The script below accomplishes that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea53d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2, 1, 0.0, False], [1, 1, 5, 0.0, True]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "# create lists to record game history and outcome \n",
    "history = []\n",
    "winlose = [0]\n",
    "# start the game\n",
    "state = env.reset()\n",
    "while True:\n",
    "    # randomly choose an action  \n",
    "    action = env.action_space.sample()\n",
    "    # make a move\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    # recording game hisotry\n",
    "    history.append([state, action, new_state, reward, done])\n",
    "    # prepare for the next round\n",
    "    state = new_state\n",
    "    # stop if the game is over\n",
    "    if done==True:\n",
    "        # if end up in state 15, change outcome to 1\n",
    "        if new_state==15:\n",
    "            winlose[0] = 1\n",
    "        break  \n",
    "    \n",
    "print(history) \n",
    "print(winlose)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb762289",
   "metadata": {},
   "source": [
    "In the game above, the player made two moves, and lost the game. So the winlose list has a value of 0 in it (meaning loss). If the player wins, then the value in the list is 1. The history list records all intermediate steps. In each step, we have values of current state, move, next state, reward, and whether the game is over. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a0fe6",
   "metadata": {},
   "source": [
    "#### Simulate Many Games\n",
    "Next, we'll simulate 10,000 games and record all the intermediate steps and outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79bce2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "# create lists to record all game histories and outcomes \n",
    "histories = []\n",
    "outcomes = []\n",
    "\n",
    "# Define one_game() function\n",
    "def one_game():\n",
    "    history=[]\n",
    "    outcome=0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        # randomly choose an action  \n",
    "        action = env.action_space.sample()\n",
    "        # make a move\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # recording game hisotry\n",
    "        history.append([state, action, new_state, reward, done])\n",
    "        # prepare for the next round\n",
    "        state = new_state\n",
    "        # stop if the game is over\n",
    "        if done==True:\n",
    "            # Once the game is over, manually add in the four end state\n",
    "            # and action combinations, to cover all hypothetical situations\n",
    "            history.append([state, 0, new_state, reward, done])\n",
    "            history.append([state, 1, new_state, reward, done])\n",
    "            history.append([state, 2, new_state, reward, done])\n",
    "            history.append([state, 3, new_state, reward, done])\n",
    "            # if end up in state 15, change outcome to 1\n",
    "            if new_state==15:\n",
    "                outcome = 1\n",
    "            break  \n",
    "\n",
    "    return history, outcome\n",
    "\n",
    "# Play 10,000 games\n",
    "for i in range(10000):\n",
    "    history, outcome = one_game()\n",
    "    # record history and outcome\n",
    "    histories.append(history)\n",
    "    outcomes.append(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39517c4f",
   "metadata": {},
   "source": [
    "Next, we'll save the simulated data on the computer for later use. We can use the ***pickle*** library to do that. The ***pickle*** library is in the Python Standard Library so it comes with Pyton installation on your computer. No installation is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543e2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the simulation data on your computer\n",
    "with open('frozen_games.pickle', 'wb') as fp:\n",
    "    pickle.dump((histories, outcomes), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b528c",
   "metadata": {},
   "source": [
    "You can load up the saved simulation data from your computer, and print out the first five games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e192171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 3, 0, 0.0, False],\n",
      "  [0, 3, 0, 0.0, False],\n",
      "  [0, 1, 4, 0.0, False],\n",
      "  [4, 1, 8, 0.0, False],\n",
      "  [8, 2, 9, 0.0, False],\n",
      "  [9, 1, 13, 0.0, False],\n",
      "  [13, 2, 14, 0.0, False],\n",
      "  [14, 1, 14, 0.0, False],\n",
      "  [14, 1, 14, 0.0, False],\n",
      "  [14, 3, 10, 0.0, False],\n",
      "  [10, 1, 14, 0.0, False],\n",
      "  [14, 1, 14, 0.0, False],\n",
      "  [14, 3, 10, 0.0, False],\n",
      "  [10, 1, 14, 0.0, False],\n",
      "  [14, 3, 10, 0.0, False],\n",
      "  [10, 2, 11, 0.0, True]],\n",
      " [[0, 1, 4, 0.0, False],\n",
      "  [4, 3, 0, 0.0, False],\n",
      "  [0, 2, 1, 0.0, False],\n",
      "  [1, 2, 2, 0.0, False],\n",
      "  [2, 3, 2, 0.0, False],\n",
      "  [2, 1, 6, 0.0, False],\n",
      "  [6, 3, 2, 0.0, False],\n",
      "  [2, 0, 1, 0.0, False],\n",
      "  [1, 2, 2, 0.0, False],\n",
      "  [2, 2, 3, 0.0, False],\n",
      "  [3, 2, 3, 0.0, False],\n",
      "  [3, 2, 3, 0.0, False],\n",
      "  [3, 1, 7, 0.0, True]],\n",
      " [[0, 3, 0, 0.0, False],\n",
      "  [0, 1, 4, 0.0, False],\n",
      "  [4, 1, 8, 0.0, False],\n",
      "  [8, 1, 12, 0.0, True]],\n",
      " [[0, 0, 0, 0.0, False],\n",
      "  [0, 3, 0, 0.0, False],\n",
      "  [0, 1, 4, 0.0, False],\n",
      "  [4, 3, 0, 0.0, False],\n",
      "  [0, 3, 0, 0.0, False],\n",
      "  [0, 1, 4, 0.0, False],\n",
      "  [4, 0, 4, 0.0, False],\n",
      "  [4, 0, 4, 0.0, False],\n",
      "  [4, 2, 5, 0.0, True]],\n",
      " [[0, 1, 4, 0.0, False], [4, 2, 5, 0.0, True]]]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# read the data and print out the first 10 games\n",
    "with open('frozen_games.pickle', 'rb') as fp:\n",
    "    histories, outcomes=pickle.load(fp)\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(histories[:5])    \n",
    "pprint(outcomes[:5])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd249d",
   "metadata": {},
   "source": [
    "Next we'll train the deep neural network using the simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Game Strategies: Train the Deep Neural Network\n",
    "\n",
    "We'll train the deep neural network so that it can learn from the simulated data. To do that, we'll first prepare the data so that we can feed them into the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7aaab",
   "metadata": {},
   "source": [
    "### 3.1. Preparing the Data\n",
    "\n",
    "In the next few sections, you’ll learn how to convert the game history and outcome data into a form that the computer understands before you feed them into the deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc40f0",
   "metadata": {},
   "source": [
    "#### Associate Game Outcome with Each Step\n",
    "We'll associate each state-action pair with the final game outcome so that the model can estimate the probability of winning for each state-action combination. We'll use the first game above as an example. In the first game, the outcome is 0, meaning the player lost the game. There are 16 steps in game 1, so we'll create 16 values of X and y, as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6879fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 0, 0.0, False]\n",
      "[0, 3, 0, 0.0, False]\n",
      "[0, 1, 4, 0.0, False]\n",
      "[4, 1, 8, 0.0, False]\n",
      "[8, 2, 9, 0.0, False]\n",
      "[9, 1, 13, 0.0, False]\n",
      "[13, 2, 14, 0.0, False]\n",
      "[14, 1, 14, 0.0, False]\n",
      "[14, 1, 14, 0.0, False]\n",
      "[14, 3, 10, 0.0, False]\n",
      "[10, 1, 14, 0.0, False]\n",
      "[14, 1, 14, 0.0, False]\n",
      "[14, 3, 10, 0.0, False]\n",
      "[10, 1, 14, 0.0, False]\n",
      "[14, 3, 10, 0.0, False]\n",
      "[10, 2, 11, 0.0, True]\n"
     ]
    }
   ],
   "source": [
    "game1_history = histories[0]  \n",
    "game1_outcome = outcomes[0]     \n",
    "# Print out each state-action pair\n",
    "for i in game1_history:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab6eaf",
   "metadata": {},
   "source": [
    "The output above shows that the player starts at state 0 (top left corner), and chooses action 3 (going up), ended up in the same position (state 0). In the second round, the player starts at state 0, and chooses action 3 again, ended up in state 0 again. In the third round, the player starts at state 0, and chooses action 1 (going down), ended up in state 4, and so on. So we'll record X and y for game 1 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aba3315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3],\n",
      " [0, 3],\n",
      " [0, 1],\n",
      " [4, 1],\n",
      " [8, 2],\n",
      " [9, 1],\n",
      " [13, 2],\n",
      " [14, 1],\n",
      " [14, 1],\n",
      " [14, 3],\n",
      " [10, 1],\n",
      " [14, 1],\n",
      " [14, 3],\n",
      " [10, 1],\n",
      " [14, 3],\n",
      " [10, 2]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Create empty X and y lists\n",
    "game1_X, game1_y = [], []    \n",
    "# Create X and y for each game step\n",
    "for i in game1_history:\n",
    "    game1_X.append([i[0], i[1]])\n",
    "    game1_y.append(game1_outcome)\n",
    "    \n",
    "# Print out X, y\n",
    "pprint(game1_X)\n",
    "pprint(game1_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06b89d",
   "metadata": {},
   "source": [
    "The above outcome reflects the 16 steps in game 1. X is a state-action pairk, while y is the ultimate game outcome. \n",
    "\n",
    "However, if we feed the data into a neural network, the algorithm will mistakenly think that state 14 is greater than 13. Action 3 is greater than action 2. To avoid such confusion, we need to use one-hot encoder to convert them into a vector of 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04cbcc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3\n",
      "0 3\n",
      "0 1\n",
      "4 1\n",
      "8 2\n",
      "9 1\n",
      "13 2\n",
      "14 1\n",
      "14 1\n",
      "14 3\n",
      "10 1\n",
      "14 1\n",
      "14 3\n",
      "10 1\n",
      "14 3\n",
      "10 2\n",
      "[array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1.]),\n",
      " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1.]),\n",
      " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 1., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# Define a onehot_encoder() function\n",
    "def onehot_encoder(value, length):\n",
    "    onehot=np.zeros((1,length))\n",
    "    onehot[0,value]=1\n",
    "    return onehot\n",
    "# Change both state value and action value into onehot_encoder\n",
    "game1onehot_X = []\n",
    "for s, a in game1_X:\n",
    "    print(s,a)\n",
    "\n",
    "    onehot_s = onehot_encoder(s, 16)\n",
    "    onehot_a = onehot_encoder(a, 4)\n",
    "    sa = np.concatenate([onehot_s, onehot_a], axis=1)\n",
    "    game1onehot_X.append(sa.reshape(-1,))\n",
    "    \n",
    "# Print out new X\n",
    "pprint(game1onehot_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cd720",
   "metadata": {},
   "source": [
    "That's the format we want. We now apply that to the whole dataset and save it in the local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93c1a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty X and y lists\n",
    "X, y = [], []    \n",
    "# Create X and y for each game step\n",
    "for gamei, yi in zip(histories, outcomes):\n",
    "    for step in gamei:\n",
    "        s, a = step[0], step[1]\n",
    "        onehot_s = onehot_encoder(s, 16)\n",
    "        onehot_a = onehot_encoder(a, 4)\n",
    "        sa = np.concatenate([onehot_s, onehot_a], axis=1)\n",
    "        X.append(sa.reshape(-1,))\n",
    "        y.append(yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478c43f",
   "metadata": {},
   "source": [
    "Next, we'll save the dataset on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb977705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the simulation data on your computer\n",
    "with open('gameXy.pickle', 'wb') as fp:\n",
    "    pickle.dump((X,y),fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d832861",
   "metadata": {},
   "source": [
    "You can load up the dataset again and print out the first five observation to make sure they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ecfdafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1.]),\n",
      " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 1.]),\n",
      " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0., 0.]),\n",
      " array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1., 0.])]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# save the simulation data on your computer\n",
    "with open('gameXy.pickle', 'wb') as fp:\n",
    "    pickle.dump((X,y),fp)\n",
    "# read the data and print out the first 5 games\n",
    "with open('gameXy.pickle', 'rb') as fp:\n",
    "    myX, myy=pickle.load(fp)\n",
    "from pprint import pprint\n",
    "pprint(myX[:5])\n",
    "pprint(myy[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd4262",
   "metadata": {},
   "source": [
    "To summarize, the code below generates and prepares the data for training in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "# create lists to record all game histories and outcomes \n",
    "histories = []\n",
    "outcomes = []\n",
    "\n",
    "# Define one_game() function\n",
    "def one_game():\n",
    "    history=[]\n",
    "    outcome=0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        # randomly choose an action  \n",
    "        action = env.action_space.sample()\n",
    "        # make a move\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # recording game hisotry\n",
    "        history.append([state, action, new_state, reward, done])\n",
    "        # prepare for the next round\n",
    "        state = new_state\n",
    "        # stop if the game is over\n",
    "        if done==True:\n",
    "            # Record game history\n",
    "            history.append([state, 0, new_state, reward, done])\n",
    "            history.append([state, 1, new_state, reward, done])\n",
    "            history.append([state, 2, new_state, reward, done])\n",
    "            history.append([state, 3, new_state, reward, done])\n",
    "            # if end up in state 15, change outcome to 1\n",
    "            if new_state==15:\n",
    "                outcome = 1\n",
    "            break  \n",
    "\n",
    "    return history, outcome\n",
    "\n",
    "# Play 10,000 games\n",
    "for i in range(10000):\n",
    "    history, outcome = one_game()\n",
    "    # record history and outcome\n",
    "    histories.append(history)\n",
    "    outcomes.append(outcome)\n",
    "\n",
    "import pickle\n",
    "# save the simulation data on your computer\n",
    "with open('frozen_games.pickle', 'wb') as fp:\n",
    "    pickle.dump((histories, outcomes), fp)\n",
    "    \n",
    "# read the data and print out the first 10 games\n",
    "with open('frozen_games.pickle', 'rb') as fp:\n",
    "    histories, outcomes=pickle.load(fp)\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(histories[:10])    \n",
    "pprint(outcomes[:10])  \n",
    "\n",
    "# Define a onehot_encoder() function\n",
    "def onehot_encoder(value, length):\n",
    "    onehot=np.zeros((1,length))\n",
    "    onehot[0,value]=1\n",
    "    return onehot\n",
    "# Create empty X and y lists\n",
    "X, y = [], []    \n",
    "# Create X and y for each game step\n",
    "for gamei, yi in zip(histories, outcomes):\n",
    "    for step in gamei:\n",
    "        s, a = step[0], step[1]\n",
    "        onehot_s = onehot_encoder(s, 16)\n",
    "        onehot_a = onehot_encoder(a, 4)\n",
    "        sa = np.concatenate([onehot_s, onehot_a], axis=1)\n",
    "        X.append(sa.reshape(-1,))\n",
    "        y.append(yi)\n",
    "        \n",
    "# save the simulation data on your computer\n",
    "with open('gameXy.pickle', 'wb') as fp:\n",
    "    pickle.dump((X,y),fp)\n",
    "# read the data and print out the first 5 games\n",
    "with open('gameXy.pickle', 'rb') as fp:\n",
    "    myX, myy=pickle.load(fp)\n",
    "from pprint import pprint\n",
    "pprint(myX[:5])\n",
    "pprint(myy[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d794163",
   "metadata": {},
   "source": [
    "### 3.2. Train the Deep Neural Network\n",
    "Now that the dataset is ready, we are ready to train our deep neural network. \n",
    "\n",
    "Here we are essentially performing a binary classification. We classify each state-action pair into win or lose. The output layer has one neuron with sigmoid activation. So we can think of the output as the probability of winning the game. \n",
    "\n",
    "There are four hidden layers in the model, with 128, 64, 32, and 16 neurons, respectively. But we do have a lot of freedom here. fewer layers with various numbers of neurons will generate similar results. \n",
    "\n",
    "Later, we'll use the trained model to play the Frozen Lake game. When playing, at each state, we'll ask the following question:\n",
    "1. If I choose action 0 (i.e., move left), what's the probability of winning the game? We'll combine the current state and action 0 and feed this state-action pair to the trained deep neural network and get a probability; let's call it p(win|s,a0).\n",
    "2. If I choose action 1 (i.e., move down), what's the probability of winning the game? We'll use the trained neural network and get p(win|s,a1).\n",
    "3. If I choose action 2 (i.e., move right), what's the probability of winning the game? We'll use the trained neural network and get p(win|s,a2).\n",
    "4. If I choose action 3 (i.e., move up), what's the probability of winning the game? We'll consult the trained neural network and get p(win|s,a3).\n",
    "We then compare p(win|s,a0), p(win|s,a1), p(win|s,a2), and p(win|s,a3), and pick the action that leads to the highest p(win|s,a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# load the data       \n",
    "with open('gameXy.pickle', 'rb') as fp:\n",
    "    X, y = pickle.load(fp)\n",
    "\n",
    "X = np.array(X).reshape((-1, 20))\n",
    "y = np.array(y).reshape((-1, 1))\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(20,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model.fit(X,y, epochs=50)\n",
    "\n",
    "model.save('files/ch08/trained_frozen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341cc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that the model is trained, we can use it to play the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6011a93",
   "metadata": {},
   "source": [
    "## 4. Play the Game with the Trained Model\n",
    "To play the game with the trained model, we'll look at the state at each move, and hypothetically take actions 0, 1, 2, and 3. Then we use the trained model to predict the probability of winning with each of the four state-action pairs. We'll pick the action that leads to the highest probability of winning. We repeat at each step until the game ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eafae1ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 245ms/step\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "9 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "14 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "15 1.0 True {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# import needed modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(20,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "reload = tf.keras.models.load_model(\"files/ch08/trained_frozen.h5\")\n",
    "\n",
    "# Define a onehot_encoder() function\n",
    "def onehot_encoder(value, length):\n",
    "    onehot=np.zeros((1,length))\n",
    "    onehot[0,value]=1\n",
    "    return onehot\n",
    "\n",
    "action0=onehot_encoder(0, 4)\n",
    "action1=onehot_encoder(1, 4)\n",
    "action2=onehot_encoder(2, 4)\n",
    "action3=onehot_encoder(3, 4)\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "state = env.reset()   \n",
    "\n",
    "# save the predictions in each step\n",
    "predictions = []\n",
    "while True:\n",
    "    # Convert state and action into onehots \n",
    "    state_arr = onehot_encoder(state, 16)\n",
    "    # Use the trained model to predict the prob of winning \n",
    "    sa0 = np.concatenate([state_arr, action0], axis=1)    \n",
    "    sa1 = np.concatenate([state_arr, action1], axis=1)  \n",
    "    sa2 = np.concatenate([state_arr, action2], axis=1)  \n",
    "    sa3 = np.concatenate([state_arr, action3], axis=1)\n",
    "    sa = np.concatenate([sa0, sa1, sa2, sa3], axis=0)\n",
    "    prediction = reload.predict(sa)\n",
    "    action = np.argmax(prediction)\n",
    "    predictions.append(prediction)\n",
    "    print(action)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(new_state, reward, done, info) \n",
    "    state = new_state\n",
    "    if done == True:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "The player wins the game with the shortest possible path. So the deep learning game strategy works!!!\n",
    "\n",
    "Next, we save the predictions in each stage of the game for later use. We'll use these probabilities later when we create animations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869d3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('files/ch08/frozen_predictions.p', 'wb') as fp:\n",
    "    pickle.dump(predictions,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e11b0",
   "metadata": {},
   "source": [
    "## 5. Test the Efficacy of the Game Strategy\n",
    "Winning one game can be a coincicence. We need a scientic way of testing the efficacy. For that, we'll let the trained model play the game 1000 times, and record how many times the model wins and how many times the model loses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(20,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "reload = tf.keras.models.load_model(\"files/ch08/trained_frozen.h5\")\n",
    "\n",
    "# Define a onehot_encoder() function\n",
    "def onehot_encoder(value, length):\n",
    "    onehot=np.zeros((1,length))\n",
    "    onehot[0,value]=1\n",
    "    return onehot\n",
    "\n",
    "action0=onehot_encoder(0, 4)\n",
    "action1=onehot_encoder(1, 4)\n",
    "action2=onehot_encoder(2, 4)\n",
    "action3=onehot_encoder(3, 4)\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "\n",
    "def test():\n",
    "    state = env.reset()\n",
    "    winlose=0\n",
    "    while True:\n",
    "        # Convert state and action into onehots \n",
    "        state_arr = onehot_encoder(state, 16)\n",
    "        # Use the trained model to predict the prob of winning \n",
    "        sa0 = np.concatenate([state_arr, action0], axis=1)    \n",
    "        sa1 = np.concatenate([state_arr, action1], axis=1)  \n",
    "        sa2 = np.concatenate([state_arr, action2], axis=1)  \n",
    "        sa3 = np.concatenate([state_arr, action3], axis=1)\n",
    "        sa = np.concatenate([sa0, sa1, sa2, sa3], axis=0)\n",
    "        action = np.argmax(reload.predict(sa))\n",
    "        #print(action)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #env.render()\n",
    "        #print(new_state, reward, done, info) \n",
    "        state = new_state\n",
    "        if done == True:\n",
    "            # change winlose to 1 if the last state is 15\n",
    "            if state==15:\n",
    "                winlose=1\n",
    "            break\n",
    "    return winlose\n",
    "\n",
    "winloses = []\n",
    "for i in range(1000):\n",
    "    winlose = test()\n",
    "    winloses.append(winlose)\n",
    "\n",
    "# Print out the number of winning games\n",
    "print(\"the number of winning games is\", winloses.count(1))\n",
    "\n",
    "# Print out the number of losing games\n",
    "print(\"the number of losing games is\", winloses.count(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48598568",
   "metadata": {},
   "source": [
    "The strategy has won all 1000 games. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48956fd",
   "metadata": {},
   "source": [
    "## 6. Animate the Decision Process\n",
    "We'll create an animation to show how the agent uses the trained model to make decisions on what's the best next move. \n",
    "\n",
    "We'll first draw a game board with the current position fo the agent. We'll then hypothetically play all four next moves, and let the trained DNN model tells us the probability of winning for each action. The agent will pick the action with the highest probability of winning. We'll highlight the best action in the animation in each stage of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc24e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load up date during the training process\n",
    "preds = pickle.load(open('files/ch08/frozen_predictions.p', 'rb'))\n",
    "\n",
    "states = [(-6,2),(-6,0),(-6,-2),(-5,-2),(-5,-4),(-4,-4),(-3,-4)]\n",
    "actions = [1,1,2,1,2,2]\n",
    "\n",
    "grid = [[\"S\", \"F\", \"F\", \"F\"],\n",
    "        [\"F\", \"H\", \"F\", \"H\"],\n",
    "        [\"F\", \"F\", \"F\", \"H\"],\n",
    "        [\"H\", \"F\", \"F\", \"G\"]]\n",
    "\n",
    "hs = [3.1,1.3,-1.4,-3.2]\n",
    "\n",
    "# Generate six pictures\n",
    "for stage in range(7):\n",
    "\n",
    "    fig = plt.figure(figsize=(14,10), dpi=100)\n",
    "    ax = fig.add_subplot(111) \n",
    "    ax.set_xlim(-7, 7)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    #plt.grid()\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # table grid\n",
    "    for x in range(-6,-1,1):\n",
    "        ax.plot([x,x],[-4,4],color='gray',linewidth=3)\n",
    "    for y in range(-4,5,2):\n",
    "        ax.plot([-6,-2],[y,y],color='gray',linewidth=3)\n",
    "    for row in range(4):\n",
    "        for col in range(4):\n",
    "            plt.text(col-5.8,2.6-2*row,grid[row][col],fontsize=60) \n",
    "\n",
    "    # highlight current state\n",
    "    ax.add_patch(Rectangle(states[stage], 1,2,facecolor='r',alpha=0.8))             \n",
    "    plt.savefig(f\"files/ch08/frozen_stage{stage}step1.png\")        \n",
    "            \n",
    "    if stage<=5:\n",
    "        # reload trained model\n",
    "        ps = preds[stage].reshape(4,)\n",
    "        # Draw connections between neurons\n",
    "        xys = [[(0,-3.2),(-2,0)],                   \n",
    "               [(0,-1.4),(-2,0)],\n",
    "               [(0,1.3),(-2,0)],\n",
    "               [(0,3.1),(-2,0)]]\n",
    "        for xy in xys:\n",
    "            ax.annotate(\"\",xy=xy[0],xytext=xy[1],\n",
    "            arrowprops=dict(arrowstyle = '->', color = 'g', linewidth = 2))  \n",
    "        # Put explanation texts on the graph\n",
    "        plt.text(-1.5,1.25,\"left\",fontsize=20,color='g',rotation=55) \n",
    "        plt.text(-1.25,0.25,\"down\",fontsize=20,color='g',rotation=35) \n",
    "        plt.text(-1.25,-0.85,\"right\",fontsize=20,color='g',rotation=-35)\n",
    "        plt.text(-1.5,-1.8,\"up\",fontsize=20,color='g',rotation=-55)     \n",
    "        # add rectangle to plot\n",
    "        for i in range(4):\n",
    "            ax.add_patch(Rectangle((0,-0.6+hs[i]), 2, 1.3,\n",
    "                         facecolor = 'b',alpha=0.1)) \n",
    "            plt.text(0.2,hs[i]-0.5,\"Deep\\nNeural\\nNetwork\",fontsize=20) \n",
    "            plt.text(2.6, hs[i]-0.15, f\"Prob(win)={ps[i]:.4f}\", fontsize=25, color=\"r\")  \n",
    "            ax.annotate(\"\",xy=(2.5,hs[i]),xytext=(2,hs[i]),\n",
    "            arrowprops=dict(arrowstyle = '->', color = 'g', linewidth = 2))   \n",
    "\n",
    "        plt.savefig(f\"files/ch08/frozen_stage{stage}step2.png\") \n",
    "        \n",
    "        # highlight the best action\n",
    "        ax.add_patch(Rectangle((2.5,hs[actions[stage]]-0.4), 4.25, 1,\n",
    "                     facecolor = 'b',alpha=0.5))     \n",
    "        plt.savefig(f\"files/ch08/frozen_stage{stage}step3.png\")     \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3467a2be",
   "metadata": {},
   "source": [
    "If you open the file frozen_stage0step1.png, you'll see the following:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_stage0step1.png\" />which is the starting position of the agent. The file frozen_stage0step2.png below:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_stage0step2.png\" /> shows the probabilities of winning the game for the four possible actions: left, down, right, and up. The file frozen_stage0step3.png below:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_stage0step3.png\" /> highlights action with the greatest probability of winning the game, which is down. That's the action the agent chooses in the first stage of the game.\n",
    "\n",
    "We repeat this process in every stage of the game until the game ends.\n",
    "\n",
    "We can combine the above pictures into an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec0f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio, PIL\n",
    "\n",
    "frames=[]\n",
    "for stage in range(6):\n",
    "    for step in range(3):\n",
    "        frame=PIL.Image.open(f\"files/ch08/frozen_stage{stage}step{step+1}.png\")   \n",
    "        frame=np.asarray(frame)\n",
    "        frames.append(np.array(frame))\n",
    "frame=PIL.Image.open(f\"files/ch08/frozen_stage6step1.png\")   \n",
    "frame=np.asarray(frame)\n",
    "# put three frames at the end to highlight success\n",
    "frames.append(np.array(frame))        \n",
    "frames.append(np.array(frame)) \n",
    "frames.append(np.array(frame)) \n",
    "\n",
    "imageio.mimsave('files/ch08/frozen_stages.gif', frames, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c728bb",
   "metadata": {},
   "source": [
    "The animation looks as follows:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_stages.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069fc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
