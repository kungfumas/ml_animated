{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 16: Play Atari Pong with Policy Gradients\n",
    "\n",
    "Policy gradients are a policy-based reinforcement learning (RL) mechanism, as opposed to the value based RL that we talked about so far. But it’s a very powerful tool, and we’ll use it to play Atari games such as Breakout or Pong in this book. \n",
    "\n",
    "The idea behind policy gradients is different from Q learning: it directly tells the model how to adjust the weights to reach the optimum. If the prediction is smaller than the desired outcome, the model will adjust the weights so that the prediction will increase. Similarly, if the prediction is greater than the desired outcome, the model will adjust the weights so that the prediction will decrease. Further, the magnitude of the adjustment is directly proportional to the rewards: the greater the reward, the greater the adjustment. \n",
    "\n",
    "We’ll use the Atari Pong game as an example on how to do that in this chapter. The code I use in this chapter is largely based on Andrej's code below \n",
    "https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "\n",
    "At the end of this chapter, you'll create an animation to illustrate how deep Q learning works. Specifically, at each time step, you'll put the graph of the cart pole on the left. You'll draw on the right the current state of the game: cart position, cart velocity, pole angle, and pole velocity. You'll feed the information to the trained deep Q network to get the Q-values of moving the cart left and moving the cart right, respectively. The move with higher Q-value is then highlighted in red on the graph, and that's the action taken by the agent. You'll repeat this in each of the 200 time steps, like so:  \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/cartpole_DeepQs.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 16}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 16 in a subfolder /files/ch16. The code in the cell below will create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch16\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "## 1. Get Started with Atari Games\n",
    "\n",
    "In this section, you'll start to play the Atari Pong game in the OpenAI Gym environment and understand its features.\n",
    "\n",
    "First, we need to install Atari in the OpenAI Gym environment. Activate the virtual environment ***animatedML***, then enter the following command in the Anaconda prompt or a terminal (MAC or Linux):\n",
    "\n",
    "`conda install -c conda-forge atari_py`\n",
    "\n",
    "After that, you need to install ROMS on your computer. Go to the link below \n",
    "\n",
    "http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html\n",
    "\n",
    "and download the file Roms.rar to your computer. Extract the two folders, ROMS and HC Roms, and place them in a folder on your computer. For example, I place them in C:\\temp on my computer. \n",
    "\n",
    "After that, run the following command in the Anaconda prompt or a terminal (MAC or Linux) with the virtual environment ***animatedML*** activated:\n",
    "\n",
    "`python -m atari_py.import_roms <path to folder>`\n",
    "\n",
    "For example, since I saved the files in C:\\temp on my computer, I ran:\n",
    "\n",
    "`python -m atari_py.import_roms C:\\temp` \n",
    "\n",
    "to install ROMS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7aaab",
   "metadata": {},
   "source": [
    "### 1.1. The Pong Game\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adfa3a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Pong-v0\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26b04e",
   "metadata": {},
   "source": [
    "You should see a Pong game frame in a separate window. \n",
    "\n",
    "You can check the action space and observation space of the game as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ee5e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space in the Pong game is Discrete(6)\n",
      "The observation space in the Pong game is Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print out all possible actions in this game\n",
    "actions = env.action_space\n",
    "print(f\"The action space in the Pong game is {actions}\")\n",
    "\n",
    "# Print out the observation space in this game\n",
    "obs_space = env.observation_space\n",
    "print(f\"The observation space in the Pong game is {obs_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a68bf5",
   "metadata": {},
   "source": [
    "There are six possible actions the gaent can take. But for all practical purpose, we only need to decide whether the paddle should go up (action 2) or down (action 3). So we can treat this as a binary classification problem. \n",
    "\n",
    "Each observation is a color picture 210 pixels tall and 160 pixels wide. The following cell displays an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1401e695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "env.reset()\n",
    "# run 200 steps so that the pong appears in the picture\n",
    "for _ in range(200):\n",
    "    action = np.random.choice([2,3])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "plt.imshow(obs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a304c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dc9ad6c",
   "metadata": {},
   "source": [
    "### 1.2. Process the Game Frames\n",
    "The input size of the game frame is too large, we'll process the image to reduce the size while retaining vital information to train the model to win the game.\n",
    "\n",
    "Specifically, we'll perform cropping, downsizing, and differencing before we feed the data into the model to train the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526aec3",
   "metadata": {},
   "source": [
    "#### Cropping\n",
    "We'll remove the top and the bottom of the game frame to reduce input size as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cc83d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPgUlEQVR4nO3dfZBV9X3H8ffHRfABFRCCFDC7JtQOyVhlGItJ6nRCkyAxbjJmHJxMNQkzTKckarVjIP6R/JOZ2LSxcaY1Y6ItZqzGGp0wGWOlVOukU4hAUB4UWVFkmQWMD/iEPPntH+dHe4Fdce+5554rv89rZuee87vn3vO95+5+9pxzH76KCMwsXyfUXYCZ1cshYJY5h4BZ5hwCZplzCJhlziFglrnKQkDSHEmbJPVJWlTVesysHFXxPgFJXcCzwGeAfuAJ4MqI2NjylZlZKVXtCVwI9EXElojYB9wL9Fa0LjMrYURF9zsZ2NYw3w/8yVALjx55Qow72acnzKq07fWDv4+ICUeOVxUCxyRpAbAAYOxJJ3DDJ06vqxSzLFz38KtbBxuv6t/vdmBqw/yUNPZ/IuL2iJgZETNHj1RFZZjZsVQVAk8A0yT1SBoJzAOWVrQuMyuhksOBiDgg6RvAvwNdwJ0RsaGKdZlZOZWdE4iIh4CHqrp/M2sNn5I3y5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHNNh4CkqZIelbRR0gZJ16bxcZKWSdqcLse2rlwza7UyewIHgBsiYjowC1goaTqwCFgeEdOA5WnezDpU0yEQEQMRsSZNvwE8TdF5qBdYkhZbAnyxZI1mVqGWnBOQ1A1cAKwEJkbEQLpqBzCxFesws2qUDgFJo4FfANdFxOuN10XR8njQtseSFkhaJWnVm/ta3xnZzN6fUiEg6USKALg7Ih5IwzslTUrXTwJ2DXZbtyEz6wxlXh0QcAfwdET8sOGqpcDVafpq4JfNl2dmVSvTgeiTwF8A6yStTWPfBr4P3CdpPrAVuKJUhWZWqaZDICJ+Awy1Hz+72fs1s/byOwbNMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMlfmo8Qd7eRRozi3p+eo8We3vsDbe96poSKzznTchkBXVxcTxh39befP9/fXUI1Z5/LhgFnmHAJmmXMImGXOIWCWuVb0HeiS9DtJv0rzPZJWSuqT9HNJI8uXaWZVacWewLUULcgOuRm4JSI+CrwKzG/BOsysImWbj0wBPg/8NM0L+DRwf1rEvQit45x2VjfnXX4N511+DeO6P1Z3ObUr+z6BfwBuBE5L82cCr0XEgTTfT9Gk9CiSFgALAMae5FMT1j6nn9XDeV/6JgBvvTzAKy9sqLmiepXpQHQpsCsiVjdze7chM+sMZTsQXSZpLnAScDrwI2CMpBFpb2AKsL18mWZWlab3BCJicURMiYhuYB7wnxHxFeBR4MtpMfciNOtwVRyMfwu4XlIfxTmCOypYh1nT3j24n71vvsbeN1/j3f376i6ndi35AFFEPAY8lqa3ABe24n7NqjCw/r+5f+FFAMS7B2uupn7H7acIzYYUQRw8cOzlMuHX5swy5xAwy5xDwCxzDgGzzDkEzDJ3XL86EBF1l2DW8Y7bEHhrzx5+s3rNUeN79++voRqzznXchkBE8M4+vxvM7Fh8TsAscw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHNlm4+MkXS/pGckPS3pIknjJC2TtDldjm1VsWbWemX3BH4EPBwRfwT8MUU7skXA8oiYBixP82bWoco0HzkDuJj0bcIRsS8iXgN6KdqPgduQmXW8MnsCPcBLwD+nrsQ/lXQqMDEiBtIyO4CJZYs0s+qUCYERwAzgtoi4AHiLI3b9o/hA/6Af6pe0QNIqSave3OfP/ZvVpUwI9AP9EbEyzd9PEQo7JU0CSJe7BruxexGadYYybch2ANsknZuGZgMbgaUU7cfAbcjMOl7ZLxX5JnC3pJHAFuBrFMFyn6T5wFbgipLrMLMKlQqBiFgLzBzkqtll7tfM2sfvGDTLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Asc2XbkP21pA2S1ku6R9JJknokrZTUJ+nn6fsHzaxDlelANBm4BpgZER8HuoB5wM3ALRHxUeBVYH4rCjWzapQ9HBgBnCxpBHAKMAB8mqIHAbgNmVnHK9N3YDvwd8CLFH/8u4HVwGsRcSAt1g9MLlukmVWnzOHAWIrmoz3AHwCnAnOGcXu3ITPrAGUOB/4ceD4iXoqI/cADwCeBMenwAGAKsH2wG7sNmVlnKBMCLwKzJJ0iSfx/G7JHgS+nZdyGzKzDlTknsJLiBOAaYF26r9uBbwHXS+oDzgTuaEGdZlaRsm3IvgN854jhLcCFZe7XzNrH7xg0y5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHPHDAFJd0raJWl9w9g4ScskbU6XY9O4JN2aWpA9JWlGlcWbWXnvZ0/gXzi6n8AiYHlETAOWp3mAS4Bp6WcBcFtryjSzqhwzBCLiceCVI4Z7KVqMweGtxnqBu6KwgqIHwaQW1WpmFWj2nMDEiBhI0zuAiWl6MrCtYTm3ITPrcKVPDEZEAMPuI+Y2ZGadodkQ2HloNz9d7krj24GpDcu5DZlZh2s2BJZStBiDw1uNLQWuSq8SzAJ2Nxw2mFkHOmYHIkn3AH8GjJfUT9Fx6PvAfZLmA1uBK9LiDwFzgT7gbeBrFdRsZi10zBCIiCuHuGr2IMsGsLBsUWbWPn7HoFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeYcAmaZa7YN2Q8kPZNajT0oaUzDdYtTG7JNkj5XUd1m1iLNtiFbBnw8Is4DngUWA0iaDswDPpZu80+SulpWrZm1XFNtyCLikYg4kGZXUPQXgKIN2b0RsTcinqf41uELW1ivmbVYK84JfB34dZp2GzKzD5hSISDpJuAAcHcTt3UbMrMOcMy+A0OR9FXgUmB26jcAw2xDBtwOcPYZI5wCZjVpak9A0hzgRuCyiHi74aqlwDxJoyT1ANOA35Yv08yq0mwbssXAKGCZJIAVEfGXEbFB0n3ARorDhIURcbCq4s2svGbbkN3xHst/D/hemaLMrH38jkGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMtf0OwbNrJyDJ3axZ/xpR42f/NLrdB14t211OATMavL2hNPZNO8TR41PX/I4p7z8Rtvq8OGAWeYcAmaZcwiYZc4hYJY5h4BZ5hwCZplzCJhlziFgljmHgFnmHAJmmWuqDVnDdTdICknj07wk3ZrakD0laUYVRZtZ6zTbhgxJU4HPAi82DF9C8Q3D04AFwG3lSzSzKjXVhiy5heJrxxt7BvQCd0VhBTBG0qSWVGpmlWi270AvsD0injziKrchM/uAGfZHiSWdAnyb4lCgaZIWUBwyMPYkn580q0szf30fAXqAJyW9QNFqbI2ksxhmG7KImBkRM0ePVBNlmFkrDDsEImJdRHwoIrojoptil39GROygaEN2VXqVYBawOyIGWluymbXS+3mJ8B7gf4BzJfVLmv8eiz8EbAH6gJ8Af9WSKs2sMs22IWu8vrthOoCF5csys3bxGTmzzPmLRs1qMvKtd/jQmuePGh/xzr621uEQMKvJqN17OPuxjXWX4cMBs9w5BMwy5xAwy5xDwCxzDgGzzDkEzDLnEDDLnEPALHMOAbPMOQTMMucQMMucQ8Ascw4Bs8w5BMwy5xAwy5xDwCxzDgGzzHXENwuNHH0G3Rd9vu4yzI5vD/9s0GEVXxBcr/PPPz+WL19edxlmx7Xx48evjoiZR453RAhIegl4C/h93bUA43EdjVzH4T7IdXw4IiYcOdgRIQAgadVgKeU6XIfrqLYOnxg0y5xDwCxznRQCt9ddQOI6Duc6Dnfc1dEx5wTMrB6dtCdgZjWoPQQkzZG0SVKfpEVtXO9USY9K2ihpg6Rr0/h3JW2XtDb9zG1DLS9IWpfWtyqNjZO0TNLmdDm24hrObXjMayW9Lum6dmwPSXdK2iVpfcPYoI9fhVvT78tTkmZUXMcPJD2T1vWgpDFpvFvSnobt8uOK6xjyeZC0OG2PTZI+N+wVRkRtP0AX8BxwDjASeBKY3qZ1TwJmpOnTgGeB6cB3gb9p83Z4ARh/xNjfAovS9CLg5jY/LzuAD7djewAXAzOA9cd6/MBc4NeAgFnAyorr+CwwIk3f3FBHd+Nybdgegz4P6Xf2SWAU0JP+nrqGs7669wQuBPoiYktE7APuBXrbseKIGIiINWn6DeBpYHI71v0+9QJL0vQS4IttXPds4LmI2NqOlUXE48ArRwwP9fh7gbuisAIYI2lSVXVExCMRcSDNrgCmtGJdw63jPfQC90bE3oh4Huij+Lt63+oOgcnAtob5fmr4Q5TUDVwArExD30i7f3dWvRueBPCIpNWSFqSxiRExkKZ3ABPbUMch84B7GubbvT1g6Mdf5+/M1yn2Qg7pkfQ7Sf8l6U/bsP7BnofS26PuEKidpNHAL4DrIuJ14DbgI8D5wADw920o41MRMQO4BFgo6eLGK6PY72vLyziSRgKXAf+WhurYHodp5+MfiqSbgAPA3WloADg7Ii4Argf+VdLpFZZQ2fNQdwhsB6Y2zE9JY20h6USKALg7Ih4AiIidEXEwIt4FfsIwd62aERHb0+Uu4MG0zp2HdnPT5a6q60guAdZExM5UU9u3RzLU42/774ykrwKXAl9JgUTa/X45Ta+mOBb/w6pqeI/nofT2qDsEngCmSepJ/4HmAUvbsWJJAu4Ano6IHzaMNx5ffglYf+RtW1zHqZJOOzRNcSJqPcV2uDotdjXwyyrraHAlDYcC7d4eDYZ6/EuBq9KrBLOA3Q2HDS0naQ5wI3BZRLzdMD5BUleaPgeYBmypsI6hnoelwDxJoyT1pDp+O6w7r+Ls5jDPhM6lODP/HHBTG9f7KYpdzKeAtelnLvAzYF0aXwpMqriOcyjO7j4JbDi0DYAzgeXAZuA/gHFt2CanAi8DZzSMVb49KEJnANhPcUw7f6jHT/GqwD+m35d1wMyK6+ijOOY+9Dvy47Ts5en5WgusAb5QcR1DPg/ATWl7bAIuGe76/I5Bs8zVfThgZjVzCJhlziFgljmHgFnmHAJmmXMImGXOIWCWOYeAWeb+F8fWEjCGakC7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_cropped = obs[35:195]\n",
    "plt.imshow(obs_cropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2290089",
   "metadata": {},
   "source": [
    "The size of the picture is now 160 by 160 by 3. We need to further reduce the size of the picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa53494",
   "metadata": {},
   "source": [
    "#### Downsizing\n",
    "We'll use every other row and every other column so that the input size is smaller. Further, we'll use just one of the three color channels to further reduce size, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae73b29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_downsized = obs_cropped[::2,::2,0]\n",
    "plt.imshow(obs_downsized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d77fc",
   "metadata": {},
   "source": [
    "The size of the processed picture is now 80 by 80, a small fraction of the origional size. However, the picture doesn't tell us the movement of the Pong ball. We can potentially use two consecutive pictures, but a better idea is to use the difference between two consecutuve frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fae7e7",
   "metadata": {},
   "source": [
    "#### Differencing\n",
    "We'll get the difference of the two consetive frames after processing. The script below shows how. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a0dfc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "action = np.random.choice([2,3])\n",
    "next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "next_obs_cropped = next_obs[35:195]\n",
    "next_obs_downsized = next_obs_cropped[::2,::2,0]\n",
    "\n",
    "dif = next_obs_downsized - obs_downsized\n",
    "plt.imshow(dif)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d421f4e",
   "metadata": {},
   "source": [
    "Now we are ready to feed the data into a model to train the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1e904",
   "metadata": {},
   "source": [
    "## 2. Train the Agent Using Policy Gradients\n",
    "\n",
    "We'll use policy gradients to train the agent. \n",
    "\n",
    "### 2.1. The Idea Behind Policy Gradients\n",
    "\n",
    "Policy gradients train the model by adjusting the weights, similar to the idea of gradient descent we discussed in Chapters 2 and 3. However, instead of adjusting the parameters by $$ -1 \\times Learning Rate \\times gradient.$$ we we discussed in Chapter 2, we'll adjust the parameters by $$ -1 \\times Learning Rate \\times gradient\\times reward$$\n",
    "\n",
    "The idea is that if the action leads to high rewards, we ajdust the parameters more, so as to reach the optimal outcome. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d8261",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Different Versions of the Pong Game\n",
    "\n",
    "For games in the OpenAI Gym environment, there are different versions:\n",
    "* v0: there is a 25% chance that the previous action will be repeated instead of the issued action;\n",
    "* v4: 0% probability of repeating the previous action; but skip 2-5 frames randomly;\n",
    "* Deterministic-v4: 0% probability of repeating the previous action; skip a fixed 4 frames. \n",
    "\n",
    "When we train the game, we don't use the aciton with the highest probability. Instead, we randomly choose the action proportional to the predicted probability from the model. This is similar to the Exploration versus Exploitation in Q training.  \n",
    "\n",
    "Further, the program is trained on the v0 version of the game. We'll discuss which version to test the trained model on later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edacff0",
   "metadata": {},
   "source": [
    "### 2.3. The Script to Train the Model\n",
    "The program is similar to the original code by Andrej Karpathy. I made some needed changes due to Python package updates, such as cPickle, r.size...\n",
    "\n",
    "For more detailed explanations, read through Andrej's post here:  http://karpathy.github.io/2016/05/31/rl/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc4ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('pg_pong.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "  model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, len(r))):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "round = 0\n",
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if round>0 else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "  round += 1\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  # grad that encourages the action that was taken to be taken \n",
    "  dlogps.append(y - aprob) \n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "\n",
    "  drs.append(reward) \n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "    round=0\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.items():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    if running_reward is None:\n",
    "        running_reward = reward_sum  \n",
    "    else:\n",
    "        running_reward = running_reward * 0.99 + reward_sum * 0.01\n",
    "    print('episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('pg_pong.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6a8d3",
   "metadata": {},
   "source": [
    "It takes one to two days to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d794163",
   "metadata": {},
   "source": [
    "## 3. Test the Trained Model\n",
    "We'll first test one game using the trained model. We then test it on 100 games and see what's the average score. We finally discuss testing on different versions of the Pong game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1a8fb",
   "metadata": {},
   "source": [
    "### 3.1. Test One Game\n",
    "The program below test the game for one episode. Each player can score between -21 to 21 points. 21 is the perfect score, and -21 is the worst possible score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677a0d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlliu2\\AppData\\Local\\Temp\\ipykernel_13944\\4234674740.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return I.astype(np.float).ravel()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is 21.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "model = pickle.load(open('files/ch16/pg_pong.p', 'rb'))\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) \n",
    "\n",
    "def prepro(I):\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "  env.render()\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if 0.5 < aprob else 3 \n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "  if done: \n",
    "    print(f\"score is {reward_sum}\")\n",
    "    break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea611d",
   "metadata": {},
   "source": [
    "The trained agent had a perfect score of 21."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b0580",
   "metadata": {},
   "source": [
    "### 3.2. Test on Multiple Games\n",
    "Next, we'll test on 20 games and see what is the average score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d41120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlliu2\\AppData\\Local\\Temp\\ipykernel_13944\\58405484.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return I.astype(np.float).ravel()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "score is 21.0\n",
      "the average score is 21.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "#model = pickle.load(open('pg_pong.p', 'rb'))\n",
    "model = pickle.load(open('files/ch16/pg_pong.p', 'rb'))\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) \n",
    "\n",
    "def prepro(I):\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "\n",
    "\n",
    "scores = []\n",
    "for i in range(20):  \n",
    "    observation = env.reset()\n",
    "    prev_x = None # used in computing the difference frame\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "      cur_x = prepro(observation)\n",
    "      x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "      prev_x = cur_x\n",
    "      #env.render()\n",
    "      aprob, h = policy_forward(x)\n",
    "      action = 2 if 0.5 < aprob else 3 \n",
    "      observation, reward, done, info = env.step(action)\n",
    "      reward_sum += reward\n",
    "      if done: \n",
    "        scores.append(reward_sum)\n",
    "        print(f\"score is {reward_sum}\")\n",
    "        break\n",
    "env.close()\n",
    "\n",
    "print(f\"the average score is {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba6d83",
   "metadata": {},
   "source": [
    "The trained agent plays perfectly in every game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc97f4",
   "metadata": {},
   "source": [
    "### 3.3. Test on Pong-v0\n",
    "We have tested the trained agent on PongDeterministic-v4. Next, we test what if we use Pong-v0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ada54e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlliu2\\AppData\\Local\\Temp\\ipykernel_13944\\1371521744.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return I.astype(np.float).ravel()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is -18.0\n",
      "score is -19.0\n",
      "score is -14.0\n",
      "score is -18.0\n",
      "score is -21.0\n",
      "score is -19.0\n",
      "score is -9.0\n",
      "score is -13.0\n",
      "score is -19.0\n",
      "score is -15.0\n",
      "score is -14.0\n",
      "score is -10.0\n",
      "score is -21.0\n",
      "score is -15.0\n",
      "score is -21.0\n",
      "score is -13.0\n",
      "score is -15.0\n",
      "score is -12.0\n",
      "score is -12.0\n",
      "score is -14.0\n",
      "the average score is -15.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "#model = pickle.load(open('pg_pong.p', 'rb'))\n",
    "model = pickle.load(open('files/ch16/pg_pong.p', 'rb'))\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) \n",
    "\n",
    "def prepro(I):\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "\n",
    "scores = []\n",
    "for i in range(20):  \n",
    "    observation = env.reset()\n",
    "    prev_x = None # used in computing the difference frame\n",
    "    reward_sum = 0\n",
    "    while True:\n",
    "      cur_x = prepro(observation)\n",
    "      x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "      prev_x = cur_x\n",
    "      #env.render()\n",
    "      aprob, h = policy_forward(x)\n",
    "      action = 2 if 0.5 < aprob else 3 \n",
    "      observation, reward, done, info = env.step(action)\n",
    "      reward_sum += reward\n",
    "      if done: \n",
    "        scores.append(reward_sum)\n",
    "        print(f\"score is {reward_sum}\")\n",
    "        break\n",
    "env.close()\n",
    "\n",
    "print(f\"the average score is {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b7b39",
   "metadata": {},
   "source": [
    "The agent performs poorly in this version of the game. This is due to that fact that the paddle repeats the previous move 25% of the time, instead of the move predicted by the trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afdb60c",
   "metadata": {},
   "source": [
    "## 4. Animate the Pong Game Before and After Training\n",
    "Next, we'll animate how the agent performs before and after training. We'll put the two games side by side so that you can compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2649d",
   "metadata": {},
   "source": [
    "### 4.1. Record A Full Game with the Trained Agent\n",
    "We'll first record a full game using the trained model.\n",
    "\n",
    "The script below accomplishes that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60f2169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlliu2\\AppData\\Local\\Temp\\ipykernel_13944\\3406337539.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return I.astype(np.float).ravel()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "#model = pickle.load(open('pg_pong.p', 'rb'))\n",
    "model = pickle.load(open('files\\ch16\\pg_pong.p', 'rb'))\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) \n",
    "\n",
    "def prepro(I):\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "reward_sum = 0\n",
    "while True:\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "  #env.render()\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if 0.5 < aprob else 3 \n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "  frames.append(env.render(mode='rgb_array'))\n",
    "  if done: \n",
    "    break\n",
    "env.close()\n",
    "\n",
    "import imageio\n",
    "imageio.mimsave('files\\ch16\\pong_trained.gif', frames, fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1fd1b",
   "metadata": {},
   "source": [
    "All the frames from the game are saved as numpy arrays in the list *frames*. We also converted the frames into an animation pong_trained.gif. If you open the file, you should see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe61167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/pong_trained.gif\"/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/pong_trained.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f1eb1",
   "metadata": {},
   "source": [
    "The trained agent is the player one the right with a green paddle. As you can see, the final score is 21 to 0. The trained agent won all 21 points, without letting the opponet score one single point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6f6e2",
   "metadata": {},
   "source": [
    "### 4.2. Record Games with Random Moves\n",
    "Next, we'll record game frames when the agent makes random moves. We'll use them later for comparison purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec0f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"PongDeterministic-v4\")\n",
    "\n",
    "random_frames = []\n",
    " \n",
    "observation = env.reset()\n",
    "for _ in range(len(frames)):\n",
    "  action = np.random.choice([2,3]) \n",
    "  observation, reward, done, info = env.step(action)\n",
    "  random_frames.append(env.render(mode='rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c69a94",
   "metadata": {},
   "source": [
    "Note that by using the command\n",
    "```python\n",
    "for _ in range(len(frames))\n",
    "```\n",
    "we ensure that we get the same number of frames here as in the game with the trained agent. Therefore, later when we combine the frames, we don't have to worry about matching the number of frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023e7e1",
   "metadata": {},
   "source": [
    "### 4.3. Combine the Animations\n",
    "We'll combine the two games and form one single animation, with the game with untrained agents on the left, and the game with the trained agent on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3a9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = []\n",
    "for i in range(len(frames)):\n",
    "    if i%2==0:\n",
    "        f = frames[i]\n",
    "        rf = random_frames[i]\n",
    "        middle = np.full(f.shape, 255).astype(\"uint8\")\n",
    "        frf = np.concatenate([rf, middle, f], axis=1)\n",
    "        fs.append(frf)\n",
    "imageio.mimsave('files\\ch16\\pong_compare.gif', fs, fps=24) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c728bb",
   "metadata": {},
   "source": [
    "The animation looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731b2efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/pong_compare.gif\" />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/pong_compare.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802cfdb",
   "metadata": {},
   "source": [
    "Again, the agent has the green paddle. The left side shows what happens if the agent chooses random moves. The right side shows a full game when the agent is trained with policy gradients. The trained agent plays perfectly, earning a score of 21-0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999148fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
