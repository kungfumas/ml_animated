{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 17: Play Breakout with Policy Gradients\n",
    "\n",
    "We extend the policy gradients approach you learned in Chapter 16 to another, more complicated Atari game, Breakout. \n",
    "\n",
    "Specifically, since the possible actions are four instead of just two, you need to change the problem from binary classification to multi-class classification. Further, you need to change the reward system of the game by counting the number of lives remaining for the agent. A more nuanced reward system will greatly improve the training. \n",
    "\n",
    "You'll also learn the limitations of the policy gradients in training the Breakout game. When taking the difference of two consecutive frames, the layers of bricks disappear and this affects later stages of the game. \n",
    "\n",
    "Non-the-less, the agent will learn to send the ball to the back to layers of bricks to score more efficiently. You'll learn how to capture those episodes and create an animation of those frames. Like so: <br>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/breakout_tunnel.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 17}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 17 in a subfolder /files/ch17. The code in the cell below will create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch17\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "## 1. Get Started with the Breakout Game\n",
    "\n",
    "In this section, you'll learn the special features of the Breakout game. I'll focus on the features that are different from the Pong game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7aaab",
   "metadata": {},
   "source": [
    "### 1.1. The Breakout Game\n",
    "\n",
    "Run the lines of code below to start the Breakout game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adfa3a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26b04e",
   "metadata": {},
   "source": [
    "You should see a Breakout game frame in a separate window. \n",
    "\n",
    "You can check the action space and observation space of the game as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ee5e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space for the Breakout game is Discrete(4)\n",
      "The observation space for the Breakout game is Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print out all possible actions in this game\n",
    "actions = env.action_space\n",
    "print(f\"The action space for the Breakout game is {actions}\")\n",
    "\n",
    "# Print out the observation space in this game\n",
    "obs_space = env.observation_space\n",
    "print(f\"The observation space for the Breakout game is {obs_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a68bf5",
   "metadata": {},
   "source": [
    "There are four possible actions the agent can take: \n",
    "* action 0: doing nothing\n",
    "* action 1: firing the ball\n",
    "* action 2: moving the paddle to the right\n",
    "* action 3: moving the paddle to the left\n",
    "\n",
    "Unlike in the Pong game, the ball is automatically fired, here we need to trigger the firing action. So we have to treat this as a multi-class classification problem. \n",
    "\n",
    "Each observation is a color picture that is 210 pixels tall and 160 pixels wide. The following cell displays an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1401e695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "env.reset()\n",
    "# run 20 steps so that the ball appears in the picture\n",
    "for _ in range(20):\n",
    "    action = np.random.choice([0,1,2,3])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "plt.imshow(obs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a9c5f",
   "metadata": {},
   "source": [
    "If you don't see a ball in the picture, rerun the above cell until you see a ball."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9ad6c",
   "metadata": {},
   "source": [
    "### 1.2. Process the Game Frames\n",
    "The input size of the game frame is too large, we'll process the image to reduce the size while retaining vital information to train the model to win the game.\n",
    "\n",
    "Specifically, we'll perform cropping, downsizing, and differencing before we feed the data into the model to train the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526aec3",
   "metadata": {},
   "source": [
    "#### Cropping\n",
    "We'll remove the top and the bottom of the game frame to reduce input size as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc83d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_cropped = obs[35:195]\n",
    "plt.imshow(obs_cropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2290089",
   "metadata": {},
   "source": [
    "The size of the picture is now 160 by 160 by 3. We need to further reduce the size of the picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa53494",
   "metadata": {},
   "source": [
    "#### Downsizing\n",
    "We'll use every other row and every other column so that the input size is smaller. Further, we'll use just one of the three color channels to further reduce size, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae73b29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_downsized = obs_cropped[::2,::2,0]\n",
    "plt.imshow(obs_downsized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d77fc",
   "metadata": {},
   "source": [
    "The size of the processed picture is now 80 by 80 by 1, a small fraction of the original size. However, the picture doesn't tell us the movement of the Breakout ball. We can potentially use two consecutive pictures, but that will double the input size to 80 by 80 by 2. With that, the number of parameters will be too large. The training will be too slow. Therefore, we'll use the difference between two consecutive frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fae7e7",
   "metadata": {},
   "source": [
    "#### Differencing\n",
    "We'll get the difference of the two consecutive frames after processing. The script below shows how. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0dfc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "action = np.random.choice([2,3])\n",
    "next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "next_obs_cropped = next_obs[35:195]\n",
    "next_obs_downsized = next_obs_cropped[::2,::2,0]\n",
    "\n",
    "dif = next_obs_downsized - obs_downsized\n",
    "plt.imshow(dif)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d421f4e",
   "metadata": {},
   "source": [
    "Note the limitation in differencing the frames. The layers of the bricks disappear from the picture. This is not a problem in early stages of the game since as long as the ball is caught by the paddle, it will bounce up and hit a brick on the way up and you'll score no matter what. \n",
    "\n",
    "But in later stages of the game when many bricks are gone, you don't know where the remaining bricks are. It's impossible for the agent to aim at the bricks and score. \n",
    "\n",
    "We'll address this limitation in Chapter 18, by using the Baselines game wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210b90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becfea11",
   "metadata": {},
   "source": [
    "Run the above line of code to close the game environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1e904",
   "metadata": {},
   "source": [
    "## 2. Train the Agent Using Policy Gradients\n",
    "\n",
    "We'll use policy gradients to train the agent. \n",
    "\n",
    "### 2.1. Changes we need to make when training the game\n",
    "\n",
    "The first thing we need to change is how the actions are determined. In the Pong game, we used the following line of code: \n",
    "\n",
    "```python\n",
    "action = 2 if np.random.uniform() < aprob else 3\n",
    "```\n",
    "\n",
    "We choose action 2 if the predicted probability is greater than a random number; otherwise we choose action 3. \n",
    "\n",
    "Now in the Breakout game, we have four actions to choose from. The model will make a prediction with four values, corresponding to the probabilities of the four actions. Therefore, we'll use this line of code: \n",
    "\n",
    "```python\n",
    "action=np.random.choice([0,1,2,3], p=aprob)\n",
    "```\n",
    "\n",
    "We choose the four actions randomly, but the probability of each action is proportional to the predicted probability. For example, if the prediction probabilities are [0.2, 0.3, 0.1, 0.4], we'll pick action 0 with probability 20%, action 1 with probability 30%, action 2 with probability 10%, and action 3 with probability 40%. This is similar to exploration and exploitation in the training of the Q-tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d8261",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Counting the Number of Lives Remaining\n",
    "\n",
    "In the Pong game, there are three possible rewards: -1, 0, and 1. Every time you fail to catch the ball, you get a reward of -1. \n",
    "\n",
    "However, in the Breakout game, there are only two possible rewards: 0 and 1. Every time you fail to catch the ball, the reward is still 0.  \n",
    "\n",
    "Run the script below to show what I mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4816786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "1.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 5}\n",
      "0.0 False {'ale.lives': 4}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "while True:\n",
    "    action = np.random.choice([0,1,2,3])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(reward, done, info)\n",
    "    env.render()\n",
    "    if info[\"ale.lives\"]==4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f546f8",
   "metadata": {},
   "source": [
    "You should have seen that the paddle just missed the ball. However, the reward is still 0 in the last step. We therefore will hard code in a reward of -1 whenever the number of live remaining decreases by 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edacff0",
   "metadata": {},
   "source": [
    "### 2.3. The Script to Train the Model\n",
    "The program below trains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf98732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "# Hyperparameters\n",
    "num_actions = 4\n",
    "H = 200 \n",
    "batch_size = 10 \n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 \n",
    "decay_rate = 0.99 \n",
    "D = 80 * 80 \n",
    "\n",
    "# Create model\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(H,D) / np.sqrt(D) \n",
    "model['W2'] = np.random.randn(num_actions,H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } \n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } \n",
    "\n",
    "# Define the one-hot encoder and softmax functions\n",
    "def onehot_encoder(action):\n",
    "    onehot=np.zeros((1,num_actions))\n",
    "    onehot[0,action]=1\n",
    "    return onehot\n",
    "\n",
    "def softmax(x):\n",
    "    xi=np.exp(x)\n",
    "    return xi/xi.sum() \n",
    "\n",
    "# Preprocss the pixels\n",
    "def prepro(I):\n",
    "  I = I[35:195] \n",
    "  I = I[::2,::2,0] \n",
    "  I[I == 144] = 0 \n",
    "  I[I == 109] = 0 \n",
    "  I[I != 0] = 1 \n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 \n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 \n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = softmax(logp)\n",
    "  return p, h \n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  dW2 = np.dot(epdlogp.T, eph)\n",
    "  dh = np.dot(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 \n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None \n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "# Count the number of lives to determine when to give reward -1\n",
    "lives=5\n",
    "\n",
    "while episode_number<250000:\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "  \n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "  action=np.random.choice([0,1,2,3], p=aprob)\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) \n",
    "  hs.append(h) \n",
    "  y = onehot_encoder(action)\n",
    "  # gradient for adjust weights, magic part\n",
    "  dlogps.append(y - aprob) \n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  # Not in Andrej's script, my addition to solve the reward structure problem\n",
    "  if (lives-info[\"ale.lives\"])==1:\n",
    "    lives -= 1\n",
    "    reward=-1\n",
    "  reward_sum += reward\n",
    "\n",
    "  drs.append(reward) \n",
    "\n",
    "  if done==True:\n",
    "    episode_number += 1\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "    \n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "    # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    epdlogp *= discounted_epr \n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] \n",
    "\n",
    "    # Reset lives to 5 after each episode\n",
    "    lives=5\n",
    "    fire=True\n",
    "    reward_sum = 0    \n",
    "    observation = env.reset() \n",
    "    prev_x = None        \n",
    "    \n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.items():\n",
    "        g = grad_buffer[k] \n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) \n",
    "    \n",
    "    # Show progress and save model\n",
    "    if episode_number % 10 == 0: \n",
    "        print(f'this is episode {episode_number} with reward {reward_sum}')\n",
    "    if episode_number % 100 == 0: \n",
    "        pickle.dump(model, open('v0breakpg.p', 'wb'))\n",
    "    if episode_number % 10000 == 0: \n",
    "        pickle.dump(model, open(f'v0breakpg{episode_number}.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6a8d3",
   "metadata": {},
   "source": [
    "It takes several days to train the model. I trained the model half a million episodes. The trained model v0breakpg.p is paced in files/ch17/ in this GitHub repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d794163",
   "metadata": {},
   "source": [
    "## 3. Test the Trained Model\n",
    "We'll play the game using the trained model. We'll test it on 100 games and see what's the average score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c6a97",
   "metadata": {},
   "source": [
    "### 3.1. Play One Game\n",
    "You'll play one game using the trained model. You'll turn on the graphical rendering so you can see the game frames. The program below tests the game for one episode. Each time the ball hit a brick, there is a reward of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eee3708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward total is 57.0\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open('files/ch17/v0breakpg.p', 'rb'))\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None \n",
    "reward_sum = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "  env.render()\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  aprob, h = policy_forward(x)\n",
    "  action=np.random.choice([0,1,2,3], p=aprob)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward  \n",
    "\n",
    "  if done==True:\n",
    "    print(f'reward total is {reward_sum}')\n",
    "    break        \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1a8fb",
   "metadata": {},
   "source": [
    "It's clear that the trained agent is able to catch the ball quite often. But it's far from perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058e09d",
   "metadata": {},
   "source": [
    "### 3.2. Play Multiple Games"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514ab27",
   "metadata": {},
   "source": [
    "We now play 5 games and turn off the graphical rendering. We'll see what the average score is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "245a2649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 reward total was 36.0\n",
      "episode 2 reward total was 33.0\n",
      "episode 3 reward total was 25.0\n",
      "episode 4 reward total was 69.0\n",
      "episode 5 reward total was 25.0\n",
      "running mean: 37.6\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open('files/ch17/v0breakpg.p', 'rb'))\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None \n",
    "running_reward = []\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "# See five episode in action\n",
    "while episode_number<5:\n",
    "\n",
    "  #env.render()\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  aprob, h = policy_forward(x)\n",
    "  action=np.random.choice([0,1,2,3], p=aprob)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward  \n",
    "\n",
    "  if done==True:\n",
    "    episode_number += 1\n",
    "    print(f'episode {episode_number} reward total was {reward_sum}')\n",
    "    running_reward.append(reward_sum)\n",
    "    reward_sum = 0    \n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None        \n",
    "    \n",
    "print(f'running mean: {np.mean(running_reward)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b7b39",
   "metadata": {},
   "source": [
    "The average score is 37.6 per episode.\n",
    "\n",
    "Next, we'll search for episodes that the agent successfully sends the ball to the back of the wall to score more efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afdb60c",
   "metadata": {},
   "source": [
    "## 4. Search for Successful Episodes\n",
    "If the agent sends the ball to the back of the wall, the ball can hit multiple bricks and have a high score in the episode. We therefore look for an episode with total rewards above, say, 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2649d",
   "metadata": {},
   "source": [
    "### 4.1. Collect Successful Episodes\n",
    "We'll first record a full game using the trained model.\n",
    "\n",
    "The script below accomplishes that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "import imageio\n",
    "\n",
    "# Hyperparameters\n",
    "num_actions = 4\n",
    "D = 80 * 80 \n",
    "model = pickle.load(open('files/ch17/v0breakpg.p', 'rb'))\n",
    "\n",
    "def onehot_encoder(action):\n",
    "    onehot=np.zeros((1,num_actions))\n",
    "    onehot[0,action]=1\n",
    "    return onehot\n",
    "\n",
    "def softmax(x):\n",
    "    xi=np.exp(x)\n",
    "    return xi/xi.sum() \n",
    "\n",
    "# Preprocss the pixels\n",
    "def prepro(I):\n",
    "  I = I[35:195] \n",
    "  I = I[::2,::2,0] \n",
    "  I[I == 144] = 0 \n",
    "  I[I == 109] = 0 \n",
    "  I[I != 0] = 1 \n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 \n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = softmax(logp)\n",
    "  return p, h \n",
    "\n",
    "env = gym.make(\"Breakout-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None \n",
    "running_reward = []\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "\n",
    "frames=[]\n",
    "while True:\n",
    "  frames.append(env.render(mode='rgb_array'))\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  aprob, h = policy_forward(x)\n",
    "  action=np.random.choice([0,1,2,3], p=aprob)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward  \n",
    "\n",
    "  if done==True:\n",
    "    episode_number += 1\n",
    "    print(f'episode {episode_number} reward total was {reward_sum}')\n",
    "    # usually, if episode reward is above 60, you have a tunnel\n",
    "    if reward_sum>100:\n",
    "        pickle.dump(frames, open(f'files/ch17/frames{episode_number}.p', 'wb'))\n",
    "        imageio.mimsave(f\"files/ch17/frames{episode_number}.gif\", frames, fps=60)\n",
    "        break\n",
    "    running_reward.append(reward_sum)\n",
    "    reward_sum = 0    \n",
    "    observation = env.reset() \n",
    "    prev_x = None \n",
    "    frame = []\n",
    "    \n",
    "print(f'running mean: {np.mean(running_reward)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1fd1b",
   "metadata": {},
   "source": [
    "It usually takes 5 to 10 minutes for you to have an episode with rewards above 100. Once done, you'll have an animation in your local folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f1eb1",
   "metadata": {},
   "source": [
    "The animation is usually fairly long. We need to zoom in on the part of the animation that the agent sends the ball to the back of the wall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6f6e2",
   "metadata": {},
   "source": [
    "### 4.2. Zoom in on Certain Steps\n",
    "Next, we'll zoom in on the steps of the game when the agent sends the ball to the back of the wall. \n",
    "For example, I have recorded an episode named frames84.p. So I extract the interesting parts of the video as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec0f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import imageio\n",
    "\n",
    "frames = pickle.load(open('files/ch17/frames84.p', 'rb'))\n",
    "imageio.mimsave(\"files/ch17/episode84.gif\", frames[800:-10], fps=240)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c69a94",
   "metadata": {},
   "source": [
    "The resulting animation episode84.gif looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c1718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/episode84.gif\" />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/episode84.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0a6a9",
   "metadata": {},
   "source": [
    "I have recorded another episode named frames606.p. So I extract the interesting parts of the video as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62badf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = pickle.load(open('files/ch17/frames606.p', 'rb'))\n",
    "imageio.mimsave(\"files/ch17/episode606.gif\", frames[600:], fps=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdaf7da",
   "metadata": {},
   "source": [
    "The resulting animation episode606.gif looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b712cee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/episode606.gif\" />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/episode606.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023e7e1",
   "metadata": {},
   "source": [
    "### 4.3. Combine the Animations\n",
    "We'll combine the two animations into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8821b368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n",
      "378\n"
     ]
    }
   ],
   "source": [
    "frames1 = pickle.load(open('files/ch17/frames84.p', 'rb'))\n",
    "frames1=frames1[800:-10]\n",
    "frames1=frames1+frames1\n",
    "print(len(frames1))\n",
    "\n",
    "\n",
    "frames2 = pickle.load(open('files/ch17/frames606.p', 'rb'))\n",
    "frames2=frames2[600:]\n",
    "print(len(frames2))\n",
    "\n",
    "fs = []\n",
    "for i in range(len(frames2)):\n",
    "    f = frames2[i]\n",
    "    rf = frames1[i]\n",
    "    middle = np.full(f.shape, 255).astype(\"uint8\")\n",
    "    frf = np.concatenate([f, middle, rf], axis=1)\n",
    "    fs.append(frf)\n",
    "imageio.mimsave('files/ch17/breakout_tunnel.gif', fs, fps=240) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c728bb",
   "metadata": {},
   "source": [
    "The animation looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "731b2efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/breakout_tunnel.gif\" />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/breakout_tunnel.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802cfdb",
   "metadata": {},
   "source": [
    "It's clear that the agent has learned to send the ball to the back of the wall to score more efficiently. This is because doing so earns higher rewards. The training has adjusted the parameters based on the rewards, and that is the essence of policy gradients. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
