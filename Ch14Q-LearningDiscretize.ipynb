{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 14: Q-Learning with Continuous States\n",
    "\n",
    "The Frozen Lake game we solved with Q-learning has 16 different states and 4 possible actions in each state. Therefore, it is easy to create a Q-table with $16\\times4=64$ values. \n",
    "\n",
    "However, in many real-world problems, the number of state-action combinations is either infinite or very large. Therefore, it is infeasible to build a Q-table. \n",
    "\n",
    "To solve this problem, we can use a finite number of discrete states to approximate for the infinite number of states. The finite number of states cannot be too small, or else the true state cannot be accurately represented and the Q-learning fails. The finite number of states cannot be too large either, or else it's prohibitively costly or time-consuming to train the Q values. \n",
    "\n",
    "At the end of this chapter, you'll be able to create an animation to compare the mountain car game before and after the Q-learning as follows: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/mountain_car_compares.gif\"/>\n",
    "\n",
    "On the left, you can see that without Q learning, the mountain car stays at the valley most of the time without much movement. After the Q learning, the mountain car can reach the mountain top in every episode. This shows how powerful Q-learning is.\n",
    "\n",
    "Specifically, you’ll learn how to create a finite state space in Q-learning in this chapter in the context of playing the Mountain Car game in the OpenAI Gym library. The state variables are \n",
    "\n",
    "* the position of the car, which is a continous variable with values between -1.2 and 0.6;\n",
    "* the speed of the car, which is a continous variable with values from -0.7 to 0.7. \n",
    "\n",
    "We'll use 190 discrete values of the position and 150 discrete values of the speed to form $190\\times150=28500$ different states. After 100,000 rounds of training, the model wins the game 100% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 14}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 14 in a subfolder /files/ch14. The code in the cell below will create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch14\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. Get Started with the Mountain Car Environment\n",
    "You’ll first learn how to control the Mountain Car game in the OpenAI Gym environment. You’ll learn the parameter values, how to interact with the environment so that later you’ll be able to train the model to learn the Q table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "### 1.1. The Mountain Car Game\n",
    "If you go to the Mountain Car game site via the link https://gym.openai.com/envs/MountainCar-v0/, you’ll see a picture similar to the following\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/mountain_car.png\" />\n",
    "\n",
    "The agent tries to drive the car up to the mountain top at the right. You can see a flag at the mountain top in the picture, and you win the game if you reach there within 200 attempts. \n",
    "\n",
    "The car starts at the bottom of the valley, and you need to swing the car back and forth to build up enough momentum in order to reach the goal. \n",
    "\n",
    "We’ll write a Python script to access the OpenAI Gym environment and learn its features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d575bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark\\.conda\\envs\\animatedML\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of possible actions are 3\n",
      "the following are ten sample actions\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "the shape of the observation space is (2,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "#obs = env.reset()\n",
    "# check the action space\n",
    "number_actions = env.action_space.n\n",
    "print(\"the number of possible actions are\", number_actions)\n",
    "# sample the action space ten times\n",
    "print(\"the following are ten sample actions\")\n",
    "for i in range(10):\n",
    "   print(env.action_space.sample())\n",
    "# check the shape of the observation space\n",
    "print(\"the shape of the observation space is\", env.observation_space.shape)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f161b",
   "metadata": {},
   "source": [
    "There are 3 possible actions: 0, 1, and 2, with the following meanings:\n",
    "* 0: move to the left\n",
    "* 1: no movement\n",
    "* 2: move to the right\n",
    "\n",
    "There are two state variables: \n",
    "\n",
    "* the position of the car, which is a continous variable with values between -1.2 and 0.6;\n",
    "* the speed of the car, which is a continous variable with values from -0.7 to 0.7. A negative value means the car is moving to the left, while a positive value means the car is moving to the right. \n",
    "\n",
    "To win the game, you need to reach the top of the mountain: that is, the position of the car needs to be greater or equal to 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e363e07",
   "metadata": {},
   "source": [
    "Below, you'll try to reach the mountain top in 200 attempts, by randomly selecting actions. The graphical rendering will show you how the game looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5181bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "time.sleep(5)\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e57b74",
   "metadata": {},
   "source": [
    "This is a difficult game. With random actions, the car stays at the bottom of the valley without much movemnt, let alone reachign the mountain top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552389bf",
   "metadata": {},
   "source": [
    "Next, you'll print out the state variables of the game, and learn how to convert them into discrete numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa92c7",
   "metadata": {},
   "source": [
    "### 1.2. Convert A Continuos State into Discrete Values\n",
    "Let's first have a look at the state values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e63483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41920721  0.        ]\n",
      "[-0.41997741 -0.0007702 ]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "obs = env.reset()\n",
    "# print out the state\n",
    "print(obs)\n",
    "action = env.action_space.sample()\n",
    "# print the new state\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(obs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e210a36",
   "metadata": {},
   "source": [
    "Next, we define a varaible *state*, which has discrete values. We multiply the first element in the observation, the car position, by 100, and take the integer value. The value is roughly between -120 and 60. We add 125 to it so that it remains positive and can be used as an index value in the Q table.\n",
    "\n",
    "We then multiply the second element in the observation, the car speed, by 1000, and take the integer value as well. We add 75 to it so that it remains positive and can be used as an index value in the Q table.\n",
    "\n",
    "The code below prints out ten samples of the variable *state*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07afb032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 70]\n",
      "[78, 69]\n",
      "[78, 68]\n",
      "[77, 67]\n",
      "[76, 68]\n",
      "[75, 67]\n",
      "[74, 66]\n",
      "[73, 67]\n",
      "[72, 66]\n",
      "[72, 67]\n"
     ]
    }
   ],
   "source": [
    "def obs_to_state(obs):\n",
    "    state = [int(obs[0]*100)+125, int(obs[1]*1000)+75]\n",
    "    return state\n",
    "\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    state = obs_to_state(obs)\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35c82f",
   "metadata": {},
   "source": [
    "The variable *state*, which is a list, now has two integers in it. \n",
    "\n",
    "Since the value of the car position is between -1.2 to 0.6, and the speed of the car is between -0.7 to 0.7, *state* has roughly a maximum of $180\\times140=25200$ values. We create a Q table with dimensions 190 by 150 by 3. The first dimension of the Q-table corresponds to the 180 or so discrete car positions; the second dimension corresponds to the 140 or so discrete car speeds; the third dimension corresponds to the three possible actions. We use 190 and 150 instead of 180 and 140 to have some margin of safety to avoid possible IndexError. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab561b",
   "metadata": {},
   "source": [
    "### 1.3. Understand the Reward Structure of the Game\n",
    "Next, we'll play a game until it's finished and print out all the rewards and when an episode is considered finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7aaeb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [70, 75] -1.0 False {}\n",
      "2 [70, 75] -1.0 False {}\n",
      "3 [70, 76] -1.0 False {}\n",
      "4 [71, 77] -1.0 False {}\n",
      "5 [71, 79] -1.0 False {}\n",
      "6 [72, 80] -1.0 False {}\n",
      "7 [72, 81] -1.0 False {}\n",
      "8 [73, 82] -1.0 False {}\n",
      "9 [74, 82] -1.0 False {}\n",
      "10 [75, 83] -1.0 False {}\n",
      "11 [76, 84] -1.0 False {}\n",
      "12 [76, 84] -1.0 False {}\n",
      "13 [77, 83] -1.0 False {}\n",
      "14 [78, 82] -1.0 False {}\n",
      "15 [79, 82] -1.0 False {}\n",
      "16 [79, 80] -1.0 False {}\n",
      "17 [80, 79] -1.0 False {}\n",
      "18 [80, 78] -1.0 False {}\n",
      "19 [80, 77] -1.0 False {}\n",
      "20 [80, 75] -1.0 False {}\n",
      "21 [81, 76] -1.0 False {}\n",
      "22 [81, 75] -1.0 False {}\n",
      "23 [81, 75] -1.0 False {}\n",
      "24 [81, 75] -1.0 False {}\n",
      "25 [80, 73] -1.0 False {}\n",
      "26 [80, 72] -1.0 False {}\n",
      "27 [80, 71] -1.0 False {}\n",
      "28 [79, 72] -1.0 False {}\n",
      "29 [79, 71] -1.0 False {}\n",
      "30 [78, 70] -1.0 False {}\n",
      "31 [78, 71] -1.0 False {}\n",
      "32 [77, 71] -1.0 False {}\n",
      "33 [77, 72] -1.0 False {}\n",
      "34 [77, 72] -1.0 False {}\n",
      "35 [76, 72] -1.0 False {}\n",
      "36 [76, 71] -1.0 False {}\n",
      "37 [76, 72] -1.0 False {}\n",
      "38 [75, 73] -1.0 False {}\n",
      "39 [75, 72] -1.0 False {}\n",
      "40 [75, 73] -1.0 False {}\n",
      "41 [74, 73] -1.0 False {}\n",
      "42 [74, 73] -1.0 False {}\n",
      "43 [74, 73] -1.0 False {}\n",
      "44 [74, 73] -1.0 False {}\n",
      "45 [73, 72] -1.0 False {}\n",
      "46 [73, 72] -1.0 False {}\n",
      "47 [73, 71] -1.0 False {}\n",
      "48 [72, 72] -1.0 False {}\n",
      "49 [72, 71] -1.0 False {}\n",
      "50 [71, 71] -1.0 False {}\n",
      "51 [71, 71] -1.0 False {}\n",
      "52 [71, 73] -1.0 False {}\n",
      "53 [70, 73] -1.0 False {}\n",
      "54 [70, 74] -1.0 False {}\n",
      "55 [70, 74] -1.0 False {}\n",
      "56 [70, 73] -1.0 False {}\n",
      "57 [70, 75] -1.0 False {}\n",
      "58 [70, 74] -1.0 False {}\n",
      "59 [70, 74] -1.0 False {}\n",
      "60 [69, 73] -1.0 False {}\n",
      "61 [69, 74] -1.0 False {}\n",
      "62 [69, 74] -1.0 False {}\n",
      "63 [69, 75] -1.0 False {}\n",
      "64 [69, 75] -1.0 False {}\n",
      "65 [69, 75] -1.0 False {}\n",
      "66 [69, 74] -1.0 False {}\n",
      "67 [69, 74] -1.0 False {}\n",
      "68 [68, 74] -1.0 False {}\n",
      "69 [68, 74] -1.0 False {}\n",
      "70 [68, 75] -1.0 False {}\n",
      "71 [69, 76] -1.0 False {}\n",
      "72 [69, 77] -1.0 False {}\n",
      "73 [69, 79] -1.0 False {}\n",
      "74 [70, 78] -1.0 False {}\n",
      "75 [70, 77] -1.0 False {}\n",
      "76 [70, 78] -1.0 False {}\n",
      "77 [71, 78] -1.0 False {}\n",
      "78 [71, 78] -1.0 False {}\n",
      "79 [71, 79] -1.0 False {}\n",
      "80 [72, 79] -1.0 False {}\n",
      "81 [72, 79] -1.0 False {}\n",
      "82 [73, 80] -1.0 False {}\n",
      "83 [74, 81] -1.0 False {}\n",
      "84 [74, 81] -1.0 False {}\n",
      "85 [75, 82] -1.0 False {}\n",
      "86 [76, 82] -1.0 False {}\n",
      "87 [76, 81] -1.0 False {}\n",
      "88 [77, 82] -1.0 False {}\n",
      "89 [78, 82] -1.0 False {}\n",
      "90 [79, 83] -1.0 False {}\n",
      "91 [80, 84] -1.0 False {}\n",
      "92 [81, 84] -1.0 False {}\n",
      "93 [81, 83] -1.0 False {}\n",
      "94 [82, 82] -1.0 False {}\n",
      "95 [83, 81] -1.0 False {}\n",
      "96 [83, 81] -1.0 False {}\n",
      "97 [84, 80] -1.0 False {}\n",
      "98 [84, 78] -1.0 False {}\n",
      "99 [85, 77] -1.0 False {}\n",
      "100 [85, 77] -1.0 False {}\n",
      "101 [85, 77] -1.0 False {}\n",
      "102 [86, 78] -1.0 False {}\n",
      "103 [86, 78] -1.0 False {}\n",
      "104 [86, 77] -1.0 False {}\n",
      "105 [86, 75] -1.0 False {}\n",
      "106 [86, 74] -1.0 False {}\n",
      "107 [86, 74] -1.0 False {}\n",
      "108 [86, 74] -1.0 False {}\n",
      "109 [85, 74] -1.0 False {}\n",
      "110 [85, 73] -1.0 False {}\n",
      "111 [85, 72] -1.0 False {}\n",
      "112 [84, 71] -1.0 False {}\n",
      "113 [84, 70] -1.0 False {}\n",
      "114 [83, 69] -1.0 False {}\n",
      "115 [82, 67] -1.0 False {}\n",
      "116 [82, 67] -1.0 False {}\n",
      "117 [81, 66] -1.0 False {}\n",
      "118 [80, 65] -1.0 False {}\n",
      "119 [79, 66] -1.0 False {}\n",
      "120 [78, 65] -1.0 False {}\n",
      "121 [77, 65] -1.0 False {}\n",
      "122 [75, 64] -1.0 False {}\n",
      "123 [74, 64] -1.0 False {}\n",
      "124 [73, 62] -1.0 False {}\n",
      "125 [71, 61] -1.0 False {}\n",
      "126 [70, 63] -1.0 False {}\n",
      "127 [69, 62] -1.0 False {}\n",
      "128 [68, 63] -1.0 False {}\n",
      "129 [66, 64] -1.0 False {}\n",
      "130 [65, 63] -1.0 False {}\n",
      "131 [64, 64] -1.0 False {}\n",
      "132 [63, 63] -1.0 False {}\n",
      "133 [62, 65] -1.0 False {}\n",
      "134 [61, 66] -1.0 False {}\n",
      "135 [60, 66] -1.0 False {}\n",
      "136 [59, 67] -1.0 False {}\n",
      "137 [58, 68] -1.0 False {}\n",
      "138 [58, 70] -1.0 False {}\n",
      "139 [57, 70] -1.0 False {}\n",
      "140 [57, 72] -1.0 False {}\n",
      "141 [56, 72] -1.0 False {}\n",
      "142 [56, 73] -1.0 False {}\n",
      "143 [56, 74] -1.0 False {}\n",
      "144 [56, 74] -1.0 False {}\n",
      "145 [56, 74] -1.0 False {}\n",
      "146 [56, 76] -1.0 False {}\n",
      "147 [56, 76] -1.0 False {}\n",
      "148 [56, 78] -1.0 False {}\n",
      "149 [57, 78] -1.0 False {}\n",
      "150 [57, 79] -1.0 False {}\n",
      "151 [58, 82] -1.0 False {}\n",
      "152 [59, 82] -1.0 False {}\n",
      "153 [59, 83] -1.0 False {}\n",
      "154 [60, 83] -1.0 False {}\n",
      "155 [61, 83] -1.0 False {}\n",
      "156 [62, 83] -1.0 False {}\n",
      "157 [63, 82] -1.0 False {}\n",
      "158 [63, 82] -1.0 False {}\n",
      "159 [64, 84] -1.0 False {}\n",
      "160 [65, 85] -1.0 False {}\n",
      "161 [66, 84] -1.0 False {}\n",
      "162 [67, 85] -1.0 False {}\n",
      "163 [68, 84] -1.0 False {}\n",
      "164 [69, 85] -1.0 False {}\n",
      "165 [70, 85] -1.0 False {}\n",
      "166 [71, 85] -1.0 False {}\n",
      "167 [72, 84] -1.0 False {}\n",
      "168 [73, 85] -1.0 False {}\n",
      "169 [75, 86] -1.0 False {}\n",
      "170 [76, 86] -1.0 False {}\n",
      "171 [77, 87] -1.0 False {}\n",
      "172 [78, 85] -1.0 False {}\n",
      "173 [79, 85] -1.0 False {}\n",
      "174 [80, 84] -1.0 False {}\n",
      "175 [81, 84] -1.0 False {}\n",
      "176 [82, 84] -1.0 False {}\n",
      "177 [83, 82] -1.0 False {}\n",
      "178 [83, 81] -1.0 False {}\n",
      "179 [84, 79] -1.0 False {}\n",
      "180 [84, 79] -1.0 False {}\n",
      "181 [85, 79] -1.0 False {}\n",
      "182 [85, 78] -1.0 False {}\n",
      "183 [85, 76] -1.0 False {}\n",
      "184 [85, 75] -1.0 False {}\n",
      "185 [85, 73] -1.0 False {}\n",
      "186 [85, 73] -1.0 False {}\n",
      "187 [85, 74] -1.0 False {}\n",
      "188 [84, 72] -1.0 False {}\n",
      "189 [84, 70] -1.0 False {}\n",
      "190 [83, 69] -1.0 False {}\n",
      "191 [82, 67] -1.0 False {}\n",
      "192 [81, 68] -1.0 False {}\n",
      "193 [81, 67] -1.0 False {}\n",
      "194 [80, 66] -1.0 False {}\n",
      "195 [79, 65] -1.0 False {}\n",
      "196 [77, 64] -1.0 False {}\n",
      "197 [76, 64] -1.0 False {}\n",
      "198 [75, 65] -1.0 False {}\n",
      "199 [74, 66] -1.0 False {}\n",
      "200 [73, 66] -1.0 True {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    state = [int(obs[0]*100)+125, int(obs[1]*1000)+75]\n",
    "    return state\n",
    "\n",
    "obs = env.reset()\n",
    "episode = 0\n",
    "while True:\n",
    "    episode += 1 \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    state = obs_to_state(obs)\n",
    "    print(episode, state, reward, done, info)\n",
    "    env.render()\n",
    "    # play till a full episode is finished\n",
    "    if done:\n",
    "        break\n",
    "time.sleep(5)\n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9db4d",
   "metadata": {},
   "source": [
    "Each episode has a maximum of 200 steps. The episode is considered finished when the mountain car reaches the mountain top or when the number of attempts has reached 200, whichever comes first. \n",
    "\n",
    "In each step, the reward is -1, unless the mountain car reaches the mountain top, in which case the reward is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 2. Train the Q-Table for the Mountain Car Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "We now train the Q-table for the mountain car game. \n",
    "\n",
    "We first populate the 190 by 150 by 3 Q table with zeros. In each step, unless the mountain car reaches the top, we use Q-learning to update the Q values as follows:\n",
    "\n",
    " $$ New\\ Q(s,a) = learning\\ rate * [Reward + discount\\ factor * max\\  Q(s’, a’)]+ (1-learning\\ rate) * Old\\  Q(s, a)$$\n",
    " \n",
    "If the mountain car reaches the top in the step, we update the Q value as follows.\n",
    "\n",
    " $$ New\\ Q(s,a) = Reward $$\n",
    "\n",
    "After many rounds of trial and error, the update will be minimal, which means the Q values converge to the equilibrium values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c4b83",
   "metadata": {},
   "source": [
    "Here is the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, time\n",
    "import numpy as np\n",
    "env = gym.make('MountainCar-v0')\n",
    "#env.reset()\n",
    "\n",
    "learning_rate=0.2\n",
    "discount_rate=0.99\n",
    "\n",
    "max_exp=0.9\n",
    "min_exp=0.01\n",
    "\n",
    "max_episode=100000\n",
    "max_steps=200\n",
    "Q = np.zeros((190, 150, 3))\n",
    "\n",
    "def obs_to_state(obs):\n",
    "    state = [int(obs[0]*100)+125, int(obs[1]*1000)+75]\n",
    "    return state\n",
    "\n",
    "outcome=[]\n",
    "def update_Q(episode):\n",
    "    # Initial state is the starting position\n",
    "    \n",
    "    obs=env.reset()      \n",
    "    # Play a full game till it ends\n",
    "    for _ in range(max_steps):\n",
    "        state = obs_to_state(obs)\n",
    "        # Select the best action or the random action\n",
    "        if np.random.uniform(0,1,1)>min_exp+(max_exp-min_exp)*episode/max_episode:\n",
    "            action = np.argmax(Q[state[0], state[1], :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        # Use the selected action to make the move\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        new_state = obs_to_state(new_obs)\n",
    "        # Update Q values\n",
    "        if done==True:\n",
    "            if new_obs[0]>=0.5:\n",
    "                Q[state[0], state[1], action] = reward\n",
    "                outcome.append(1)\n",
    "                break    \n",
    "            else:\n",
    "                outcome.append(0)\n",
    "                break      \n",
    "        else:\n",
    "            Q[state[0], state[1], action] = learning_rate*\\\n",
    "            (reward+discount_rate*np.max(Q[new_state[0], new_state[1], :]))\\\n",
    "            + (1-learning_rate)*Q[state[0], state[1], action]\n",
    "            obs=new_obs \n",
    "            continue\n",
    "   \n",
    "for episode in range(max_episode):\n",
    "    update_Q(episode)\n",
    "    if episode%1000 == 0:\n",
    "        print(\"this is episode\", episode)\n",
    "    \n",
    "print(Q)   \n",
    "import pickle\n",
    "with open('files/ch14/mountain_car_Qs.pickle', 'wb') as fp:\n",
    "    pickle.dump((Q, outcome),fp)\n",
    "# Read the data and print out the first 10 games\n",
    "with open('files/ch14/mountain_car_Qs.pickle', 'rb') as fp:\n",
    "    (myQ, myoutcome)=pickle.load(fp)\n",
    "print(myQ.shape, len(myoutcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "The training takes about 30 minutes. The exact amount of time depends on your computer speed. The model is saved as a pickle file on your computer for later use. I put it in this GitHub respository as well. So use the file files/ch14/mountain_car_Qs.pickle if you have trouble training the Q table by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5412a83",
   "metadata": {},
   "source": [
    "## 3. Test the Trained Q-Values\n",
    "Now, you can test if the trained Q-table works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72640a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 150, 3)\n",
      "congrats, you made it in 86 attempts\n",
      "congrats, you made it in 155 attempts\n",
      "congrats, you made it in 112 attempts\n",
      "congrats, you made it in 83 attempts\n",
      "congrats, you made it in 88 attempts\n",
      "congrats, you made it in 92 attempts\n",
      "congrats, you made it in 155 attempts\n",
      "congrats, you made it in 114 attempts\n",
      "congrats, you made it in 155 attempts\n",
      "congrats, you made it in 162 attempts\n",
      "success rate 1.0\n"
     ]
    }
   ],
   "source": [
    "import gym, time\n",
    "import numpy as np\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "import pickle\n",
    "# Read the data and print out the first 10 games\n",
    "with open('files/ch14/mountain_car_Qs.pickle', 'rb') as fp:\n",
    "    (Q, outcome)=pickle.load(fp)\n",
    "print(Q.shape)\n",
    "def obs_to_state(obs):\n",
    "    state = [int(obs[0]*100)+125, int(obs[1]*1000)+75]\n",
    "    return state\n",
    "\n",
    "num_success=0\n",
    "def test_Q():\n",
    "    global num_success\n",
    "    obs=env.reset()   \n",
    "    steps = 0\n",
    "    # Play a full game till it ends\n",
    "    while True:\n",
    "        steps += 1\n",
    "        state = obs_to_state(obs)\n",
    "        # Select the best action or the random action\n",
    " \n",
    "        action = np.argmax(Q[state[0], state[1], :])\n",
    "\n",
    "        \n",
    "        # Use the selected action to make the move\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        obs=new_obs\n",
    "        env.render()\n",
    "        # Update Q values\n",
    "        if done==True:\n",
    "            if new_obs[0]>=0.5:\n",
    "                print(f\"congrats, you made it in {steps} attempts\")\n",
    "                num_success += 1\n",
    "            else:\n",
    "                print(\"sorry, better luck next time\")\n",
    "            env.close()\n",
    "            break    \n",
    "  \n",
    "for _ in range(10):\n",
    "    test_Q()\n",
    "\n",
    "print(\"success rate\", num_success/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ce5dd",
   "metadata": {},
   "source": [
    "We tested the game for 10 episodes and the mountain car has reached the mountain top in every episode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb298811",
   "metadata": {},
   "source": [
    "## 4. Animate the Game Before and After the Q-Learning\n",
    "We'll first animate the mountain car game before Q-learning. You'll see that the mountain car stays at the valley without much movement. After Q-learning, the mountain car made to the top in every episode. We'll put the animation before and after the Q-learning side by side to show what difference the Q-learning makes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7789cc1",
   "metadata": {},
   "source": [
    "First, let's animate the game before Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac17214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "frames=[]\n",
    "for _ in range(5):\n",
    "    env.reset()\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    while True:      \n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        if done:\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "# Now double the size.\n",
    "frames4=[]\n",
    "for frame in frames:\n",
    "    frame4 = frame.repeat(2, axis=0).repeat(2, axis=1)\n",
    "    frames4.append(frame4)\n",
    "imageio.mimsave('files/ch14/mountain_car_beforeQ.gif', frames4, fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "Here we double the height and width of each frame so we have four times the resolution as before, using the *repeat()* method in the ***numpy*** library. \n",
    "\n",
    "If you open the gif file in your local folder, you should see an animation as follows\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/mountain_car_beforeQ.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "The above animation shows that the mountain car stays at the valley most of the time without much movement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2dd43",
   "metadata": {},
   "source": [
    "Now, let's create an animation of the mountain car with the help of the Q-learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea53d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Read the data and print out the first 10 games\n",
    "with open('files/ch14/mountain_car_Qs.pickle', 'rb') as fp:\n",
    "    (Q, outcome)=pickle.load(fp)\n",
    "print(Q.shape)\n",
    "def obs_to_state(obs):\n",
    "    state = [int(obs[0]*100)+125, int(obs[1]*1000)+75]\n",
    "    return state\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "Q_frames=[]\n",
    "for _ in range(10):\n",
    "    obs=env.reset() \n",
    "    Q_frames.append(env.render(mode='rgb_array'))\n",
    "    while True:    \n",
    "        state = obs_to_state(obs)\n",
    "        # Select the best action or the random action\n",
    "        action = np.argmax(Q[state[0], state[1], :])        \n",
    "        # Use the selected action to make the move\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        Q_frames.append(env.render(mode='rgb_array'))\n",
    "        obs=new_obs\n",
    "        env.render()                \n",
    "        if done:\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "# Now double the size.\n",
    "Q_frames4=[]\n",
    "for Q_frame in Q_frames:\n",
    "    Q_frame4 = Q_frame.repeat(2, axis=0).repeat(2, axis=1)\n",
    "    Q_frames4.append(Q_frame4)\n",
    "imageio.mimsave('files/ch14/mountain_car_withQ.gif', Q_frames4, fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb762289",
   "metadata": {},
   "source": [
    "If you open, the file mountain_car_withQ.gif, you'll see the following: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/mountain_car_withQ.gif\"/>\n",
    "\n",
    "The mountain can can now reach the mountain top in every episode without much effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a0fe6",
   "metadata": {},
   "source": [
    "Next, we'll put the frames before Q-learning and after Q-learning side by side. You can see the comparision in the same animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4deaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames = []\n",
    "num_frames = min(len(frames4), len(Q_frames4))\n",
    "for i in range(num_frames):\n",
    "    frame4 = frames4[i]\n",
    "    Q_frame4 = Q_frames4[i]\n",
    "    frame = np.concatenate([frame4, Q_frame4], axis=1)\n",
    "    frames.append(frame)\n",
    "imageio.mimsave('files/ch14/mountain_car_compares.gif', frames, fps=24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea21ada",
   "metadata": {},
   "source": [
    "If you open the file mountain_car_compares.gif, you'll see the following: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/mountain_car_compares.gif\"/>\n",
    "\n",
    "Now you can compare the mountain car game with and without Q-learning. This shows how powerful Q-learning is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acfa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
