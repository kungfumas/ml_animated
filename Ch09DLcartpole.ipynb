{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 9: Apply Deep Learning to Any Game\n",
    "\n",
    "Games in the OpenAI Gym environment are designed for reinforcement learning, how to win a game with a deep learning game strategy sometimes proves challenging. \n",
    "\n",
    "The CartPole game is such a case. Winning is defined as making the Cart Pole stand upright for at least 195 consecutive steps. If we randomly choose actions, we'll end up with all losing games. Then the simulated data have no use for training purpose. \n",
    "\n",
    "In such cases, we need to be creative and redefine what's considered \"winning.\" Specifically, in this chapter, you'll use random actions to simulate 10,000 games. The last ten steps of the game are considered \"losing\" while all steps before that are considered \"winning.\" You can think of winning as the ability to stay upright for at least another ten steps. You can use other cutoff values such as 15 or 20 instead of 10 steps, and the strategy works just the same. \n",
    "\n",
    "After that, you'll feed the data into a deep neural network. Once the model is trained, the agent will ask the model this question: for the given situation, what's the probability of winning if I were to take move left hypothetically; also for the given situation, what's the probability of winning if I were to move right hypothetically?. The agent will pick the action with the higher probability of winning. Such a strategy will help the agent win the game 100% of the time. That is, the agent can stay upright for at least 195 steps in every game.\n",
    "\n",
    "At the end of this chapter, you'll create an animation to show how the game is played with and without deep learning strategy. Readers can see the two scenarios side by side. \n",
    "The animation looks as follows:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/compare_cartpole.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 9}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 9 in a subfolder /files/ch09. The code in the cell below will create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch09\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. Play the Cart Pole Game in OpenAI Gym\n",
    "As we discussed in Chapter 8, you need to install the ***gym*** library first. Further, we'll use version 0.15.7 of the library because newer versions are not compatible with the ***Baselines*** library, which we need to play the Atari games.\n",
    "\n",
    "Before you get started, install the OpenAI Gym library as follows:\n",
    "\n",
    "`pip install gym==0.15.7`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "### 1.1. Features of the Cart Pole Game "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "If you go to the official Cartpole game website https://gym.openai.com/envs/CartPole-v0/. \n",
    "\n",
    "The problem is considered solved as getting an average reward of 195 or above in 100 consecutive trials. \n",
    "\n",
    "The code in the cell below will get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark\\.conda\\envs\\animatedML\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym \n",
    " \n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "\n",
    "We can also print out all possible actions and states of the game as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052e7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space in the Cart Pole game is Discrete(2)\n",
      "The state space in the Cart Pole game is Box(4,)\n"
     ]
    }
   ],
   "source": [
    "# Print out all possible actions in this game\n",
    "actions = env.action_space\n",
    "print(f\"The action space in the Cart Pole game is {actions}\")\n",
    "\n",
    "# Print out all possible states in this game\n",
    "states = env.observation_space\n",
    "print(f\"The state space in the Cart Pole game is {states}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The action space in the Cart Pole game has two values: 0 and 1, with the following meanings:\n",
    "* 0: moving left\n",
    "* 1: moving right\n",
    "\n",
    "The state in the Cart Pole game is a collection of four values, with the following meanings:\n",
    "\n",
    "* The position of the cart, with values between -4.8 and 4.8; \n",
    "* The velociyt of the cart, with values between -4 and 4; \n",
    "* The angle of the pole, with values between -0.42 and 0.42;\n",
    "* The angular velocity of the pole, with values between -4 and 4; \n",
    "\n",
    "\n",
    "The agent earns a reward of 1 for every time that the pole stays upright. If the pole is more than 15 degrees from \n",
    " vertical or the cart moves more than 2.4 units from the center, the agent loses the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444cdab",
   "metadata": {},
   "source": [
    "### 1.2. Play A Full Game\n",
    "You can play a complete game as follows by using random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629df3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-0.00077322 -0.1786936   0.00362296  0.28238639] 1.0 False {}\n",
      "0\n",
      "[-0.00434709 -0.37386704  0.00927069  0.57620978] 1.0 False {}\n",
      "1\n",
      "[-0.01182443 -0.17887626  0.02079488  0.28646172] 1.0 False {}\n",
      "1\n",
      "[-0.01540195  0.01594305  0.02652412  0.00040919] 1.0 False {}\n",
      "0\n",
      "[-0.01508309 -0.17954906  0.0265323   0.30134138] 1.0 False {}\n",
      "0\n",
      "[-0.01867407 -0.37503893  0.03255913  0.60227256] 1.0 False {}\n",
      "1\n",
      "[-0.02617485 -0.18038716  0.04460458  0.32002035] 1.0 False {}\n",
      "0\n",
      "[-0.0297826  -0.37611503  0.05100499  0.62642954] 1.0 False {}\n",
      "1\n",
      "[-0.0373049  -0.18174075  0.06353358  0.35023625] 1.0 False {}\n",
      "1\n",
      "[-0.04093971  0.01242282  0.0705383   0.07824482] 1.0 False {}\n",
      "0\n",
      "[-0.04069126 -0.18363572  0.0721032   0.39232236] 1.0 False {}\n",
      "0\n",
      "[-0.04436397 -0.37970288  0.07994965  0.70683892] 1.0 False {}\n",
      "1\n",
      "[-0.05195803 -0.18577423  0.09408642  0.44035529] 1.0 False {}\n",
      "0\n",
      "[-0.05567351 -0.38209301  0.10289353  0.76115164] 1.0 False {}\n",
      "0\n",
      "[-0.06331537 -0.57847056  0.11811656  1.0843574 ] 1.0 False {}\n",
      "0\n",
      "[-0.07488478 -0.77493596  0.13980371  1.41164771] 1.0 False {}\n",
      "1\n",
      "[-0.0903835  -0.58179648  0.16803667  1.16573361] 1.0 False {}\n",
      "0\n",
      "[-0.10201943 -0.77865856  0.19135134  1.50603815] 1.0 False {}\n",
      "0\n",
      "[-0.1175926  -0.97551641  0.2214721   1.85185   ] 1.0 True {}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()   \n",
    "\n",
    "while True:\n",
    "    action = actions.sample()\n",
    "    print(action)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(new_state, reward, done, info)    \n",
    "    if done == True:\n",
    "        break\n",
    "\n",
    "env.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "The above cell used several methods in the game environment:\n",
    "* The sample() method randomly selects an action from the action space. That is, it will return one of the values among {0, 1}. \n",
    "* The step() method is where the agent is interacting with the environment, and it takes the agent’s action as input. The output are four values: the new state, the reward, a variable *done* indicating whether the game has ended. The *info* variable provides some information about the game. \n",
    "* The render() method shows a diagram of the resulting state. \n",
    "\n",
    "The game loop is an infinite *while* loop. If the *done* variable returns a value *True*, the game ends, and we stop the infinite while loop.\n",
    "\n",
    "Note that since the actions are chosen randomly, when you run the script, you’ll most likely get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa3a085",
   "metadata": {},
   "source": [
    "## 2. Deep Learning Game Strategies: Generating Data\n",
    "\n",
    "In the next few sections, you’ll learn how to use deep neural networks to train intelligent game strategies.\n",
    "\n",
    "First, you’ll learn how to generate simulated game data for training purpose. However, you'll see that all the rewards are 1. As soon as the game ends, another episode of game starts again. Therefore, we need to creatively define what's considered winning. \n",
    "\n",
    "Once you have a trained model, you’ll learn how to use the model to design a best-move game strategy and play against the computer. Finally, you’ll test the effectiveness of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62465307",
   "metadata": {},
   "source": [
    "### 2.1. How to Define Winning and Losing?\n",
    "In the Frozen Lake game in Chapter 8, winning and losing is easy to define: if the agent reaches the goal, it's a win. On the other hand, if the agent falls into one of the hole, it's a loss. \n",
    "\n",
    "There could be other ways of defining winning and losing, but we'll define a step as a win if it survives at least another ten steps. Otherwise, it's a losing strategy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "### 2.2. Prepare Data for the Neural Network\n",
    "How to use neural network to train a game strategy in this case? Here is a summary of what we’ll do to train the game strategy:\n",
    "1. We’ll let the player randomly choose actions and complete a game and record the whole game history. The game history will contain all the intermediate states and actions from the very first move to the very last move.\n",
    "2. We then associate each state-action pair with a game outcome (win or lose). The state-action pair is similar to X (i.e., image pixels) in our image classification problem, and the outcome is similar to y (i.e., image labels such as horse, deer, airplane and so on) in the image classification problem.\n",
    "3. We’ll simulate a large number of games, say, 10000 of them. Use the histories of the games and the corresponding outcome as X and y pairs to feed into a Deep Neural Networks model. After training is done, we have a trained model.\n",
    "4. At each move of the game, we look at all possible next moves, and feed the hypothetical state-action pair into the pretained model. The model will tell you the probability of winning the game if that state-action pair were chosen.\n",
    "5. You select the move with the highest chance of winning based on the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2dd43",
   "metadata": {},
   "source": [
    "#### Simulate One Game\n",
    "\n",
    "First we’ll simulate one game and record the whole game history and the game outcome. The script below accomplishes that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea53d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the game history is [[array([-0.00482245, -0.03372653,  0.01694466,  0.01906569]), 0, array([-0.00549698, -0.22908734,  0.01732597,  0.31704633]), 1.0, False], [array([-0.00549698, -0.22908734,  0.01732597,  0.31704633]), 1, array([-0.01007873, -0.0342164 ,  0.0236669 ,  0.02987737]), 1.0, False], [array([-0.01007873, -0.0342164 ,  0.0236669 ,  0.02987737]), 1, array([-0.01076306,  0.1605583 ,  0.02426445, -0.25524544]), 1.0, False], [array([-0.01076306,  0.1605583 ,  0.02426445, -0.25524544]), 0, array([-0.00755189, -0.03490154,  0.01915954,  0.04499106]), 1.0, False], [array([-0.00755189, -0.03490154,  0.01915954,  0.04499106]), 0, array([-0.00824992, -0.23029292,  0.02005936,  0.34365689]), 1.0, False], [array([-0.00824992, -0.23029292,  0.02005936,  0.34365689]), 0, array([-0.01285578, -0.42569441,  0.0269325 ,  0.64259722]), 1.0, False], [array([-0.01285578, -0.42569441,  0.0269325 ,  0.64259722]), 1, array([-0.02136967, -0.23095801,  0.03978444,  0.35851575]), 1.0, False], [array([-0.02136967, -0.23095801,  0.03978444,  0.35851575]), 0, array([-0.02598883, -0.42662229,  0.04695476,  0.66347346]), 1.0, False], [array([-0.02598883, -0.42662229,  0.04695476,  0.66347346]), 1, array([-0.03452128, -0.23218393,  0.06022423,  0.38593702]), 1.0, False], [array([-0.03452128, -0.23218393,  0.06022423,  0.38593702]), 1, array([-0.03916495, -0.03796633,  0.06794297,  0.11283399]), 1.0, False], [array([-0.03916495, -0.03796633,  0.06794297,  0.11283399]), 1, array([-0.03992428,  0.1561196 ,  0.07019965, -0.15766334]), 1.0, False], [array([-0.03992428,  0.1561196 ,  0.07019965, -0.15766334]), 0, array([-0.03680189, -0.03993353,  0.06704638,  0.1563138 ]), 1.0, False], [array([-0.03680189, -0.03993353,  0.06704638,  0.1563138 ]), 0, array([-0.03760056, -0.23594818,  0.07017266,  0.46937204]), 1.0, False], [array([-0.03760056, -0.23594818,  0.07017266,  0.46937204]), 0, array([-0.04231952, -0.43198762,  0.0795601 ,  0.78332132]), 1.0, False], [array([-0.04231952, -0.43198762,  0.0795601 ,  0.78332132]), 1, array([-0.05095928, -0.23804392,  0.09522652,  0.51669202]), 1.0, False], [array([-0.05095928, -0.23804392,  0.09522652,  0.51669202]), 1, array([-0.05572015, -0.04438276,  0.10556036,  0.25547069]), 1.0, False], [array([-0.05572015, -0.04438276,  0.10556036,  0.25547069]), 0, array([-0.05660781, -0.24084109,  0.11066978,  0.579495  ]), 1.0, False], [array([-0.05660781, -0.24084109,  0.11066978,  0.579495  ]), 1, array([-0.06142463, -0.04742982,  0.12225968,  0.32362346]), 1.0, False], [array([-0.06142463, -0.04742982,  0.12225968,  0.32362346]), 0, array([-0.06237323, -0.24406128,  0.12873215,  0.65222392]), 1.0, False], [array([-0.06237323, -0.24406128,  0.12873215,  0.65222392]), 0, array([-0.06725445, -0.44071868,  0.14177663,  0.98251195]), 1.0, False], [array([-0.06725445, -0.44071868,  0.14177663,  0.98251195]), 0, array([-0.07606883, -0.63742632,  0.16142686,  1.31615574]), 1.0, False], [array([-0.07606883, -0.63742632,  0.16142686,  1.31615574]), 0, array([-0.08881735, -0.83417982,  0.18774998,  1.65470264]), 1.0, False], [array([-0.08881735, -0.83417982,  0.18774998,  1.65470264]), 0, array([-0.10550095, -1.03093142,  0.22084403,  1.99951844]), 1.0, True]]\n",
      "the win/loss history is [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "the game has lasted 23 steps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "# create lists to record all game histories and outcomes \n",
    "history = []\n",
    "winlose = []\n",
    "state = env.reset()\n",
    "while True:\n",
    "    # randomly choose an action  \n",
    "    action = env.action_space.sample()\n",
    "    # make a move\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    # recording game hisotry\n",
    "    history.append([state, action, new_state, reward, done])\n",
    "    # temporarily record the step as winning\n",
    "    winlose.append(1)\n",
    "    # prepare for the next round\n",
    "    state = new_state\n",
    "    # stop if the game is over\n",
    "    if done==True:\n",
    "        break  \n",
    "\n",
    "# The last ten steps of the game is considered not winning\n",
    "for i in range(len(history)):\n",
    "    if len(history)-i<=10 and len(history)<195:\n",
    "        winlose[i] = 0\n",
    "        \n",
    "print(\"the game history is\", history)     \n",
    "print(\"the win/loss history is\", winlose) \n",
    "print(f\"the game has lasted {len(history)} steps\") \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb762289",
   "metadata": {},
   "source": [
    "The game has lasted 23 steps. So the first 13 steps are classifed as winning steps, while the last ten are losing steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a0fe6",
   "metadata": {},
   "source": [
    "#### Simulate Many Games\n",
    "Next, we'll simulate 10,000 games and record all the intermediate steps and outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bce2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "# create lists to record all game histories and outcomes \n",
    "histories = []\n",
    "winloses = []\n",
    "# Define one_game() function\n",
    "def one_game():\n",
    "    history = []\n",
    "    winlose = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        # randomly choose an action  \n",
    "        action = env.action_space.sample()\n",
    "        # make a move\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # recording game hisotry\n",
    "        history.append([state, action, new_state, reward, done])\n",
    "        # temporarily record the step as winning\n",
    "        winlose.append(1)\n",
    "        # prepare for the next round\n",
    "        state = new_state\n",
    "        # stop if the game is over\n",
    "        if done==True:\n",
    "            break  \n",
    "    return history, winlose\n",
    "\n",
    "# Play 10,000 games\n",
    "for j in range(10000):\n",
    "    # play a game\n",
    "    history, winlose = one_game()\n",
    "    # The last ten steps of the game is considered not winning\n",
    "    for i in range(len(history)):\n",
    "        if len(history)-i<=10 and len(history)<195:\n",
    "            winlose[i] = 0\n",
    "    # record history and outcome\n",
    "    histories.append(history)\n",
    "    winloses.append(winlose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39517c4f",
   "metadata": {},
   "source": [
    "Next, we'll save the simulated data on the computer for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543e2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the simulation data on your computer\n",
    "with open('files/ch09/CartPole_games.pickle', 'wb') as fp:\n",
    "    pickle.dump((histories,winloses), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878c75d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "676b528c",
   "metadata": {},
   "source": [
    "You can load up the saved simulation data from your computer, and print out the first game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e192171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[array([-0.0299482 ,  0.01083027, -0.0244265 ,  0.0119515 ]),\n",
      "   0,\n",
      "   array([-0.0297316 , -0.18393301, -0.02418747,  0.29682859]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.0297316 , -0.18393301, -0.02418747,  0.29682859]),\n",
      "   1,\n",
      "   array([-0.03341026,  0.01152524, -0.0182509 , -0.00338346]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.03341026,  0.01152524, -0.0182509 , -0.00338346]),\n",
      "   1,\n",
      "   array([-0.03317975,  0.20690412, -0.01831857, -0.30176843]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.03317975,  0.20690412, -0.01831857, -0.30176843]),\n",
      "   0,\n",
      "   array([-0.02904167,  0.01204798, -0.02435394, -0.01491862]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.02904167,  0.01204798, -0.02435394, -0.01491862]),\n",
      "   0,\n",
      "   array([-0.02880071, -0.18271639, -0.02465231,  0.26998195]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.02880071, -0.18271639, -0.02465231,  0.26998195]),\n",
      "   0,\n",
      "   array([-0.03245504, -0.37747803, -0.01925267,  0.5547886 ]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.03245504, -0.37747803, -0.01925267,  0.5547886 ]),\n",
      "   1,\n",
      "   array([-0.0400046 , -0.18209112, -0.0081569 ,  0.2561026 ]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.0400046 , -0.18209112, -0.0081569 ,  0.2561026 ]),\n",
      "   1,\n",
      "   array([-0.04364642,  0.01314634, -0.00303485, -0.03914194]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.04364642,  0.01314634, -0.00303485, -0.03914194]),\n",
      "   0,\n",
      "   array([-0.04338349, -0.18193196, -0.00381769,  0.25258192]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.04338349, -0.18193196, -0.00381769,  0.25258192]),\n",
      "   0,\n",
      "   array([-0.04702213, -0.37699919,  0.00123395,  0.54405823]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.04702213, -0.37699919,  0.00123395,  0.54405823]),\n",
      "   1,\n",
      "   array([-0.05456212, -0.1818946 ,  0.01211512,  0.25176436]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.05456212, -0.1818946 ,  0.01211512,  0.25176436]),\n",
      "   0,\n",
      "   array([-0.05820001, -0.37718744,  0.0171504 ,  0.54824387]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.05820001, -0.37718744,  0.0171504 ,  0.54824387]),\n",
      "   0,\n",
      "   array([-0.06574376, -0.57254607,  0.02811528,  0.84628069]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.06574376, -0.57254607,  0.02811528,  0.84628069]),\n",
      "   1,\n",
      "   array([-0.07719468, -0.37781874,  0.04504089,  0.56256994]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.07719468, -0.37781874,  0.04504089,  0.56256994]),\n",
      "   1,\n",
      "   array([-0.08475106, -0.18335681,  0.05629229,  0.28441042]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.08475106, -0.18335681,  0.05629229,  0.28441042]),\n",
      "   0,\n",
      "   array([-0.08841819, -0.37923456,  0.0619805 ,  0.59430283]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.08841819, -0.37923456,  0.0619805 ,  0.59430283]),\n",
      "   0,\n",
      "   array([-0.09600288, -0.57516678,  0.07386656,  0.90584743]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.09600288, -0.57516678,  0.07386656,  0.90584743]),\n",
      "   0,\n",
      "   array([-0.10750622, -0.77120703,  0.09198351,  1.22080296]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.10750622, -0.77120703,  0.09198351,  1.22080296]),\n",
      "   1,\n",
      "   array([-0.12293036, -0.57738293,  0.11639957,  0.95830093]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.12293036, -0.57738293,  0.11639957,  0.95830093]),\n",
      "   0,\n",
      "   array([-0.13447802, -0.77386123,  0.13556558,  1.28516833]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.13447802, -0.77386123,  0.13556558,  1.28516833]),\n",
      "   0,\n",
      "   array([-0.14995524, -0.97042317,  0.16126895,  1.61704039]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.14995524, -0.97042317,  0.16126895,  1.61704039]),\n",
      "   0,\n",
      "   array([-0.16936371, -1.16703751,  0.19360976,  1.95534292]),\n",
      "   1.0,\n",
      "   False],\n",
      "  [array([-0.16936371, -1.16703751,  0.19360976,  1.95534292]),\n",
      "   0,\n",
      "   array([-0.19270446, -1.36361709,  0.23271662,  2.30126929]),\n",
      "   1.0,\n",
      "   True]]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# read the data and print out the first 10 games\n",
    "with open('files/ch09/CartPole_games.pickle', 'rb') as fp:\n",
    "    histories, outcomes=pickle.load(fp)\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(histories[:1])    \n",
    "pprint(outcomes[:1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd249d",
   "metadata": {},
   "source": [
    "Next we'll train the deep neural network using the simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Game Strategies: Train the Deep Neural Network\n",
    "\n",
    "We'll train the deep neural network so that it can learn from the simulated data. To do that, we'll first prepare the data so that we can feed them into the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7aaab",
   "metadata": {},
   "source": [
    "### 3.1. Preparing the Data\n",
    "\n",
    "We'll create X and y for each game step and put them together for training. \n",
    "\n",
    "Since the outcome variable is either 1 or 0, this is similar to a binary classification problem. \n",
    "\n",
    "What we are doing here is to associate each state and action combination (s,a) with the outcome 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6879fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.0299482 ,  0.01083027, -0.0244265 ,  0.0119515 ,  0.        ]),\n",
      " array([-0.0297316 , -0.18393301, -0.02418747,  0.29682859,  1.        ]),\n",
      " array([-0.03341026,  0.01152524, -0.0182509 , -0.00338346,  1.        ]),\n",
      " array([-0.03317975,  0.20690412, -0.01831857, -0.30176843,  0.        ]),\n",
      " array([-0.02904167,  0.01204798, -0.02435394, -0.01491862,  0.        ])]\n",
      "[1, 1, 1, 1, 1]\n",
      "0.5520433080762739\n"
     ]
    }
   ],
   "source": [
    "# Create empty X and y lists\n",
    "X, y = [], []    \n",
    "# Create X and y for each game step\n",
    "for history, winlose in zip(histories, outcomes):\n",
    "    for i in range(len(history)):\n",
    "        state, action, new_state, reward, done = history[i]\n",
    "        s = np.array(state).reshape((4,1))\n",
    "        a = np.array(action).reshape((1,1))\n",
    "        sa = np.concatenate([s, a], axis = 0)\n",
    "        # Each observation of X is a (state, action) combination\n",
    "        X.append(sa.reshape(-1,))\n",
    "        # Each y is the outcome for the state action combination\n",
    "        y.append(winlose[i])\n",
    "        \n",
    "# Print the first five observations of X and y\n",
    "from pprint import pprint\n",
    "pprint(X[:5])\n",
    "pprint(y[:5])\n",
    "# See what's the average value of y\n",
    "print(np.mean(np.array(y).reshape(-1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab6eaf",
   "metadata": {},
   "source": [
    "The output above shows that the averge value of y is about 0.552. This means 55.2% of steps are wins and the rest 44.85 are losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ddcfe",
   "metadata": {},
   "source": [
    "Finally we save the processed data for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df12a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed data on your computer\n",
    "with open('files/ch09/CartPoleXy.pickle', 'wb') as fp:\n",
    "    pickle.dump((X,y),fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d794163",
   "metadata": {},
   "source": [
    "### 3.2. Train the Deep Neural Network\n",
    "Now that the dataset is ready, we are ready to train our deep neural network. \n",
    "\n",
    "Here we are essentially performing a binary classification. We classify each state-action pair into win or lose. The output layer has one neuron with sigmoid activation. So we can think of the output as the probability of winning the game. \n",
    "\n",
    "There are two hidden layers in the model, with 64 and 16 neurons, respectively. But we do have a lot of freedom here. More layers with various numbers of neurons will generate similar results. \n",
    "\n",
    "Later, we'll use the trained model to play the Cart Pole game. When playing, at each state, we'll ask the following question:\n",
    "1. If I choose action 0 (i.e., move left), what's the probability of winning the game? We'll combine the current state and action 0 and feed this state-action pair to the trained deep neural network and get a probability; let's call it p(win|s,a0).\n",
    "2. If I choose action 1 (i.e., move right), what's the probability of winning the game? We'll use the trained neural network and get p(win|s,a1).\n",
    "We then compare p(win|s,a0) with p(win|s,a1) and pick the action that leads to the higher p(win|s,a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# load the data       \n",
    "with open('files/ch09/CartPoleXy.pickle', 'rb') as fp:\n",
    "    X, y = pickle.load(fp)\n",
    "\n",
    "X = np.array(X).reshape((-1, 5))\n",
    "y = np.array(y).reshape((-1, 1))\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(5,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model.fit(X,y, epochs=50)\n",
    "\n",
    "model.save('files/ch09/trained_cartpole.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea611d",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can use it to play the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6011a93",
   "metadata": {},
   "source": [
    "## 4. Play the Game with the Trained Model\n",
    "To play the game with the trained model, we'll look at the state at each move, and hypothetically take actions 0 and 1. Then we use the trained model to predict the probability of winning with each of the two state-action pairs. We'll pick the action that leads to the higher probability of winning. We repeat at each step until the game ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eafae1ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlliu2\\Anaconda3\\envs\\animatedML\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "score is 200\n"
     ]
    }
   ],
   "source": [
    "# import needed modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "reload = tf.keras.models.load_model(\"files/ch09/trained_cartpole.h5\")\n",
    "\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(1,201):\n",
    "    # Save the screen for later use\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    # Use the trained model to predict the prob of winning \n",
    "    s = np.array(state).reshape((4,1))\n",
    "    a0 = np.array([0]).reshape((1,1))\n",
    "    a1 = np.array([1]).reshape((1,1))    \n",
    "    sa0 = np.concatenate([s, a0], axis = 0).reshape(-1,5)\n",
    "    sa1 = np.concatenate([s, a1], axis = 0).reshape(-1,5)\n",
    "    sa = np.concatenate([sa0, sa1], axis=0).reshape(-1,5)\n",
    "    # pick the action with higher probability of winning\n",
    "    action = np.argmax(reload.predict(sa))\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    state = new_state\n",
    "    if done == True:\n",
    "        print(f\"score is {i}\")\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "The cart pole stays upright for all 200 steps. So the deep learning game strategy works really well!!!\n",
    "\n",
    "Next, we'll create an animation to compare two games: one with random moves, and the other with deep learning game strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48956fd",
   "metadata": {},
   "source": [
    "## 5. Compare Two Games\n",
    "We have alredy recorded all the frames in a game with deep learning strategies. \n",
    "\n",
    "Next, we'll record all the grames in a game with random moves. We'll then put the frames side by side with frames from the game with deep learning strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655b167",
   "metadata": {},
   "source": [
    "### 5.1. Record A Game with Random Moves\n",
    "We will make random moves, and play the Cart Pole game for 200 steps. We then record all frames in a list called *random_frames*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc24e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlliu2\\Anaconda3\\envs\\animatedML\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "\n",
    "random_frames=[]\n",
    "# Record 200 frames\n",
    "for i in range(1,201):\n",
    "    random_frames.append(env.render(mode='rgb_array'))\n",
    "    # pick a random action \n",
    "    action = env.action_space.sample()\n",
    "    # play the game\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    state = new_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3467a2be",
   "metadata": {},
   "source": [
    "We have now recorded 200 frames in a random-move game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afdb60c",
   "metadata": {},
   "source": [
    "### 5.2. How to Combine Frames without Saving Them\n",
    "Each Cart Pole game has 200 frames. We want to combine the frames from two games in each step. If we have to save each combined frame, that would be too many pictures and take too much space on your computer. \n",
    "\n",
    "Instead, we'll find a way to combine two frames without saving them. \n",
    "\n",
    "Here is how you can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1a6905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2400x1000 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a list to store the combined frames\n",
    "combined = []\n",
    "\n",
    "fig = plt.figure(figsize=(24,10), dpi=100)\n",
    "ax = fig.add_subplot(111) \n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(-5,5)\n",
    "plt.axis(\"off\")\n",
    "ax.text(-9,4,\"The Cart Pole Game with Random Moves, Step 1\",fontsize=20)\n",
    "ax.text(1,4,\"The Cart Pole Game with Deep Learning, Step 1\",fontsize=20)\n",
    "# Add the frame from the random-move game to the left\n",
    "newax = fig.add_axes([0.05, 0.2, 0.55, 0.55])\n",
    "newax.imshow(random_frames[0])\n",
    "newax.axis('off')    \n",
    "\n",
    "# Add the frame from the deep-learning game to the right\n",
    "newax2 = fig.add_axes([0.425, 0.2, 0.55, 0.55])\n",
    "newax2.imshow(frames[0])\n",
    "newax2.axis('off') \n",
    "# plot the picture \n",
    "fig.canvas.draw()\n",
    "# Now we can save it to a numpy array.\n",
    "twoframes = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "twoframes = twoframes.reshape(fig.canvas.get_width_height()[::-1]+ (3,))\n",
    "combined.append(twoframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84925bed",
   "metadata": {},
   "source": [
    "The combined frame is shown above. \n",
    "\n",
    "More important, we can now save the combined frame as a numpy array in a list temporarily without saving on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6f6e2",
   "metadata": {},
   "source": [
    "### 5.3. Combine Frames in Each Step\n",
    "Next, we'll combine the two frames in each of the 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec0f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the combined frames\n",
    "combined = []\n",
    "\n",
    "for step in range(0,200):\n",
    "      \n",
    "    fig = plt.figure(figsize=(24,10), dpi=100)\n",
    "    ax = fig.add_subplot(111) \n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-5,5)\n",
    "    #plt.grid()\n",
    "    plt.axis(\"off\")\n",
    "    ax.text(-9,4,f\"The Cart Pole Game with Random Moves, Step {step+1}\",fontsize=20)\n",
    "    ax.text(1,4,f\"The Cart Pole Game with Deep Learning, Step {step+1}\",fontsize=20)\n",
    "    # Add the frame from the random-move game to the left\n",
    "    newax = fig.add_axes([0.05, 0.2, 0.55, 0.55])\n",
    "    newax.imshow(random_frames[step])\n",
    "    newax.axis('off')    \n",
    "\n",
    "    # Add the frame from the deep-learning game to the right   \n",
    "    newax2 = fig.add_axes([0.425, 0.2, 0.55, 0.55])\n",
    "    newax2.imshow(frames[step])\n",
    "    newax2.axis('off')\n",
    "\n",
    "    # plot the picture \n",
    "    fig.canvas.draw()\n",
    "    # Now we can save it to a numpy array.\n",
    "    twoframes = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    twoframes = twoframes.reshape(fig.canvas.get_width_height()[::-1]+ (3,))\n",
    "    combined.append(twoframes)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c69a94",
   "metadata": {},
   "source": [
    "We now have 200 combined frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2023e7e1",
   "metadata": {},
   "source": [
    "### 5.4. Create An Animation of Combined Frames\n",
    "We'll use the ***imageio*** library to convert the 200 combined frames into an animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3a9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "imageio.mimsave('compare_cartpole.gif', combined, fps=12) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c728bb",
   "metadata": {},
   "source": [
    "The animation looks as follows:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/compare_cartpole.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069fc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
