{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 13: Introduction to Reinforcement Learning\n",
    "\n",
    "In this chapter, you’ll learn how reinforcement learning works.\n",
    "\n",
    "After this chapter, you'll be able to create an animation to show in the Frozen Lake game how the Q-learning works in each step of the game. In particular, in each state, you'll put the game board on the left and the Q-table on the right. You'll highlight the row corresponding to the state and compare the Q-values under the four actions. You'll then highlight the best action in red. Like so: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_q_steps.gif\"/>\n",
    "\n",
    "We’ll use the Frozenlake game in OpenAI Gym to illustrate the concept of dynamic programming, Bellman equation, and how to implement Q learning.\n",
    "\n",
    "Machine learning can be classified into three different areas: Supervised learning, unsupervised learning, and reinforcement learning. \n",
    "\n",
    "In supervised learning, we show the machine learning models many examples of input-output pairs. The output values are also called target variables (or labels). The model extracts features from the input data (e.g., images) and associate them with the output (image labels such as horses, deer, cats, or dogs). We then apply the trained model on new examples, and make predictions on what the output should be (is the image a horse or a deer). \n",
    "\n",
    "In the previous chapters, we have discussed deep neural networks, which are examples of supervised learning.\n",
    "\n",
    "In contrast, in unsupervised learning, there are no pre-assigned target variables (labels) for the training data. The unsupervised learning models must find naturally-occurring patterns from the training data. Examples of unsupervised learning methods include clustering, principal component analysis, and data visualization (plotting, graphing, and so on). \n",
    "\n",
    "In reinforcement learning, an agent operates in an environment through trial and error. The agent learns to achieve the optimal outcome by receiving feedback from the environment in the form of rewards. For the rest of the book, we’ll discuss various types of reinforcement learning methods, which include tabular Q learning, deep Q learning, policy gradients, and double deep Q learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 13}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 13 in a subfolder /files/ch13. The code in the cell below will create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch13\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. Basics of Reinforcement Learning\n",
    "Reinforcement Learning (RL) is one type of Machine Learning (ML). In a typical RL problem, an agent decides how to choose among a list of actions in an environment step by step in order to maximize the cumulative payoff from all the steps that he or she has taken. \n",
    "\n",
    "RL is widely used in many different fields, from control theory, operations research to statistics. The optimal actions are solved by using a Markov Decision Process (MDP). The agent uses trial and error to interact with the environment and to see what rewards from those actions are. The agent then adjusts the decision based on the outcome: rewarding good choices and penalizing bad ones. Hence the name reinforcement learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "### 1.1. Basic Concepts\n",
    "Let’s first discuss a few basic concepts related to RL: environment, agent, state, action, reward.\n",
    " \n",
    "* Environment: the world in which agent(s) live and interact with each other or with nature. More important, an environment is where the agent(s) can explore and learn the best strategies. Examples include the Frozen Lake game, the popular Breakout Atari game, or a real-world problem that we need to solve. \n",
    "* Agent: the player of the game. In most games, there is one player and the opponent is enbedded into the environment. But you have seen two-player games such as Tic Tac Toe or Connect Four earlier in this book. \n",
    "* State: the current situation of the game. The current game board in the Connect Four game, for example, is the current state of the game.\n",
    "* Action: what the player decides to do given the current game situation. \n",
    "* Reward: the payoff from the game. You can assign a value to each situation. Positive values are rewards and negative values penalties.\n",
    "\n",
    "These concepts will become clearer as we move along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa92c7",
   "metadata": {},
   "source": [
    "### 1.2. The Bellman Equation and Q-Learning\n",
    "Q-learning is one way to solve the optimization problem in RL. Q learning is a value-based approach. Another approach is policy gradients, which is a policy-based approach. We’ll discuss both in this book.\n",
    "\n",
    "The agent is trying to learn the best strategy in order to maximize his or her expected payoff over time. A strategy (also called a policy) maps a certain state to a certain action. A strategy is basically a decision rule that tells the agent what to do in a certain situation.\n",
    "\n",
    "The Q-value, $Q(s, a)$, measures how good a strategy is. You can interpret the letter Q as quality. The better the strategy, the higher the payoff to the agent, and the higher the Q-value. The agent is trying to find the best strategy that maximizes the Q value.\n",
    "\n",
    "An agent’s action now not only affects the reward in this period, but also rewards in future periods. Therefore, finding the best strategy can be complicated and involves dynamic programming. For details of daynamic programming and the Bellan equation, see, e.g., https://en.wikipedia.org/wiki/Bellman_equation\n",
    "\n",
    "In the setting of Q-learning, the Bellman equation is as follows:\n",
    "$$Q(s,a) = Reward + Discount\\ Factor * max\\ Q(s’, a’)$$\n",
    "where $Q(s, a)$ is the Q value to the agent in the current state $s$ when an action $a$ is taken. Reward is the payoff to the agent as a result of this action. Discount factor is a constant between 0 and 1, and it measures how much the agent discounts future reward as opposed to current reward. Lastly, $max Q(s’, a’)$ is the maximum future reward, assuming optimal strategies will be applied in the future as well. \n",
    "\n",
    "In order to find out the Q values, we’ll try different actions in each state multiple times. We’ll adjust the Q values based on the outcome, increase the Q value if the reward is high and decrease the Q value if the reward is low or even negative. Hence the name reinforecement learing.\n",
    "\n",
    "Rather than providing you with a lot of abstract technical jargon, I will use a simple example to show you how reinforcement learning works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 2. Use Q Values to Play the Frozen Lake Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "You have learned how to play the Frozen Lake game using deep learning in Chapter 8. So I assume you know how the game works. If not, check Chapter 8 for details. \n",
    "\n",
    "The OpenAI Gym environement is designed for training RL game strategies. In particular, in this chapter, you'll learn how to use Q-learning to play the Frozen Lake game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312e9d5",
   "metadata": {},
   "source": [
    "### 2.1. The Logic Behind Q Learning\n",
    "What if you have a Q-table to guide you to successfully play the Frozen Lake game? The Q-table is a 16 by 4 matrix, with the rows representing the 16 states: 0 means the top left corner (that is, the starting position), 3 means the top right corner, and 15 means the bottom right corner (that is, the goal position; i.e., the winning position). The four columns represent the four actions that the agent can take in any state: 0 means going left, 1 going down, 2 going right, and 3 going up.\n",
    "\n",
    "The Q table can be downloaded from my website https://gattonweb.uky.edu/faculty/lium/ml/Qtable.csv\n",
    "\n",
    "<br> If you open the file, you'll see a table as follows (I added two rows and one column for explanation purpose):\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/Qtable.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "With the guidance of the Q-table, reaching the destination (i.e., state 15, or the lower right corner) safely is easy for a computer program. Here are the steps:\n",
    "\n",
    "1.\tThe computer starts at state 0.\n",
    "2.\tIt looks at the above Q table and consults the row corresponding to state 0, which has four values: 0.531, 0.59, 0.59, and 0.531. The four values tell the computer the total payoff from taking the four actions in state 0. \n",
    "3.\tThe computer chooses the action that leads to the highest Q value: taking actions 1 or 2 both have a payoff of 0.59, higher than those form taking actions 0 or 3. We have a tie here, so the computer chooses action 1 (that is, going down) in this case (the first in the two tied actions, 1 and 2). \n",
    "4.\tSince the computer has chosen going down in state 0, the new state is now the first column in the second row based on the map of the frozen lake. Therefore, the new state is state 4.\n",
    "5.\tThe computer now chooses the best action in state 4 based on the above Q table, following the same logic in steps 2 and 3. This means the computer takes action 1 again.\n",
    "6.\tThe computer repeats the above steps until the game ends (that is, either the agent falls into a hole or reaches the destination).\n",
    "\n",
    "Based on the numbers in the Q table and the logic in the above steps, the computer will take the following actions sequentially: down, down, right, down, right, and right. It will pass the following states: state 0 to state 4 to state 8 to state 9 to state 13 to state 14 to state 15. \n",
    "\n",
    "As you can see, the computer has successfully reached the goal (state 15) without falling into one of the four holes (states 5, 7, 11, and 12).\n",
    "\n",
    "We’ll code that in next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c4b83",
   "metadata": {},
   "source": [
    "### 2.2. A Python Script to Win the Frozenlake Game\n",
    "\n",
    "Run the following short script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052e7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state is 0 and the action is 1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state is 4 and the action is 1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "current state is 8 and the action is 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "current state is 9 and the action is 1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "current state is 13 and the action is 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "current state is 14 and the action is 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Congratulations, you have reached the destination!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()\n",
    "\n",
    "Q = np.loadtxt(r'https://gattonweb.uky.edu/faculty/lium/ml/Qtable.csv', delimiter=\",\")\n",
    "\n",
    "def play_game():\n",
    "    state=env.reset()\n",
    "    env.render()\n",
    "    while True:\n",
    "        action = np.argmax(Q[state, :])\n",
    "        print(f'current state is {state} and the action is {action}')\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # new_state becomes the state in the next round\n",
    "        state=new_state\n",
    "        env.render()\n",
    "        if done==True:\n",
    "            if reward ==1:\n",
    "                print('Congratulations, you have reached the destination!')\n",
    "            else:\n",
    "                print('Sorry, better luck next time.')\n",
    "            break    \n",
    "\n",
    "play_game()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The Q values are saved in a CSV file. We use the *loadtxt()* method in ***numpy*** to load up the Q table.\n",
    "\n",
    "The state=env.reset() command resets the game so that the initial state is 0. \n",
    "\n",
    "We then use a while loop to play the game. In each iteration, the computer chooses the action that leads to the highest Q value in that state. Note here the *argmax()* method in ***numpy*** returns the argument that leads to the highest value. This is different from the *max()* method in ***numpy***, which returns the highest value among a group of values. \n",
    "\n",
    "We then print out the current state, and the action taken by the computer in that state so that we can keep track of the path taken by the computer.\n",
    "\n",
    "The *env.step()* method returns the new state and the reward based on the action taken. It also tells us whether the game has ended. If the game has not ended, we set the new state as the current state and go to the next iteration to repeat the process. If the game has ended, the while loop stops and the script ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "The computer has successfully reached state 15, taking the shortest possible path. \n",
    "\n",
    "You can run the script multiple times, and the output will be the same every time, because there is no randomness involved.\n",
    "\n",
    "Amazing, right? You may wonder, how did you come up with the numbers in the Q-table to play the game? That’s what we’ll discuss next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064889bb",
   "metadata": {},
   "source": [
    "## 3. Training the Q-Values\n",
    "In this section, we’ll first discuss what is Q-learning and the logic behind it. We then code in the logic and use a script to generate the Q values that we have just used in the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba3931",
   "metadata": {},
   "source": [
    "### 3.1. What Is Q-Learning?\n",
    "The Q-values form a table of S rows and A columns, and we call it the Q-table. We need to find out the Q-values so that the player can use these values to figure out the optimal strategies in every situation. \n",
    "\n",
    "Before Q-learning starts, we set all the values in the Q-table as 0.\n",
    "\n",
    "At each iteration, we’ll use reinforcement learning (Q-learning, to be exact) to update Q values as follows:\n",
    "\n",
    " $$ New\\ Q(s,a) = learning\\ rate * [Reward + discount\\ factor * max\\  Q(s’, a’)]+ (1-learning\\ rate) * Old\\  Q(s, a)$$\n",
    "Here the learning rate, which has a value between 0 and 1, is how fast you update the Q values. The updated $Q(s, a)$ is a weighted average of the Q value based on that obtained from the Bellman’s equation and the previous $Q(s, a)$. Here is when updating (i.e., learning) happens.\n",
    "\n",
    "After many rounds of trial and error, the update will be minimal, which means the Q values converge to the equilibrium value. \n",
    "\n",
    "If you look at the above equation, when \n",
    "$$Q(s,a) = Reward + discount\\ factor * max\\ Q(s’, a’)$$\n",
    "There is no update, and we have \n",
    "$$New\\  Q(s,a) = Old\\  Q(s, a)$$\n",
    "And that is the equilibrium state we are looking for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334083a",
   "metadata": {},
   "source": [
    "### 3.2. Let the Learning Begin\n",
    "We’ll write a Python script and let the agent randomly select moves to play the game many rounds. Unavoidably, there will be many mistakes along the way. But we’ll assign a low reward if the agent fails so that it assigns a low Q value to actions taken in that state. On the other hand, if the agent makes right choices and successfully reaches the destination, we’ll assign a high reward so that the agent assigns high Q values to actions taken. \n",
    "\n",
    "It’s through such repeated rewards and punishments that the agent learns the correct Q values.\n",
    "\n",
    "The script in the following cell trains the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea32e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.531441 0.59049  0.59049  0.531441]\n",
      " [0.531441 0.       0.6561   0.59049 ]\n",
      " [0.59049  0.729    0.59049  0.6561  ]\n",
      " [0.6561   0.       0.59049  0.59049 ]\n",
      " [0.59049  0.6561   0.       0.531441]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.       0.81     0.       0.6561  ]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.6561   0.       0.729    0.59049 ]\n",
      " [0.6561   0.81     0.81     0.      ]\n",
      " [0.729    0.9      0.       0.729   ]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.       0.       0.       0.      ]\n",
      " [0.       0.81     0.9      0.729   ]\n",
      " [0.81     0.9      1.       0.81    ]\n",
      " [0.       0.       0.       0.      ]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()\n",
    "\n",
    "learning_rate=0.6\n",
    "discount_rate=0.9\n",
    "\n",
    "max_exp=0.7\n",
    "min_exp=0.3\n",
    "max_steps=50\n",
    "max_episode=10000\n",
    "\n",
    "Q = np.zeros((16, 4))\n",
    "\n",
    "def update_Q(episode):\n",
    "    # The initial state is the starting position (state 0)\n",
    "    state=env.reset()      \n",
    "    # Play a full game till it ends\n",
    "    for _ in range(max_steps):\n",
    "        # Select the best action or the random action\n",
    "        if np.random.uniform(0,1,1)>min_exp+(max_exp-min_exp)*episode/max_episode:\n",
    "            action = np.argmax(Q[state, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        # Use the selected action to make the move\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # Update Q values\n",
    "        if done==True:\n",
    "            Q[state, action] = reward\n",
    "            break    \n",
    "        else:\n",
    "            Q[state, action] = learning_rate*(reward+discount_rate*np.max(Q[new_state, :]))\\\n",
    "                + (1-learning_rate)*Q[state, action]\n",
    "            state=new_state    \n",
    "       \n",
    "for episode in range(max_episode):\n",
    "    update_Q(episode)\n",
    "    \n",
    "print(Q) \n",
    "\n",
    "# Save the trained Q for later use\n",
    "np.savetxt(\"files/ch13/trained_frozenlake_Qs.csv\", Q, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b528f6d",
   "metadata": {},
   "source": [
    "We set the learning rate to 0.6. This value can take different values in this simple case. You can set it to a much smaller value such as 0.1, as long as you train the model many times, the Q values will converge. \n",
    "\n",
    "The discount rate we use here is 0.9. This value will directly affect the Q values. Remember in the last subsection, we discussed that in equilibrium when the Q values converge, we have \n",
    "$$Q(s,a) = Reward + discount\\ factor * max\\ Q(s’, a’)$$\n",
    "You can see that the converged Q value is a function of discount factor. In our case, as long as you set it anywhere between 0.9 and 1, the resulting Q values will work in the sense that it will successfully guide the agent to the destination safely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433fc0b",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Exploration versus Exploitation}}$<br>\n",
    "***\n",
    "Another important parameter in the process of training the Q-tables is the exploration rate. Exploration means that the agent will randomly selects an action in that given state. This is important for training the Q values because without it, the Q values may be trapped in the wrong equilibrium and cannot get out of it. With exploration, it gives the agent the chance to explore new strategies and see if they lead to higher Q values. \n",
    "\n",
    "Exploitation is the opposite of exploration: it means the agent chooses the action based on the values in the current Q-table. This ensures that the final Q table converges. \n",
    "\n",
    "At each iteration, the Q values are updated. If the agent wins the game, the action earns a reward of 1. If the agent fails, the action earns a reward of -1. The update rule follows the equation we specified earlier \n",
    "$$New\\ Q(s,a) = learning\\ rate * [Reward + discount\\ factor * max\\ Q(s’, a’)]\n",
    "                        + (1-learning\\ rate) * Old\\ Q(s, a)$$\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5412a83",
   "metadata": {},
   "source": [
    "## 4. Test the Trained Q-Values\n",
    "Now, you can test if the trained Q-table works or not. You'll first use the OpenAI Gym environement to test it. You'll then use the self-made Frozen Lake game environement to test the Q-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e5ff8e",
   "metadata": {},
   "source": [
    "### 4.1. Test in the OpenAI Gym environment\n",
    "The following script is the same as you just ran in Section 2.2, except that you are using your own trained Q-table, instead of a Q-table provided by me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72640a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state is 0 and the action is 1\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state is 4 and the action is 1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "current state is 8 and the action is 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "current state is 9 and the action is 1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "current state is 13 and the action is 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "current state is 14 and the action is 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Congratulations, you have reached the destination!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()\n",
    "\n",
    "# Use the Q-table you just trained\n",
    "Q = np.loadtxt('files/ch13/trained_frozenlake_Qs.csv', delimiter=\",\")\n",
    "\n",
    "def play_game():\n",
    "    state=env.reset()\n",
    "    env.render()\n",
    "    while True:\n",
    "        action = np.argmax(Q[state, :])\n",
    "        print(f'current state is {state} and the action is {action}')\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # new_state becomes the state in the next round\n",
    "        state=new_state\n",
    "        env.render()\n",
    "        if done==True:\n",
    "            if reward ==1:\n",
    "                print('Congratulations, you have reached the destination!')\n",
    "            else:\n",
    "                print('Sorry, better luck next time.')\n",
    "            break    \n",
    "\n",
    "play_game()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ce5dd",
   "metadata": {},
   "source": [
    "You get exactly the same results as those in Section 2.2. So our training of the Q-table works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb298811",
   "metadata": {},
   "source": [
    "### 4.2. Apply the Q-Table in the Self-Made Game Environment\n",
    "You leaned how to create a game environment from scratch in Chapter 10. Recall that the custom-made game environment works exactly the same as the OpenAI gym environment, plus a graphical rendering instead of a print-out rendering. Therefore, the Q-table should work in the custom-made environment as well. \n",
    "\n",
    "Let's check out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac17214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state is 0 and the action is 1\n",
      "current state is 4 and the action is 1\n",
      "current state is 8 and the action is 2\n",
      "current state is 9 and the action is 1\n",
      "current state is 13 and the action is 2\n",
      "current state is 14 and the action is 2\n",
      "Congratulations, you have reached the destination!\n"
     ]
    }
   ],
   "source": [
    "from utils.frozenlake_env import Frozen\n",
    "import turtle as t\n",
    "import time\n",
    "\n",
    "env = Frozen()\n",
    "\n",
    "# Use the Q-table you just trained\n",
    "Q = np.loadtxt('files/ch13/trained_frozenlake_Qs.csv', delimiter=\",\")\n",
    "\n",
    "def play_game():\n",
    "    state=env.reset()\n",
    "    env.render()\n",
    "    while True:\n",
    "        # Slow down the game so that you can see the graphical rendering\n",
    "        time.sleep(1)\n",
    "        action = np.argmax(Q[state, :])\n",
    "        print(f'current state is {state} and the action is {action}')\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # new_state becomes the state in the next round\n",
    "        state=new_state\n",
    "        env.render()\n",
    "        if done==True:\n",
    "            if reward ==1:\n",
    "                print('Congratulations, you have reached the destination!')\n",
    "            else:\n",
    "                print('Sorry, better luck next time.')\n",
    "            break    \n",
    "\n",
    "play_game()\n",
    "time.sleep(5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "The trained Q-table guided the program to choose the following actions: 1, 1, 2, 1, 2, 2 (down, down, right, down, right, right) and successfully reached the destination. This is one of the shortest paths that can win the game.\n",
    "\n",
    "There is also game board in each step, showing you where the player is. At the end of the game, the graphical rendering looks as follows:<br>\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen6.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "## 5. Animate the Q-Learning Process\n",
    "In this section, we'll create an animation to show how the agent makes a decision by consulting the Q-table at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2dd43",
   "metadata": {},
   "source": [
    "### 5.1. Draw the Q-table and Highlight Values and Actions\n",
    "We'll first draw a Q-table. At each step, we'll highlight the corresponding row in blue based on which state the agent is in. We'll then hihglight in red the action with the highest Q-value in that row, and use that as the best action. We'll repeat this step by step until the game ends. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0df744",
   "metadata": {},
   "source": [
    "For that purpose, we create a list *states* to contain all the states that the agent has been to; we also create a list *actions* to contain all actions taken by the agent,  as follows\n",
    "\n",
    "```python\n",
    "states = [0,4,8,9,13,14]\n",
    "actions = [1,1,2,1,2,2]\n",
    "```\n",
    "\n",
    "In each state, we draw three pictures: picture a is the Q table; picture b is the Q table with the line corresponding to the current state highlighted in blue; picture c is the Q table with the action corresponding to the highest Q-value highlighted in red. \n",
    "\n",
    "The code in the cell below accomplishes the above tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea53d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Use the Q-table you just trained\n",
    "Q = np.loadtxt('files/ch13/trained_frozenlake_Qs.csv', delimiter=\",\")\n",
    "\n",
    "# states and actions in each step\n",
    "states = [0,4,8,9,13,14]\n",
    "actions = [1,1,2,1,2,2]\n",
    "xs = [0,0,2,0,2,2]\n",
    "ys = [6,2,-2,-3,-7,-8]\n",
    "\n",
    "for stepi in range(6):\n",
    "    fig, ax=plt.subplots(figsize=(10,9), dpi=200)\n",
    "    # table grid\n",
    "    for x in range(-2,4,1):\n",
    "        ax.plot([2*x,2*x],[-4.5,4.5],color='gray',linewidth=3)\n",
    "    for y in range(-9,10,1):\n",
    "        if y != 8:\n",
    "            ax.plot([-6,6],[y/2,y/2],color='gray',linewidth=3)\n",
    "    \n",
    "    # four actions and 16 states\n",
    "    plt.text(-3.5, 8.1/2, \"action=\",fontsize=18)\n",
    "    for i in range(16):\n",
    "        plt.text(-3.5, (6.1-i)/2, f\"state {i}\",fontsize=18)\n",
    "    actions = [\"left\", \"down\", \"right\", \"up\"]\n",
    "    for i in range(4):\n",
    "        plt.text(-1.3+2*i, 8.1/2, f\"{i}\",fontsize=18)\n",
    "        plt.text(-1.5+2*i, 7.2/2, f\"{actions[i]}\",fontsize=18)\n",
    "    # write the 64 Q-values onto the graph\n",
    "    for i in range(16):\n",
    "        for j in range(4):\n",
    "            plt.text(-1.8+2*j, (6.2-i)/2, f\"{Q[i,j]:.3f}\",fontsize=18)\n",
    "    \n",
    "    ax.set_xlim(-4,6)\n",
    "    ax.set_ylim(-4.5,4.5)\n",
    "    plt.savefig(\"files/ch13/qtableplt.png\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"files/ch13/plt_Qs_stepa{stepi}.png\")\n",
    "  \n",
    "    # highlight state row\n",
    "    ax.add_patch(Rectangle((-4,ys[stepi]/2), 12,0.5,\n",
    "                 facecolor = 'b',alpha=0.2))\n",
    "    plt.savefig(f\"files/ch13/plt_Qs_stepb{stepi}.png\")\n",
    "    # highlight action cell\n",
    "    ax.add_patch(Rectangle((xs[stepi], ys[stepi]/2), 2,0.5,facecolor='r',alpha=0.8))\n",
    "    plt.savefig(f\"files/ch13/plt_Qs_stepc{stepi}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb762289",
   "metadata": {},
   "source": [
    "If you open, for example, the picture plt_Qs_stepc2.png, you'll see the following: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/plt_Qs_stepc2.png\"/>\n",
    "\n",
    "In step 3, the agent is in state 8. Therefore, the row corresponding to state 8 in the Q-table is highlighted in light blue. The agent compares the four Q-values under the four actions. The values are 0.656, 0.000, 0.729, 0.590, respectively. Obviously, the Q-value under action=2 is the largest among the four numbers. Therefore, the agent chooses action 2 in this state. You can see that the number 0.729 is highlighted in red in the above picture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a0fe6",
   "metadata": {},
   "source": [
    "### 5.2. Animate the Use of the Q-Table\n",
    "Next, you'll combine the pictures created in the last subsection into an animation. As a result, you'll see step by step how the best actions were taken with the guidance of the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4deaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle as t\n",
    "import time\n",
    "import random\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "\n",
    "frames=[]\n",
    "\n",
    "for i in range(6):\n",
    "    for letter in [\"a\", \"b\", \"c\"]:\n",
    "        im = Image.open(f\"files/ch13/plt_Qs_step{letter}{i}.png\")\n",
    "        f1=np.asarray(im)\n",
    "        frames.append(f1)\n",
    "imageio.mimsave('files/ch13/plt_Qs_steps.gif', frames, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea21ada",
   "metadata": {},
   "source": [
    "If you open the file plt_Qs_steps.gif, you'll see the following: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/plt_Qs_steps.gif\"/>\n",
    "\n",
    "In each state, you see three frames: the Q-table, the Q-table with the row correspoding to the current state highlighted in blue, and Q-table with the best action highlighted in red. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "### 5.3. Animate Game Board Positions and the Best Actions\n",
    "We'll add the game board positions in each step to the above animation, and put them side by side with the Q-table in the animation.\n",
    "\n",
    "First, we'll need to record all the game positions, as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef028842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state is 0 and the action is 1\n",
      "current state is 4 and the action is 1\n",
      "current state is 8 and the action is 2\n",
      "current state is 9 and the action is 1\n",
      "current state is 13 and the action is 2\n",
      "current state is 14 and the action is 2\n",
      "Congratulations, you have reached the destination!\n"
     ]
    }
   ],
   "source": [
    "from utils.frozenlake_env import Frozen\n",
    "import turtle as t\n",
    "import time\n",
    "\n",
    "env = Frozen()\n",
    "\n",
    "# Use the Q-table you just trained\n",
    "Q = np.loadtxt('files/ch13/trained_frozenlake_Qs.csv', delimiter=\",\")\n",
    "\n",
    "\n",
    "state=env.reset()\n",
    "env.render()\n",
    "step = 0\n",
    "try:\n",
    "    ts = t.getscreen() \n",
    "except t.Terminator:\n",
    "    ts = t.getscreen()\n",
    "env.render()\n",
    "ts.getcanvas().postscript(file=f\"myenv{step}.ps\")\n",
    "\n",
    "while True:\n",
    "    # Slow down the game so that you can see the graphical rendering\n",
    "    #time.sleep(1)\n",
    "    action = np.argmax(Q[state, :])\n",
    "    print(f'current state is {state} and the action is {action}')\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    # new_state becomes the state in the next round\n",
    "    state=new_state\n",
    "    env.render()\n",
    "    step += 1      \n",
    "    ts.getcanvas().postscript(file=f\"files/ch13/myenv{step}.ps\")\n",
    "\n",
    "    \n",
    "    if done==True:\n",
    "        if reward ==1:\n",
    "            print('Congratulations, you have reached the destination!')\n",
    "        else:\n",
    "            print('Sorry, better luck next time.')\n",
    "        break    \n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8ad1b",
   "metadata": {},
   "source": [
    "The above program saves 7 ps files to the local folder in ps format. Next, we'll convert the files from the ps format to the png format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69484c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle as t\n",
    "import time\n",
    "import random\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "for i in range(7):\n",
    "    im = Image.open(f\"files/ch13/myenv{i}.ps\")\n",
    "    fig, ax=plt.subplots(figsize=(9,9), dpi=200)\n",
    "    newax = fig.add_axes([0,0,1,1], anchor='NE', zorder=1)\n",
    "    newax.imshow(im)\n",
    "    newax.axis('off')\n",
    "    ax.set_xlim(-4.5,4.5)\n",
    "    ax.set_ylim(-4.5,4.5)\n",
    "    plt.axis(\"off\")\n",
    "    #plt.grid()\n",
    "    plt.savefig(f\"files/ch13/myenv{i}plt.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5160388",
   "metadata": {},
   "source": [
    "You now have 7 game board position in the png format. \n",
    "\n",
    "Next, we'll put the game board on the left and the Q-table on the right to form one frame. We'll repeat the frame three times per step, with a total of 7 steps. Fianlly, we'll combined them into an animation; like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e4e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[]\n",
    "\n",
    "for i in range(6):\n",
    "    for letter in [\"a\", \"b\", \"c\"]:\n",
    "        im = Image.open(f\"files/ch13/myenv{i}plt.png\")\n",
    "        f0=np.asarray(im)\n",
    "        im = Image.open(f\"files/ch13/plt_Qs_step{letter}{i}.png\")\n",
    "        f1=np.asarray(im)\n",
    "        fs = np.concatenate([f0,f1],axis=1)\n",
    "        frames.append(fs)\n",
    "im = Image.open(f\"files/ch13/myenv6plt.png\")\n",
    "f0=np.asarray(im)\n",
    "im = Image.open(f\"files/ch13/plt_Qs_stepa5.png\")\n",
    "f1=np.asarray(im)\n",
    "fs = np.concatenate([f0,f1],axis=1)\n",
    "frames.append(fs)\n",
    "frames.append(fs)\n",
    "frames.append(fs)\n",
    "\n",
    "imageio.mimsave('files/ch13/frozen_q_steps.gif', frames, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd34819",
   "metadata": {},
   "source": [
    "If you open the gif file, you'll see the following animation:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_q_steps.gif\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87d61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
