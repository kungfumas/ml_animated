{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 10: Create Your Own Game Environment\n",
    "\n",
    "In this chapter, you’ll learn how to create your own game environments so that you can use deep learning to develop game strategies and play games in them. You'll also use these game environments to practice Reinforcement Learning and Deep Q Learning game strategies later in this book.\n",
    "\n",
    "Since you are already familiar with the Frozen Lake game from the OpenAI Gym, You’ll learn to create your own Frozen Lake game environment, with all the attributes and methods as the one in OpenAI Gym. Better yet, you'll add a graphical game board using the ***turtle*** library so that you can visualize the state you are in as the game progresses. Along the way, you’ll learn the necessary skills to create a game environment, an agent, and how the agent interacts with the environment. You’ll code in how to change from one state to another based on agent’s actions, how to determine rewards, and how to determine if the game has ended. Later in this book, you'll use these skills to create game environments for Tic Tac Toe and Connect Four. \n",
    " \n",
    "Finally, you'll learn how to save a ***turtle*** game board as a png file on your computer. You'll save all game boards in a full game, then convert them into an animation, as follows:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_steps.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 10}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 10 in a subfolder /files/ch10. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch10\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5288bb7",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Install needed modules for Chapter 10}}$<br>\n",
    "***\n",
    "To covert a ps file to a png file, you need to conda install Ghostscript. We'll use it in this chapter and many later chapters. \n",
    "\n",
    "Enter the following command in the Anaconda prompt (Windows) or the terminal (MAC/Linux) to activate the virtual environment that you have created for this book animatedML\n",
    "\n",
    "`conda activate animatedML`\n",
    "\n",
    "then enter the following command in the same Anaconda prompt or terminal:\n",
    "\n",
    "`conda install -c conda-forge ghostscript`\n",
    "\n",
    "Follow the on-screen instructions to finish the installation. \n",
    "\n",
    "After that, restart you Jupyter Notebook for it to take effect.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. Create the Frozen Lake Environment\n",
    "We have played the Frozen Lake game in the OpenAI Gym environment, without a graphical game \n",
    "board. We’ll create such an environment from scratch, with all the features and methods as in the OpenAI Gym environment and add graphical game boards to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "### 1.1. Use A Python Class to Represent the Environment\n",
    "We’ll create a Python class to represent the Frozen Lake environment. The class will have various attributes, variables, and methods to replicate those in the Frozen Lake game in OpenAI Gym. \n",
    "\n",
    "#### Game Attributes\n",
    "\n",
    "The self-made Frozen Lake game will have the following attributes:\n",
    " \n",
    "*\taction_space: an attribute that provides the space of all actions that can be taken by the agent.\n",
    "*\tobservation_space: an attribute that provides the list of all possible states in the environment.\n",
    "*\tstate: an attribute indicating which state the agent is currently in.\n",
    "*\taction: an attribute indicating the action taken by the agent.\n",
    "*\treward: is an attribute indicating the reward to the agent because of the action taken by the agent.\n",
    "*\tdone: an attribute indicating whether the game has ended.\n",
    "*\tinfo: an attribute that provides information about the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9996e",
   "metadata": {},
   "source": [
    "#### Game Methods\n",
    "\n",
    "The self-made Frozen Lake game will have a few methods as well:\n",
    " \n",
    "*\treset() is a method to set the game environment to the initial (that is, the starting) state.\n",
    "*\trender() is a method showing the current state of the environment graphically.\n",
    "*\tstep() is a method that returns the new state, the reward, the value of *done* variable, and the variable *info* based on the action taken by the agent.\n",
    "*\tsample() is a method to randomly choose an action from the action space.\n",
    "*\tclose() is a method to end the game environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "### 1.2. Create A Local Package\n",
    "We'll create a local package for this book to contain all self-made local modules. We'll call the package ***utils***. \n",
    "\n",
    "A Python module is a single file with the .py extension. In contrast, a\n",
    "Python package is a collection of Python modules contained in a single\n",
    "directory. The directory must have a file named *__init__.py* to distinguish\n",
    "it from a directory that happens to have .py extension files in it.\n",
    "I’ll guide you through the process of creating a local package\n",
    "step-by-step. \n",
    "\n",
    "The code below will create the local package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e65a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"utils\", exist_ok=True)\n",
    "outfile = open('utils/__init__.py', 'a')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788b2c6",
   "metadata": {},
   "source": [
    "We first create a local directory *utils*. We then create an empty file *__init__.py* inside the directory. This way, the directory *utils* becomes a local package, not just a local folder. \n",
    "\n",
    "Later, we'll gradually put more modules inside the package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "Now let's code in a self-made Frozen Lake game environment using a Python class. Save the code in the cell below as *frozenlake_env.py* in the folder *utils* you just created. Alternatively, you can download it from my GitHub repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75746040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle as t\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# Define an action_space helper class\n",
    "class action_space:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def sample(self):\n",
    "        return np.random.choice(range(self.n))\n",
    "    \n",
    "# Define an obervation_space helper class    \n",
    "class observation_space:\n",
    "    def __init__(self, n):\n",
    "        self.shape = (n,)\n",
    "\n",
    "# Define the Frozen game environment\n",
    "class Frozen():\n",
    "    def __init__(self):        \n",
    "        self.done=False\n",
    "        self.reward=0.0\n",
    "        self.info={'prob': 1.0}\n",
    "        self.state=0\n",
    "        self.actual_actions=[(0, -1), (1, 0), (0, 1), (-1, 0)]\n",
    "        self.action_space=action_space(4)\n",
    "        self.observation_space=observation_space(16)\n",
    "        self.showboard=False  \n",
    "        self.grid=[]\n",
    "        for x in range(4):\n",
    "            for y in range(4):\n",
    "                self.grid.append((x,y))\n",
    "\n",
    "    def reset(self):  \n",
    "        self.state=0\n",
    "        self.done=False\n",
    "        self.reward=0.0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        actual_action=self.actual_actions[action]\n",
    "        co_or=self.grid[self.state]\n",
    "        new_co_or=(actual_action[0]+co_or[0], actual_action[1]+co_or[1])\n",
    "        if actual_action[0]+co_or[0]<0 or actual_action[0]+co_or[0]>3:\n",
    "            new_co_or=co_or\n",
    "        if actual_action[1]+co_or[1]<0 or actual_action[1]+co_or[1]>3:\n",
    "            new_co_or=co_or        \n",
    "        new_state=self.grid.index(new_co_or)\n",
    "        if new_state==15:\n",
    "            self.reward=1.0\n",
    "            self.done=True\n",
    "        if new_state in [5,7,11,12]:\n",
    "            self.reward=-1.0\n",
    "            self.done=True\n",
    "        self.state=new_state\n",
    "        return new_state, self.reward, self.done, self.info\n",
    " \n",
    "    def display_board(self):\n",
    "        try:\n",
    "            t.setup(850,850, 10, 70) \n",
    "        except t.Terminator:\n",
    "            t.setup(850,850, 10, 70)                       \n",
    "        t.hideturtle()\n",
    "        t.bgcolor(\"alice blue\")\n",
    "        t.tracer(False)\n",
    "        t.title('The Frozen Lake Game')\n",
    "        t.clear()\n",
    "        t.pensize(3)\n",
    "        for i in range(-400,600,200):  \n",
    "            t.up()\n",
    "            t.goto(i, -400)\n",
    "            t.down()\n",
    "            t.goto(i, 400)\n",
    "            t.up()\n",
    "            t.goto(-400,i)\n",
    "            t.down()\n",
    "            t.goto(400,i)\n",
    "            t.up()           \n",
    "        for (x,y) in [(-100,100),(300,100),(300,-100),(-300,-300)]:\n",
    "            t.goto(x,y)\n",
    "            t.dot(150,\"light gray\")\n",
    "            t.color('blue')\n",
    "            t.goto(x-30,y-20)\n",
    "            t.write(\"hole\",font=(\"Helvetica\",30)) \n",
    "        self.xycor={(3, 0):(-400, -400), (2, 0):(-400, -200), \n",
    "               (1, 0):(-400, 0), (0, 0):(-400, 200), \n",
    "               (3, 1):(-200, -400), (2, 1):(-200, -200), \n",
    "               (1, 1):(-200, 0), (0, 1):(-200, 200), \n",
    "               (3, 2):(0, -400), (2, 2):(0, -200), \n",
    "               (1, 2):(0, 0), (0, 2):(0, 200), \n",
    "               (3, 3):(200, -400), (2, 3):(200, -200), \n",
    "               (1, 3):(200, 0), (0, 3):(200, 200)}\n",
    "        state_grid = self.grid[self.state]\n",
    "        state_xy = self.xycor[state_grid]\n",
    "        t.color('blue')\n",
    "        t.goto(-350,350)\n",
    "        t.write(\"start\",font=(\"Helvetica\",30))        \n",
    "        t.goto(300,-350)\n",
    "        t.write(\"goal\",font=(\"Helvetica\",30)) \n",
    "        t.color('black')\n",
    "        # Place a red dot in the current state\n",
    "        self.fall = t.Turtle()\n",
    "        self.fall.up()\n",
    "        self.fall.hideturtle()\n",
    "        self.fall.goto(state_xy[0]+100,state_xy[1]+100)\n",
    "        self.fall.dot(150,\"red\")\n",
    "        t.update()\n",
    "\n",
    "    def render(self):\n",
    "        if self.showboard==False:\n",
    "            self.display_board()\n",
    "            self.showboard=True        \n",
    "        state_grid = self.grid[self.state]\n",
    "        state_xy = self.xycor[state_grid]\n",
    "        # update the current position\n",
    "        self.fall.clear()\n",
    "        self.fall.goto(state_xy[0]+100,state_xy[1]+100)\n",
    "        self.fall.dot(150,\"red\")\n",
    "        t.update()\n",
    "    def close(self):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            t.bye()\n",
    "        except t.Terminator:\n",
    "            print('exit turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05110e83",
   "metadata": {},
   "source": [
    "If you run the above cell, nothing will happen. The class simply creates a game environment. We need to initiate the game environment and start playing using Python programs, just as you do with an OpenAI Gym game environment. We'll do that in the next subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "### 1.3. Verify the Custom-Made Game Environment\n",
    "Next, we'll check the attributes and methods of the self-made game environment and make sure it has all the elements that are provided by the OpenAI Gym. What you'll do is to use the old programs in Chapter 8.\n",
    "\n",
    "In Chapter 8, you used the code below to initiate the game environment:\n",
    "\n",
    "\n",
    "```python\n",
    "import gym \n",
    " \n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()                    \n",
    "env.render()\n",
    "```\n",
    "\n",
    "We'll do something similar. Run the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a847d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.frozenlake_env import Frozen\n",
    "\n",
    "env = Frozen()\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6bd0a",
   "metadata": {},
   "source": [
    "You should see a separate turtle window, with a game board as follows: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen0.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977029b",
   "metadata": {},
   "source": [
    "If you want to close the game board window, use the *close()* method, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439f093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "Next, we'll check the attributes of the environment such as the observation space and action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of possible actions are 4\n",
      "the following are ten sample actions\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "the shape of the observation space is (16,)\n"
     ]
    }
   ],
   "source": [
    "env=Frozen()\n",
    "# check the action space\n",
    "number_actions = env.action_space.n\n",
    "print(\"the number of possible actions are\", number_actions)\n",
    "# sample the action space ten times\n",
    "print(\"the following are ten sample actions\")\n",
    "for i in range(10):\n",
    "   print(env.action_space.sample())\n",
    "# check the shape of the observation space\n",
    "print(\"the shape of the observation space is\", env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The meanings of the actions in this game are exactly the same as those in the OpenAI Gym Frozen Lake game:\n",
    "* 0: Going left\n",
    "* 1: Going down\n",
    "* 2: Going right\n",
    "* 3: Going up\n",
    "\n",
    "\n",
    "The state space has 16 values: 0, 1, 2, …, 15. The top left square is state 0, the top right is state 3, and the bottom right corner is 15, as shown in the following picture:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/lakesurface.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f879a",
   "metadata": {},
   "source": [
    "## 2. Play Games in the Custom-Made Environment\n",
    "Next, we'll play games in the custom-made environment. You'll learn to save each game board as a picture. Finally, you'll record all game boards in a full game, and convert them into an animation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83ac2f",
   "metadata": {},
   "source": [
    "### 2.1. Play a full game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5af973",
   "metadata": {},
   "source": [
    "Here we'll play a full game, by randomly choosing an action from the action space each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44d0601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is state=0\n",
      "the current action is action=3\n",
      "new state=0, reward=0.0, done=False\n",
      "the current state is state=0\n",
      "the current action is action=2\n",
      "new state=1, reward=0.0, done=False\n",
      "the current state is state=1\n",
      "the current action is action=2\n",
      "new state=2, reward=0.0, done=False\n",
      "the current state is state=2\n",
      "the current action is action=1\n",
      "new state=6, reward=0.0, done=False\n",
      "the current state is state=6\n",
      "the current action is action=2\n",
      "new state=7, reward=-1.0, done=True\n",
      "Better luck next time!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Initiate the game environment\n",
    "env=Frozen()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "# Play a full game\n",
    "while True:\n",
    "    # randomly choose an action\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"the current state is state={state}\")\n",
    "    print(f\"the current action is action={action}\")    \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    print(f\"new state={new_state}, reward={reward}, done={done}\")\n",
    "    # Current new state becomes the state in the next round\n",
    "    state=new_state\n",
    "    env.render()\n",
    "    # slow down the game pace by stopping for one second\n",
    "    time.sleep(1)\n",
    "    # Stop the game if done is True\n",
    "    if done == True:\n",
    "        if state==15:\n",
    "            print(\"Congrats, you won the game!\")\n",
    "        else:\n",
    "            print(\"Better luck next time!\")\n",
    "        break\n",
    "env.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "Note that the outcome is different each time you run it because the actions are randomly chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064889bb",
   "metadata": {},
   "source": [
    "### 2.2. Play the Game Manually\n",
    "Next, you’ll learn how to manually interact with the Frozen Lake game, just like you did in Chapter 8. The following lines of code show you how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac17214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "enter 0 for left, 1 for down\n",
      "2 for right, and 3 for up\n",
      "\n",
      "the current state is state=0\n",
      "how do you want to move?\n",
      "1\n",
      "you have chosen action=1\n",
      "the current state is state=4\n",
      "how do you want to move?\n",
      "1\n",
      "you have chosen action=1\n",
      "the current state is state=8\n",
      "how do you want to move?\n",
      "2\n",
      "you have chosen action=2\n",
      "the current state is state=9\n",
      "how do you want to move?\n",
      "2\n",
      "you have chosen action=2\n",
      "the current state is state=10\n",
      "how do you want to move?\n",
      "1\n",
      "you have chosen action=1\n",
      "the current state is state=14\n",
      "how do you want to move?\n",
      "2\n",
      "you have chosen action=2\n",
      "Congrats, you have made to the destination!\n"
     ]
    }
   ],
   "source": [
    "# Initiate the game environment\n",
    "env=Frozen()\n",
    "state=env.reset()   \n",
    "env.render()   \n",
    "\n",
    "print('''\n",
    "enter 0 for left, 1 for down\n",
    "2 for right, and 3 for up\n",
    "''')\n",
    "\n",
    "while True:\n",
    "    print(f\"the current state is state={state}\")\n",
    "    try:\n",
    "        action = int(input('how do you want to move?\\n'))\n",
    "    except:\n",
    "        print('please enter 0, 1, 2, or 3')\n",
    "    print(f\"you have chosen action={action}\")\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    state = new_state\n",
    "    env.render()\n",
    "    if done==True:\n",
    "        if new_state==15:\n",
    "            print(\"Congrats, you have made to the destination!\")\n",
    "        else:\n",
    "            print(\"Game over. Better luck next time!\")\n",
    "        break  \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "### 2.3. Save the Current Game Board as A Picture\n",
    "Next, you'll learn how to save the current game board as a picture on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b16022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle as t\n",
    "\n",
    "env=Frozen()\n",
    "env.reset()                    \n",
    "# use the getscreen() method in turtle to record screen\n",
    "try:\n",
    "    ts = t.getscreen()\n",
    "except t.Terminator:\n",
    "    ts = t.getscreen()   \n",
    "env.render()\n",
    "# save the screen as a ps file\n",
    "ts.getcanvas().postscript(file=f\"files/ch10/frozent.ps\")\n",
    "time.sleep(5)\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b4cae",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{A known Terminator error in the turtle library}}$<br>\n",
    "***\n",
    "It's a known problem that turtle windows may not close properly. As a result, it gives you an error message every other time you run it. I used the exception handling above to address the issue. By initiating the ***turtle*** library twice, we can avoid the error message here.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed47308a",
   "metadata": {},
   "source": [
    "\n",
    "After that, run the following cell to save the ps files on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a79818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"files/ch10/frozent.ps\")\n",
    "pngfigure = image.convert('RGBA')\n",
    "pngfigure.save(\"files/ch10/frozent.png\", lossless = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed2f33",
   "metadata": {},
   "source": [
    "Now, if you open the file frozent.png in your local folder, you'll see the following picture:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozent.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa3a085",
   "metadata": {},
   "source": [
    "### 2.4. Animate A Full Game\n",
    "\n",
    "Now that you know how to record the board position at any moment, you can record all the game positions in a full game sequentially, and make an animation out of it, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc9082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "enter 0 for left, 1 for down\n",
      "2 for right, and 3 for up\n",
      "\n",
      "how do you want to move?\n",
      "1\n",
      "how do you want to move?\n",
      "1\n",
      "how do you want to move?\n",
      "2\n",
      "how do you want to move?\n",
      "2\n",
      "how do you want to move?\n",
      "1\n",
      "how do you want to move?\n",
      "2\n",
      "Congrats, you have made to the destination!\n"
     ]
    }
   ],
   "source": [
    "import turtle as t\n",
    "\n",
    "env = Frozen()\n",
    "env.reset()   \n",
    "\n",
    "print('''\n",
    "enter 0 for left, 1 for down\n",
    "2 for right, and 3 for up\n",
    "''')\n",
    "\n",
    "step = 1\n",
    "try:\n",
    "    ts = t.getscreen()\n",
    "except t.Terminator:\n",
    "    ts = t.getscreen()    \n",
    "env.render()\n",
    "ts.getcanvas().postscript(file=f\"frozen0.ps\")\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        action = int(input('how do you want to move?\\n'))\n",
    "    except:\n",
    "        print('please enter 0, 1, 2, or 3')\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    ts.getcanvas().postscript(file=f\"frozen{step}.ps\")\n",
    "    step += 1\n",
    "    if done==True:\n",
    "        if new_state==15:\n",
    "            print(\"Congrats, you have made to the destination!\")\n",
    "        else:\n",
    "            print(\"Game over. Better luck next time!\")\n",
    "        break  \n",
    "\n",
    "env.close()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "for i in range(step):\n",
    "    im = Image.open(f\"frozen{i}.ps\")\n",
    "    fig = im.convert('RGBA')\n",
    "    fig.save(f\"frozen{i}.png\", lossless = True)\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "frames=[]\n",
    "for i in range(step):\n",
    "    im = Image.open(f\"frozen{i}.ps\")\n",
    "    frame=np.asarray(im)\n",
    "    frames.append(frame) \n",
    "imageio.mimsave('frozen_steps.gif', frames, fps=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57415735",
   "metadata": {},
   "source": [
    "I used six steps to finish the game. All six steps, plus the initial screen, are all captured and saved as png files in the chapter folder. For example, the last position of the board is frozen6.png, and it looks as follows:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116598ed",
   "metadata": {},
   "source": [
    "It shows that I have successfully reached the goal and won the game.\n",
    "\n",
    "Finally, I have also combined the seven pictures into an animation in the gif format. If you open the file frozen_steps.gif in your chapter folder, you should see the following:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/frozen_steps.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14805aac",
   "metadata": {},
   "source": [
    "## 3. Play the Game with the Trained Model\n",
    "We have already trained a deep neural network model to play the Frozen Lake game in the OpenAI Gym environment. Since our self-made game environment has all the features, plus the graphical game board, we can play the game with the trained model from Chapter 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eafae1ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 318ms/step\n",
      "1\n",
      "4 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1\n",
      "8 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2\n",
      "9 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1\n",
      "13 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "2\n",
      "14 0.0 False {'prob': 1.0}\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "2\n",
      "15 1.0 True {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# import needed modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.frozenlake_env import Frozen\n",
    "\n",
    "# note that model is saved in the local folder for Chapter 8\n",
    "reload = tf.keras.models.load_model(\"files/ch08/trained_frozen.h5\")\n",
    "\n",
    "# Define a onehot_encoder() function\n",
    "def onehot_encoder(value, length):\n",
    "    onehot=np.zeros((1,length))\n",
    "    onehot[0,value]=1\n",
    "    return onehot\n",
    "\n",
    "action0=onehot_encoder(0, 4)\n",
    "action1=onehot_encoder(1, 4)\n",
    "action2=onehot_encoder(2, 4)\n",
    "action3=onehot_encoder(3, 4)\n",
    "\n",
    "env = Frozen()\n",
    "state = env.reset()   \n",
    "\n",
    "while True:\n",
    "    # Convert state and action into onehots \n",
    "    state_arr = onehot_encoder(state, 16)\n",
    "    # Use the trained model to predict the prob of winning \n",
    "    sa0 = np.concatenate([state_arr, action0], axis=1)    \n",
    "    sa1 = np.concatenate([state_arr, action1], axis=1)  \n",
    "    sa2 = np.concatenate([state_arr, action2], axis=1)  \n",
    "    sa3 = np.concatenate([state_arr, action3], axis=1)\n",
    "    sa = np.concatenate([sa0, sa1, sa2, sa3], axis=0)\n",
    "    action = np.argmax(reload.predict(sa, verbose=0))\n",
    "    print(action)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(new_state, reward, done, info) \n",
    "    state = new_state\n",
    "    if done == True:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "The player wins the game with the shortest possible path. So the deep learning game strategy works in the self-made environment as well!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806366f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
