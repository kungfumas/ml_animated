{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 11: Deep Learning Game Strategies: Tic Tac Toe\n",
    "\n",
    "In this chapter, you’ll combine what you have learned in Chapters 5 to 9 to design deep learning game strategies for the Tic Tac Toe game. You'll first create a game environment for Tic Tac Toe with all the features and methods of a typical OpenAI Gym game environment. The game environment also has a graphical interface. \n",
    "\n",
    "You'll use similated games as input data to feed into a deep neural network. After the model is trained, you'll use it to play games. At each step of the game, you'll look at all next moves. The model predicts the probability of winning the game with each hypothetical move. You'll pick the move with the highest probability of winning the game.\n",
    "\n",
    "Finally, you'll animate the decision making process. You'll use the deep learning game strategy to play a full game. At each step, the animation will show the game board on the left, and the probability of winning for each next move on the right. The best move will be highlighted, as follows:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_DL_steps.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 11}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 11 in a subfolder /files/ch11. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch11\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. Create the Tic Tac Toe Game Environment\n",
    "We'll create a Tic Tac Toe game environment, using the ***turtle*** library to draw game boards. We’ll create all the features and methods that a typical OpenAI Gym environment has. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "### 1.1. Use A Python Class to Represent the Environment\n",
    "We’ll create a Python class to represent the Tic Tac Toe game environment. The class will have various attributes, variables, and methods to replicate those in a typical OpenAI Gym game environment. \n",
    "\n",
    "#### Attributes\n",
    "Specifically, our self-made Tic Tac Toe game environment will have the following attributes:\n",
    " \n",
    "*\taction_space: an attribute that provides the space of all actions that can be taken by the agent. The action space will have nine values, 1 to 9. We use 1 to 9 instead of 0 to 8 to avoid confusion.\n",
    "*\tobservation_space: an attribute that provides the list of all possible states in the environment. We'll use a numpy array with 9 values to represent the nine cell on a game board.\n",
    "*\tstate: an attribute indicating which state the agent is currently in. Each of the nine cells can take values -1 (occupied by player O), 0 (empty), or 1 (occupied by player X).\n",
    "*\taction: an attribute indicating the action taken by the agent. The action is a number between 1 and 9.\n",
    "*\treward: is an attribute indicating the reward to the agent because of the action taken by the agent. The reward is 0 in each step, unless a player has won the game, in which case the winner has a reward of 1 and the lose a reward of -1. \n",
    "*\tdone: an attribute indicating whether the game has ended. This happens when one player wins or if the game is tied.\n",
    "*\tinfo: an attribute that provides information about the game. We'll set it as an empty string \"\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9996e",
   "metadata": {},
   "source": [
    "#### Methods\n",
    "our self-made Tic Tac Toe game environment will have a few methods as well:\n",
    " \n",
    "*\treset() is a method to set the game environment to the initial (that is, the starting) state. All cells on the board will be empty.\n",
    "*\trender() is a method showing the current state of the environment graphically.\n",
    "*\tstep() is a method that returns the new state, the reward, the value of *done* variable, and the varibale *info* based on the action taken by the agent.\n",
    "*\tsample() is a method to randomly choose an action from all the action space.\n",
    "*\tclose() is a method to end the game environment, including stop displaying the graph of the current state of the game board."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "### 1.2. Create A Local Module for the Tic Tac Toe Game\n",
    "We'll create a local module for the Tic Tac Toe game and place it inside the local package for this book: the package ***utils*** that we have created in Chapter 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "Now let's code in a self-made Tic Tac Toe game environment using a Python class. Save the code in the cell below as *TicTacToe_env.py* in the folder *utils* you created in Chapter 10. Alternatively, you can download it from my GitHub repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8839e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle as t\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define an action_space helper class\n",
    "class action_space:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def sample(self):\n",
    "        num = np.random.choice(range(self.n))\n",
    "        # covert to 1 to 9 in string format \n",
    "        action = str(1+num)\n",
    "        return action\n",
    "    \n",
    "# Define an obervation_space helper class    \n",
    "class observation_space:\n",
    "    def __init__(self, n):\n",
    "        self.shape = (n,)\n",
    "\n",
    "class ttt():\n",
    "    def __init__(self): \n",
    "        # use the helper action_space class\n",
    "        self.action_space=action_space(9)\n",
    "        # use the helper observation_space class\n",
    "        self.observation_space=observation_space(9)\n",
    "        self.info=\"\"  \n",
    "        self.showboard=False          \n",
    "        # Create a dictionary to map cell number to coordinates\n",
    "        self.cellcenter = {'1':(-200,-200), '2':(0,-200), '3':(200,-200),\n",
    "                    '4':(-200,0), '5':(0,0), '6':(200,0),\n",
    "                    '7':(-200,200), '8':(0,200), '9':(200,200)} \n",
    "\n",
    "    def reset(self):  \n",
    "        # The X player moves first\n",
    "        self.turn = \"X\"\n",
    "        # Count how many rounds played\n",
    "        self.rounds = 1\n",
    "        # Create a list of valid moves\n",
    "        self.validinputs = list(self.cellcenter.keys())\n",
    "        # Create a dictionary of moves made by each player\n",
    "        self.occupied = {\"X\":[],\"O\":[]}\n",
    "        # Tracking the state\n",
    "        self.state=np.array([0,0,0,0,0,0,0,0,0])\n",
    "        self.done=False\n",
    "        self.reward=0     \n",
    "        return self.state        \n",
    "        \n",
    "    # step() function: place piece on board and update state\n",
    "    def step(self, inp):\n",
    "        # Add the move to the occupied list \n",
    "        self.occupied[self.turn].append(inp)\n",
    "        # update the state: X is 1 and O is -1\n",
    "        self.state[int(inp)-1]=2*(self.turn==\"X\")-1\n",
    "        # Disallow the move in future rounds\n",
    "        self.validinputs.remove(inp) \n",
    "        # check if the player has won the game\n",
    "        if self.win_game() == True:\n",
    "            self.done=True\n",
    "            # reward is 1 if X won; -1 if O won\n",
    "            self.reward=2*(self.turn==\"X\")-1\n",
    "            self.validinputs=[]\n",
    "        # If all cellls are occupied and no winner, it's a tie\n",
    "        elif self.rounds == 9:\n",
    "            self.done=True\n",
    "            self.reward=0\n",
    "            self.validinputs=[]\n",
    "        else:\n",
    "            # Counting rounds\n",
    "            self.rounds += 1\n",
    "            # Give the turn to the other player\n",
    "            if self.turn == \"X\":\n",
    "                self.turn = \"O\"\n",
    "            else:\n",
    "                self.turn = \"X\"             \n",
    "        return self.state, self.reward, self.done, self.info\n",
    "                     \n",
    "    # Determine if a player has won the game\n",
    "    def win_game(self):\n",
    "        win = False\n",
    "        if '1' in self.occupied[self.turn] and '2' in self.occupied[self.turn] and '3' in self.occupied[self.turn]:\n",
    "            win = True\n",
    "        if '4' in self.occupied[self.turn] and '5' in self.occupied[self.turn] and '6' in self.occupied[self.turn]:\n",
    "            win = True\n",
    "        if '7' in self.occupied[self.turn] and '8' in self.occupied[self.turn] and '9' in self.occupied[self.turn]:\n",
    "            win = True\n",
    "        if '1' in self.occupied[self.turn] and '4' in self.occupied[self.turn] and '7' in self.occupied[self.turn]:\n",
    "            win = True\n",
    "        if '2' in self.occupied[self.turn] and '5' in self.occupied[self.turn] and '8' in self.occupied[self.turn]:\n",
    "            win = True\n",
    "        if '3' in self.occupied[self.turn] and '6' in self.occupied[self.turn] and '9' in self.occupied[self.turn]:\n",
    "            win = True\n",
    "        if '1' in self.occupied[self.turn] and '5' in self.occupied[self.turn] and '9' in self.occupied[self.turn]:\n",
    "            win = True\n",
    "        if '3' in self.occupied[self.turn] and '5' in self.occupied[self.turn] and '7' in self.occupied[self.turn]:\n",
    "            win = True\n",
    "        return win\n",
    "\n",
    "    def display_board(self):\n",
    "        # Set up the screen\n",
    "        try:\n",
    "            t.setup(630,630,10,70) \n",
    "        except t.Terminator:\n",
    "            t.setup(630,630,10,70)   \n",
    "        t.tracer(False)\n",
    "        t.hideturtle()\n",
    "        t.bgcolor(\"azure\")\n",
    "        t.title(\"Tic-Tac-Toe in Turtle Graphics\")\n",
    "        # Draw horizontal lines and vertical lines to form grid\n",
    "        t.pensize(5)\n",
    "        t.color('blue')\n",
    "        for i in (-300,-100,100,300):  \n",
    "            t.up()\n",
    "            t.goto(i,-300)\n",
    "            t.down()\n",
    "            t.goto(i,300)\n",
    "            t.up()\n",
    "            t.goto(-300,i)\n",
    "            t.down()\n",
    "            t.goto(300,i)\n",
    "            t.up()\n",
    "        # Go to the center of each cell, write down the cell number\n",
    "        t.color('red')\n",
    "        for cell, center in list(self.cellcenter.items()):\n",
    "            t.goto(center[0]-80,center[1]-80)\n",
    "            t.write(cell,font = ('Arial',20,'normal'))\n",
    "\n",
    "    def render(self):\n",
    "        if self.showboard==False:\n",
    "            self.display_board()\n",
    "            self.showboard=True   \n",
    "        # Place X or O in occupied cells\n",
    "        t.color('light gray')\n",
    "        if len(self.occupied[\"X\"])>0:\n",
    "            for x in self.occupied[\"X\"]:\n",
    "                t.up()\n",
    "                t.goto(self.cellcenter[x][0]-60,self.cellcenter[x][1]-60)\n",
    "                t.down()               \n",
    "                t.goto(self.cellcenter[x][0]+60,self.cellcenter[x][1]+60)\n",
    "                t.up()\n",
    "                t.goto(self.cellcenter[x][0]-60,self.cellcenter[x][1]+60)\n",
    "                t.down()               \n",
    "                t.goto(self.cellcenter[x][0]+60,self.cellcenter[x][1]-60)\n",
    "                t.up()    \n",
    "                t.update()\n",
    "        if len(self.occupied[\"O\"])>0:                \n",
    "            for o in self.occupied[\"O\"]:\n",
    "                t.up()\n",
    "                t.goto(self.cellcenter[o])\n",
    "                t.dot(160,\"light gray\") \n",
    "                t.update()\n",
    "\n",
    "    def close(self):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            t.bye()\n",
    "        except t.Terminator:\n",
    "            print('exit turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05110e83",
   "metadata": {},
   "source": [
    "If you run the above cell, nothing will happen. The class simply creates a game environment. We need to initiate the game environment and start playing using Python programs, just as you do with an OpenAI Gym game environment. We'll do that in the next subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "### 1.3. Verify the Custom-Made Game Environment\n",
    "Next, we'll check the attributes and methods of the self-made game environment and make sure it has all the elements that are provided by a typical OpenAI Gym game environment. \n",
    "\n",
    "First we'll initiate the game environment and show the game board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a847d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.TicTacToe_env import ttt\n",
    "\n",
    "env = ttt()\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6bd0a",
   "metadata": {},
   "source": [
    "You should see a separate turtle window, with a game board as follows: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_start.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977029b",
   "metadata": {},
   "source": [
    "If you want to close the game board window, use the *close()* method, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439f093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "Next, we'll check the attributes of the environment such as the observation space and action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of possible actions are 9\n",
      "the following are ten sample actions\n",
      "9\n",
      "1\n",
      "5\n",
      "5\n",
      "7\n",
      "8\n",
      "6\n",
      "3\n",
      "8\n",
      "5\n",
      "the shape of the observation space is (9,)\n"
     ]
    }
   ],
   "source": [
    "env=ttt()\n",
    "# check the action space\n",
    "number_actions = env.action_space.n\n",
    "print(\"the number of possible actions are\", number_actions)\n",
    "# sample the action space ten times\n",
    "print(\"the following are ten sample actions\")\n",
    "for i in range(10):\n",
    "   print(env.action_space.sample())\n",
    "# check the shape of the observation space\n",
    "print(\"the shape of the observation space is\", env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The meanings of the actions in this game as follows\n",
    "* 1: Placing a game piece in cell 1\n",
    "* 2: Placing a game piece in cell 2\n",
    "* ...\n",
    "* 9: Placing a game piece in cell 9\n",
    "The state space is a vector with 9 values: \n",
    "* 0 means it's empty; \n",
    "* -1 means it's occupied by player O; \n",
    "* 1 means it's occupied by player X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f879a",
   "metadata": {},
   "source": [
    "## 2. Play Games in the Tic Tac Toe Environment\n",
    "Next, we'll play games in the custom-made environment. You'll learn to save each game board as a picture. Finally, you'll record all game boards in a full game, and convert them into an animation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83ac2f",
   "metadata": {},
   "source": [
    "### 2.1. Play a full game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5af973",
   "metadata": {},
   "source": [
    "Here we'll play a full game, by randomly choosing an action from the action space each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44d0601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is state=[0 0 0 0 0 0 0 0 0]\n",
      "Player X has chosen action=4\n",
      "the current state is state=[0 0 0 1 0 0 0 0 0]\n",
      "Player O has chosen action=7\n",
      "the current state is state=[ 0  0  0  1  0  0 -1  0  0]\n",
      "Player X has chosen action=5\n",
      "the current state is state=[ 0  0  0  1  1  0 -1  0  0]\n",
      "Player O has chosen action=8\n",
      "the current state is state=[ 0  0  0  1  1  0 -1 -1  0]\n",
      "Player X has chosen action=9\n",
      "the current state is state=[ 0  0  0  1  1  0 -1 -1  1]\n",
      "Player O has chosen action=6\n",
      "the current state is state=[ 0  0  0  1  1 -1 -1 -1  1]\n",
      "Player X has chosen action=2\n",
      "the current state is state=[ 0  1  0  1  1 -1 -1 -1  1]\n",
      "Player O has chosen action=3\n",
      "the current state is state=[ 0  1 -1  1  1 -1 -1 -1  1]\n",
      "Player X has chosen action=1\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from utils.TicTacToe_env import ttt\n",
    "\n",
    "# Initiate the game environment\n",
    "env = ttt()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "# Play a full game manually\n",
    "while True:\n",
    "    print(f\"the current state is state={state}\")    \n",
    "    action = random.choice(env.validinputs)\n",
    "    time.sleep(1)\n",
    "    print(f\"Player X has chosen action={action}\")    \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    print(f\"the current state is state={new_state}\")    \n",
    "    action = random.choice(env.validinputs)\n",
    "    time.sleep(1)\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    new_new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break\n",
    "    else: \n",
    "        # play next round\n",
    "        state=new_new_state\n",
    "    \n",
    "env.close()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "Note that the outcome is different each time you run it because the actions are randomly chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064889bb",
   "metadata": {},
   "source": [
    "### 2.2. Play the Game Manually\n",
    "Next, you’ll learn how to manually interact with the Tic Tac Toe game. You'll use the key board to enter a number between 1 and 9. The following lines of code show you how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac17214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "enter 0 for left, 1 for down\n",
      "2 for right, and 3 for up\n",
      "\n",
      "enter a move in the form of 1 to 9\n",
      "the current state is state=[0 0 0 0 0 0 0 0 0]\n",
      "Player X, what's your move?\n",
      "5\n",
      "Player X has chosen action=5\n",
      "the current state is state=[0 0 0 0 1 0 0 0 0]\n",
      "Player O, what's your move?\n",
      "7\n",
      "Player O has chosen action=7\n",
      "the current state is state=[ 0  0  0  0  1  0 -1  0  0]\n",
      "Player X, what's your move?\n",
      "9\n",
      "Player X has chosen action=9\n",
      "the current state is state=[ 0  0  0  0  1  0 -1  0  1]\n",
      "Player O, what's your move?\n",
      "1\n",
      "Player O has chosen action=1\n",
      "the current state is state=[-1  0  0  0  1  0 -1  0  1]\n",
      "Player X, what's your move?\n",
      "4\n",
      "Player X has chosen action=4\n",
      "the current state is state=[-1  0  0  1  1  0 -1  0  1]\n",
      "Player O, what's your move?\n",
      "6\n",
      "Player O has chosen action=6\n",
      "the current state is state=[-1  0  0  1  1 -1 -1  0  1]\n",
      "Player X, what's your move?\n",
      "2\n",
      "Player X has chosen action=2\n",
      "the current state is state=[-1  1  0  1  1 -1 -1  0  1]\n",
      "Player O, what's your move?\n",
      "8\n",
      "Player O has chosen action=8\n",
      "the current state is state=[-1  1  0  1  1 -1 -1 -1  1]\n",
      "Player X, what's your move?\n",
      "3\n",
      "Player X has chosen action=3\n",
      "It's a tie!\n"
     ]
    }
   ],
   "source": [
    "# Initiate the game environment\n",
    "print('''\n",
    "enter 0 for left, 1 for down\n",
    "2 for right, and 3 for up\n",
    "''')\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "\n",
    "print(\"enter a move in the form of 1 to 9\")\n",
    "\n",
    "# Play a full game manually\n",
    "while True:\n",
    "    print(f\"the current state is state={state}\")    \n",
    "    action = input(\"Player X, what's your move?\\n\")\n",
    "    print(f\"Player X has chosen action={action}\")    \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    print(f\"the current state is state={new_state}\")    \n",
    "    action = input(\"Player O, what's your move?\\n\")\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    new_new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break\n",
    "    else: \n",
    "        # play next round\n",
    "        state=new_new_state\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "## 3. Train the Deep Learning Game Stratey\n",
    "In this section, you’ll learn how to use deep neural network to train intelligent game strategies for Tic Tac Toe. In particular, you’ll use the convolutional neural network that you used in image classification to train the game strategy. By treating the game board as a two-dimensional graph instead of a one-dimensional vector, you’ll greatly improve the intelligence of your game strategies.\n",
    "\n",
    "You’ll learn how to prepare data to train the model, how to interpret the prediction from the model. How to use the prediction to play games, and how to check the efficacy of your strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "### 3.1. A Summary of the Deep Learning Game Strategy\n",
    "Here is a summary of what we’ll do to train the game strategy:\n",
    "\n",
    "1.\tWe’ll let two computer players automatically play a game with random moves, and record the whole game history. The game history will contain all the game board positions from the very first move to the very last move.\n",
    "2.\tWe then associate each board position with a game outcome (win, tie, or lose). The game board position is similar to features X in our image classification problem, and the outcome is similar to labels y in our classification problem.\n",
    "3.\tWe’ll simulate 1,000,000 games. By using the histories of the games and the corresponding outcomes as Xs and ys, we feed the data into a Deep Neural Networks model. After the training is done, we have a trained model.\n",
    "4.\tWe can now use the trained model to play a game. At each move of the game, we look at all possible next moves, and feed the hypothetical game board into the pretained model. The model will tell you the probabilities of win, lose, and tie.\n",
    "5.\tYou select the move that the model predicts with the highest chance of winning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c5b19",
   "metadata": {},
   "source": [
    "### 3.2. Simulate Games\n",
    "You’ll learn how to generate data to train the DNN. The logic is as follows: you’ll generate 1,000,000 games in which both players use random moves. You’ll then record the board positions of all intermediate steps and the eventual outcomes of each board position (win, lose, or tie). \n",
    "\n",
    "First, let's simulate one game. The code in the cell below accomplishes that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [1, 0, 0]]),\n",
      " array([[ 0, -1,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 1,  0,  0]]),\n",
      " array([[ 0, -1,  0],\n",
      "       [ 1,  0,  0],\n",
      "       [ 1,  0,  0]]),\n",
      " array([[ 0, -1, -1],\n",
      "       [ 1,  0,  0],\n",
      "       [ 1,  0,  0]]),\n",
      " array([[ 0, -1, -1],\n",
      "       [ 1,  0,  0],\n",
      "       [ 1,  1,  0]]),\n",
      " array([[ 0, -1, -1],\n",
      "       [ 1,  0, -1],\n",
      "       [ 1,  1,  0]]),\n",
      " array([[ 1, -1, -1],\n",
      "       [ 1,  0, -1],\n",
      "       [ 1,  1,  0]])]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from utils.TicTacToe_env import ttt\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "\n",
    "# Define the one_game() function\n",
    "def one_game():\n",
    "    history = []\n",
    "    state=env.reset()   \n",
    "    while True:   \n",
    "        action = random.choice(env.validinputs)  \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        history.append(np.array(new_state).reshape(3,3))\n",
    "        if done:\n",
    "            break\n",
    "    return history, reward\n",
    "\n",
    "# Simulate one game and print out results\n",
    "history, outcome = one_game()\n",
    "pprint(history)\n",
    "pprint(outcome)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db511f44",
   "metadata": {},
   "source": [
    "Note here we convert the game board to a 3 by 3 array so it's easy for you to see the positions of the game pieces. \n",
    "\n",
    "Now let's simulate 1,000,000 games and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the game 1000000 times and record all games\n",
    "results = []        \n",
    "for x in range(100000):\n",
    "    history, outcome = one_game()\n",
    "    # Note here I associate each board with the game outcome\n",
    "    for board in history:\n",
    "        results.append((outcome, board))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb7be7",
   "metadata": {},
   "source": [
    "Now let's save the data on your computer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88782a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1, array([[0, 0, 0],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 0]])),\n",
      " (-1, array([[ 0,  0,  0],\n",
      "       [ 0, -1,  1],\n",
      "       [ 0,  0,  0]])),\n",
      " (-1, array([[ 0,  0,  0],\n",
      "       [ 0, -1,  1],\n",
      "       [ 0,  1,  0]])),\n",
      " (-1, array([[ 0,  0,  0],\n",
      "       [ 0, -1,  1],\n",
      "       [-1,  1,  0]])),\n",
      " (-1, array([[ 0,  0,  1],\n",
      "       [ 0, -1,  1],\n",
      "       [-1,  1,  0]])),\n",
      " (-1, array([[ 0,  0,  1],\n",
      "       [ 0, -1,  1],\n",
      "       [-1,  1, -1]])),\n",
      " (-1, array([[ 0,  1,  1],\n",
      "       [ 0, -1,  1],\n",
      "       [-1,  1, -1]])),\n",
      " (-1, array([[-1,  1,  1],\n",
      "       [ 0, -1,  1],\n",
      "       [-1,  1, -1]])),\n",
      " (0, array([[1, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0]])),\n",
      " (0, array([[ 1,  0, -1],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0]]))]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the simulation data on your computer\n",
    "with open('files/ch11/games_ttt10K.p', 'wb') as fp:\n",
    "    pickle.dump(results,fp)\n",
    "# read the data and print out the first 10 observations       \n",
    "with open('files/ch11/games_ttt10K.p', 'rb') as fp:\n",
    "    games = pickle.load(fp)\n",
    "pprint(games[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d59dd",
   "metadata": {},
   "source": [
    "The first six observations are from teh first game in which player O won by occupying cells 7, 8, and 9. Therefore you see -1 as the first element of the first six observations. The data are stored correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f5bef",
   "metadata": {},
   "source": [
    "We have the data we need. You’ll learn how to train the model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "### 3.3. Train Your Tic Tac Toe Game Strategy Using Deep Neural Network\n",
    "The following neural network trains the game strategy using the data you just created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1579eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2383/2383 [==============================] - 4s 1ms/step - loss: 0.7548 - accuracy: 0.6588\n",
      "Epoch 2/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6961 - accuracy: 0.6809\n",
      "Epoch 3/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6822 - accuracy: 0.6834\n",
      "Epoch 4/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6729 - accuracy: 0.6859\n",
      "Epoch 5/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6661 - accuracy: 0.6875\n",
      "Epoch 6/100\n",
      "2383/2383 [==============================] - 4s 1ms/step - loss: 0.6608 - accuracy: 0.6872\n",
      "Epoch 7/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6566 - accuracy: 0.6885\n",
      "Epoch 8/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6536 - accuracy: 0.6899\n",
      "Epoch 9/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6506 - accuracy: 0.6907\n",
      "Epoch 10/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6488 - accuracy: 0.6906\n",
      "Epoch 11/100\n",
      "2383/2383 [==============================] - 4s 1ms/step - loss: 0.6469 - accuracy: 0.6917\n",
      "Epoch 12/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6458 - accuracy: 0.6902\n",
      "Epoch 13/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6442 - accuracy: 0.6922\n",
      "Epoch 14/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6436 - accuracy: 0.6933\n",
      "Epoch 15/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6423 - accuracy: 0.6929\n",
      "Epoch 16/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6422 - accuracy: 0.6936\n",
      "Epoch 17/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6407 - accuracy: 0.6922\n",
      "Epoch 18/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6402 - accuracy: 0.6933\n",
      "Epoch 19/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6388 - accuracy: 0.6936\n",
      "Epoch 20/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6382 - accuracy: 0.6938\n",
      "Epoch 21/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6377 - accuracy: 0.6941\n",
      "Epoch 22/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6375 - accuracy: 0.6952\n",
      "Epoch 23/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6364 - accuracy: 0.6951\n",
      "Epoch 24/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6361 - accuracy: 0.6952\n",
      "Epoch 25/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6365 - accuracy: 0.6956\n",
      "Epoch 26/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6356 - accuracy: 0.6962\n",
      "Epoch 27/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6353 - accuracy: 0.6963\n",
      "Epoch 28/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6344 - accuracy: 0.6955\n",
      "Epoch 29/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6343 - accuracy: 0.6955\n",
      "Epoch 30/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6341 - accuracy: 0.6957\n",
      "Epoch 31/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6333 - accuracy: 0.6950\n",
      "Epoch 32/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6337 - accuracy: 0.6955\n",
      "Epoch 33/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6336 - accuracy: 0.6960\n",
      "Epoch 34/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6331 - accuracy: 0.6966\n",
      "Epoch 35/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6324 - accuracy: 0.6965\n",
      "Epoch 36/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6320 - accuracy: 0.6965\n",
      "Epoch 37/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6316 - accuracy: 0.6975\n",
      "Epoch 38/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6317 - accuracy: 0.6971\n",
      "Epoch 39/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6310 - accuracy: 0.6970\n",
      "Epoch 40/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6309 - accuracy: 0.6981\n",
      "Epoch 41/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6313 - accuracy: 0.6969\n",
      "Epoch 42/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6298 - accuracy: 0.6979\n",
      "Epoch 43/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6308 - accuracy: 0.6962\n",
      "Epoch 44/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6305 - accuracy: 0.6971\n",
      "Epoch 45/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6305 - accuracy: 0.6970\n",
      "Epoch 46/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6297 - accuracy: 0.6981\n",
      "Epoch 47/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6300 - accuracy: 0.6968\n",
      "Epoch 48/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6300 - accuracy: 0.6978\n",
      "Epoch 49/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6299 - accuracy: 0.6978\n",
      "Epoch 50/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6294 - accuracy: 0.6986\n",
      "Epoch 51/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6296 - accuracy: 0.6983\n",
      "Epoch 52/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6288 - accuracy: 0.6974\n",
      "Epoch 53/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6293 - accuracy: 0.6977\n",
      "Epoch 54/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6295 - accuracy: 0.6968\n",
      "Epoch 55/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6292 - accuracy: 0.6977\n",
      "Epoch 56/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6288 - accuracy: 0.6978\n",
      "Epoch 57/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6286 - accuracy: 0.6977\n",
      "Epoch 58/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6280 - accuracy: 0.6983\n",
      "Epoch 59/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6280 - accuracy: 0.6982\n",
      "Epoch 60/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6282 - accuracy: 0.6991\n",
      "Epoch 61/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6277 - accuracy: 0.6991\n",
      "Epoch 62/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6280 - accuracy: 0.6980\n",
      "Epoch 63/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6282 - accuracy: 0.6989\n",
      "Epoch 64/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6277 - accuracy: 0.6977\n",
      "Epoch 65/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6272 - accuracy: 0.6985\n",
      "Epoch 66/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6270 - accuracy: 0.6979\n",
      "Epoch 67/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6273 - accuracy: 0.6980\n",
      "Epoch 68/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6267 - accuracy: 0.6981\n",
      "Epoch 69/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6278 - accuracy: 0.6983\n",
      "Epoch 70/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6271 - accuracy: 0.6983\n",
      "Epoch 71/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6267 - accuracy: 0.6988\n",
      "Epoch 72/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6274 - accuracy: 0.6984\n",
      "Epoch 73/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6269 - accuracy: 0.6994\n",
      "Epoch 74/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6267 - accuracy: 0.6994\n",
      "Epoch 75/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6264 - accuracy: 0.6984\n",
      "Epoch 76/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6267 - accuracy: 0.6992\n",
      "Epoch 77/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6269 - accuracy: 0.7001\n",
      "Epoch 78/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6261 - accuracy: 0.6997\n",
      "Epoch 79/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6268 - accuracy: 0.6991\n",
      "Epoch 80/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6260 - accuracy: 0.7002\n",
      "Epoch 81/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6264 - accuracy: 0.6992\n",
      "Epoch 82/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6263 - accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6262 - accuracy: 0.6988\n",
      "Epoch 84/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6263 - accuracy: 0.6989\n",
      "Epoch 85/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6256 - accuracy: 0.7009\n",
      "Epoch 86/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6257 - accuracy: 0.6997\n",
      "Epoch 87/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6259 - accuracy: 0.6994\n",
      "Epoch 88/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6264 - accuracy: 0.6999\n",
      "Epoch 89/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6255 - accuracy: 0.6999\n",
      "Epoch 90/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6252 - accuracy: 0.7002\n",
      "Epoch 91/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6256 - accuracy: 0.6993\n",
      "Epoch 92/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6258 - accuracy: 0.6996\n",
      "Epoch 93/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6254 - accuracy: 0.6996\n",
      "Epoch 94/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6258 - accuracy: 0.6989\n",
      "Epoch 95/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6255 - accuracy: 0.7004\n",
      "Epoch 96/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6254 - accuracy: 0.6999\n",
      "Epoch 97/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6254 - accuracy: 0.6997\n",
      "Epoch 98/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6254 - accuracy: 0.6991\n",
      "Epoch 99/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6257 - accuracy: 0.6992\n",
      "Epoch 100/100\n",
      "2383/2383 [==============================] - 3s 1ms/step - loss: 0.6253 - accuracy: 0.6994\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "      \n",
    "with open('files/ch11/games_ttt10K.p', 'rb') as fp:\n",
    "    tttgames=pickle.load(fp)\n",
    "\n",
    "boards = []\n",
    "outcomes = []\n",
    "for game in tttgames:\n",
    "    boards.append(game[1])\n",
    "    outcomes.append(game[0])\n",
    "\n",
    "X = np.array(boards).reshape((-1, 3, 3, 1))\n",
    "# one_hot encoder, three outcomes: -1, 0, and 1\n",
    "y = to_categorical(outcomes, num_classes=3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=128, \n",
    "kernel_size=(3,3),padding=\"same\",activation=\"relu\",input_shape=(3,3,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "  \n",
    "# Train the model for 100 epochs\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "model.save('files/ch11/trained_ttt10K.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f26be1",
   "metadata": {},
   "source": [
    "You can simulate a larger number of games, but it will take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06d23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc541a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14805aac",
   "metadata": {},
   "source": [
    "## 4. Use the Trained Model to Play Games\n",
    "Next, we’ll use the strategy to play a game. \n",
    "\n",
    "The player X will use the best move from the trained model. Player O will randomly select a move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a00ae",
   "metadata": {},
   "source": [
    "### 4.1. Best Move Based on the Trained Deep Neural Network\n",
    "First, we'll define a *best_move()* function for player X. The function takes a board position as its first argument, and a list of possible next moves as its second argument. \n",
    "\n",
    "The function will go over each move hypothetically, and use the trained deep neural network to predict the probability of player X winning the game. The function returns the move with the highest chance of winning.\n",
    "\n",
    "We define a best_move() function for the computer to find best moves. \n",
    "What the computer does is as follows:\n",
    "1.\tLook at the current board.\n",
    "2.\tLook at all possible next moves, and add each move to the current board to form a hypothetical board\n",
    "3.\tUse the pretained model to predict the chance of winning with the hypothetical board\n",
    "4.\tChoose the move that produces the highest chance of winning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2efd4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move(board, valids):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(valids)==1:\n",
    "        return valids[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome = -1;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically to predict outcome\n",
    "    for move in valids:\n",
    "        tooccupy=deepcopy(board).reshape(9,)\n",
    "        tooccupy[int(move)-1]=1\n",
    "        prediction=reload.predict(np.array(tooccupy).reshape(-1, 3,3,1), verbose=0)\n",
    "        win_lose_dif=prediction[0][1]-prediction[0][2]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fe8eb",
   "metadata": {},
   "source": [
    "Now let's use the *best_move()* function to choose moves for player X and play a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21f51838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a move in the form of 1 to 9\n",
      "the current state is state=[0 0 0 0 0 0 0 0 0]\n",
      "Player X has chosen action=5\n",
      "the current state is state=[0 0 0 0 1 0 0 0 0]\n",
      "Player O has chosen action=7\n",
      "the current state is state=[ 0  0  0  0  1  0 -1  0  0]\n",
      "Player X has chosen action=6\n",
      "the current state is state=[ 0  0  0  0  1  1 -1  0  0]\n",
      "Player O has chosen action=1\n",
      "the current state is state=[-1  0  0  0  1  1 -1  0  0]\n",
      "Player X has chosen action=4\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.TicTacToe_env import ttt\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "reload = tf.keras.models.load_model('files/ch11/trained_ttt10K.h5')\n",
    "\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "\n",
    "print(\"enter a move in the form of 1 to 9\")\n",
    "\n",
    "# Play a full game manually\n",
    "while True:\n",
    "    print(f\"the current state is state={state}\") \n",
    "    # Use the best_move() function to select the next move\n",
    "    action = best_move(state, env.validinputs)\n",
    "    print(f\"Player X has chosen action={action}\")    \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    print(f\"the current state is state={new_state}\")    \n",
    "    action = random.choice(env.validinputs)\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    new_new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break\n",
    "    else: \n",
    "        # play next round\n",
    "        state=new_new_state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402a645",
   "metadata": {},
   "source": [
    "The computer player will look at each possible next move, and add that move to the current board to form a hypothetical board, tooccupy. We reshape the hypothetical board into a (3, 3, 1) shape to feed into the model to make predictions. \n",
    "There prediction will have three values: the probability of tying, player 1 winning, and player 2 winning. The computer will choose the move with the highest probability of winning the game. \n",
    "If we assume the computer player plays second, we simply use the third probability instead of the second probability in the prediction. \n",
    "If you play a game with the computer, you’ll find it’s impossible to win.\n",
    "Here is one example of the eventual outcome:\n",
    "\n",
    "\n",
    "Player X uses the best moves recommended by the trained model and wins the game by occupying celss 4, 5, and 6, as shown in this picture.\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_win_screen.png\" /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "071b7952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ece520",
   "metadata": {},
   "source": [
    "### 4.2. Test the Efficacy of the DNN Model\n",
    "Next, we’ll test how often the DNN trained game strategy wins against a player who makes random moves. \n",
    "The following script does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d392cd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of winning games is 984\n",
      "the number of tying games is 8\n",
      "the number of losing games is 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.TicTacToe_env import ttt\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "reload = tf.keras.models.load_model('files/ch11/trained_ttt10K.h5')\n",
    "\n",
    "def best_move(board, valids):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(valids)==1:\n",
    "        return valids[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome = -1;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically to predict outcome\n",
    "    for move in valids:\n",
    "        tooccupy=deepcopy(board).reshape(9,)\n",
    "        tooccupy[int(move)-1]=1\n",
    "        prediction=reload.predict(np.array(tooccupy).reshape(-1, 3,3,1), verbose=0)\n",
    "        win_lose_dif=prediction[0][1]-prediction[0][2]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove\n",
    "\n",
    "\n",
    "env=ttt()\n",
    "\n",
    "def test_one_game():\n",
    "    state=env.reset()   \n",
    "    while True:\n",
    "        # Use the best_move() function to select the next move\n",
    "        action = best_move(state, env.validinputs)   \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "        action = random.choice(env.validinputs)   \n",
    "        new_new_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        else: \n",
    "            # play next round\n",
    "            state=new_new_state\n",
    "    return reward\n",
    "\n",
    "#repeat the game 1000 times and record all game outcomes\n",
    "results=[]        \n",
    "for x in range(1000):\n",
    "    result=test_one_game()\n",
    "    results.append(result)    \n",
    "\n",
    "#print out the number of winning games\n",
    "print(\"the number of winning games is\", results.count(1))\n",
    "\n",
    "#print out the number of tying games\n",
    "print(\"the number of tying games is\", results.count(0))\n",
    "\n",
    "#print out the number of losing games\n",
    "print(\"the number of losing games is\", results.count(-1))                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "The player wins the game with the shortest possible path. So the deep learning game strategy works in the self-made environment as well!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806366f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "## 5. Animate the Deep Learning Process\n",
    "In this section, we'll create an animation to show how the agent makes a decision by getting the best move from the trained deep neural network at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2dd43",
   "metadata": {},
   "source": [
    "### 5.1. Print Out Probabilities of Winning for Each Next Move\n",
    "In each stage of the game, we'll first draw the game board on the left of the screen. The player X will look at all possible next moves and use the trained deep neural network to predict the probability of winning for each hypothetical next move. We'll draw the probabilities on right. Finally, we'll highlight the action with the highest probability of winng. The action is player X's next move. We'll repeat this step by step until the game ends. \n",
    "\n",
    "This animation will let us look under the hood and understand how deep learning can help us design intelligent game strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0df744",
   "metadata": {},
   "source": [
    "The next script will play a full game and record the game board and the winning probabilities in each step of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d85158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is \n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "Player X has chosen action=5\n",
      "the current state is state=[0 0 0 0 1 0 0 0 0]\n",
      "Player O has chosen action=8\n",
      "the current state is \n",
      "[ 0  0  0  0  1  0  0 -1  0]\n",
      "Player X has chosen action=1\n",
      "the current state is state=[ 1  0  0  0  1  0  0 -1  0]\n",
      "Player O has chosen action=4\n",
      "the current state is \n",
      "[ 1  0  0 -1  1  0  0 -1  0]\n",
      "Player X has chosen action=9\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.TicTacToe_env import ttt\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import turtle as t\n",
    "\n",
    "reload = tf.keras.models.load_model('files/ch11/trained_ttt100K.h5')\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()  \n",
    "step=0\n",
    "try:\n",
    "    ts=t.getscreen() \n",
    "except t.Terminator:\n",
    "    ts=t.getscreen()\n",
    "t.hideturtle()\n",
    "env.render()\n",
    "ts.getcanvas().postscript(file=f\"files/ch11/ttt_step{step}.ps\")\n",
    "\n",
    "# Create a list to record game history\n",
    "history=[]\n",
    "\n",
    "def best_move(board, valids):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(valids)==1:\n",
    "        return valids[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome=-1;\n",
    "    bestmove=None  \n",
    "    # record winning probabilities for all hypothetical moves\n",
    "    p_wins={}\n",
    "    #go through all possible moves hypothetically to predict outcome\n",
    "    for move in valids:\n",
    "        tooccupy=deepcopy(board).reshape(9,)\n",
    "        tooccupy[int(move)-1]=1\n",
    "        prediction=reload.predict(np.array(tooccupy).reshape(-1, 3,3,1),verbose=0)\n",
    "        p_win=prediction[0][1]\n",
    "        p_wins[move]=p_win\n",
    "        if p_win>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = p_win\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove, p_wins\n",
    "\n",
    "# Play a full game \n",
    "while True:\n",
    "    print(f\"the current state is \\n{state}\") \n",
    "    bestmove,p_wins=best_move(state, env.validinputs)\n",
    "    action=bestmove\n",
    "    print(f\"Player X has chosen action={action}\")  \n",
    "    old_state=deepcopy(state)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    history.append([old_state,p_wins,action,deepcopy(new_state),done])\n",
    "    env.render()\n",
    "    step += 1      \n",
    "    ts.getcanvas().postscript(file=f\"files/ch11/ttt_step{step}.ps\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    print(f\"the current state is state={new_state}\")    \n",
    "    action = random.choice(env.validinputs)\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    new_new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    step += 1      \n",
    "    ts.getcanvas().postscript(file=f\"files/ch11/ttt_step{step}.ps\")\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break\n",
    "    else: \n",
    "        # play next round\n",
    "        state=new_new_state\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e3732",
   "metadata": {},
   "source": [
    "Before making the first move, the Player X has nine hypothetical next moves: 1 to 9. The trained neural network tells us what's the probabiltiy of winning the game for each hypothetical move. We can print out the nine probabilities by using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431d5f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Player X chooses action 1, the probability of winning is 0.6460.\n",
      "If Player X chooses action 2, the probability of winning is 0.5623.\n",
      "If Player X chooses action 3, the probability of winning is 0.6474.\n",
      "If Player X chooses action 4, the probability of winning is 0.5680.\n",
      "If Player X chooses action 5, the probability of winning is 0.7345.\n",
      "If Player X chooses action 6, the probability of winning is 0.5654.\n",
      "If Player X chooses action 7, the probability of winning is 0.6453.\n",
      "If Player X chooses action 8, the probability of winning is 0.5629.\n",
      "If Player X chooses action 9, the probability of winning is 0.6471.\n"
     ]
    }
   ],
   "source": [
    "p_wins_step0=history[0][1]\n",
    "for key, value in p_wins_step0.items():\n",
    "    print(f\"If Player X chooses action {key}, the probability of winning is {value:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ffe33",
   "metadata": {},
   "source": [
    "The above results show that the probability of Player X winning the game is the highest, at 73.45%, if action 5 is taken. That's why Player X occupies cell 5 in the first move. \n",
    "\n",
    "When making the second move, Player X faces seven choices. We can also print out the probability of winning for each move as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b741f3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If Player X chooses action 1, the probability of winning is 0.8206.\n",
      "If Player X chooses action 2, the probability of winning is 0.6159.\n",
      "If Player X chooses action 3, the probability of winning is 0.7762.\n",
      "If Player X chooses action 4, the probability of winning is 0.7598.\n",
      "If Player X chooses action 6, the probability of winning is 0.7974.\n",
      "If Player X chooses action 7, the probability of winning is 0.7929.\n",
      "If Player X chooses action 9, the probability of winning is 0.7860.\n"
     ]
    }
   ],
   "source": [
    "p_wins_step1=history[1][1]\n",
    "for key, value in p_wins_step1.items():\n",
    "    print(f\"If Player X chooses action {key}, the probability of winning is {value:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb762289",
   "metadata": {},
   "source": [
    "The above results show that the probability of Player X winning the game is the highest, at 82.06%, if action 1 is taken. That's why Player X occupies cell 1 in the second move. \n",
    "\n",
    "You can also print out the the probability of Player X winning the game when making the third move, but I'll leave that for you to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a27e46",
   "metadata": {},
   "source": [
    "Let's save the game history data for later use. Run the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51cbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the game history on your computer\n",
    "with open('files/ch11/ttt_game_history.p','wb') as fp:\n",
    "    pickle.dump(history,fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a0fe6",
   "metadata": {},
   "source": [
    "### 5.2. Animate the Whole Game\n",
    "Next, you'll combine the pictures created in the last subsection into an animation. As a result, you'll see the game board step by step for the whole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4deaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "frames=[]\n",
    "for i in range(step+1):\n",
    "    im = Image.open(f\"files/ch11/ttt_step{i}.ps\")\n",
    "    frame=np.asarray(im)\n",
    "    frames.append(frame) \n",
    "imageio.mimsave(\"files/ch11/ttt_steps.gif\", frames, fps=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea21ada",
   "metadata": {},
   "source": [
    "If you open the file ttt_steps.gif, you'll see the following: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_steps.gif\"/>\n",
    "\n",
    "The animation shows the game board at each stage of the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e9512",
   "metadata": {},
   "source": [
    "### 5.3. Animate the Decision Making\n",
    "Next, we'll animate the decision making process of Player X in each stage of the game. We'll draw the probabilities of Player X winning the game for each hypothetical move. We'll highlight the move with the highest probability of winning the game. We'll animate this step by step until the game ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10fe52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# load up the history data\n",
    "history = pickle.load(open('files/ch11/ttt_game_history.p', 'rb'))\n",
    "# remember the best moves \n",
    "bests = [5, 1, 9]\n",
    "# Generate pictures\n",
    "for stage in range(3):\n",
    "    fig = plt.figure(figsize=(10,10), dpi=200)\n",
    "    ax = fig.add_subplot(111) \n",
    "    ax.set_xlim(0,8)\n",
    "    ax.set_ylim(-4.5, 3.5)\n",
    "    #plt.grid()\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"files/ch11/ttt_stage{stage*2}step1.png\") \n",
    "    xys = [[(4,-4.1),(2,0)],\n",
    "       [(4,-3.2),(2,0)],           \n",
    "       [(4,-2.3),(2,0)],           \n",
    "       [(4,-1.4),(2,0)],\n",
    "       [(4,-0.5),(2,0)],\n",
    "       [(4,0.4),(2,0)],\n",
    "       [(4,1.3),(2,0)],\n",
    "       [(4,2.2),(2,0)],\n",
    "       [(4,3.1),(2,0)]]\n",
    "    for xy in xys:\n",
    "        ax.annotate(\"\",xy=xy[0],xytext=xy[1],\n",
    "        arrowprops=dict(arrowstyle = '->', color = 'g', linewidth = 2))  \n",
    "    # add rectangle to plot\n",
    "    ax.add_patch(Rectangle((0,-0.6), 2, 1.3,\n",
    "                     facecolor = 'b',alpha=0.1))\n",
    "    plt.text(0.2,-0.5,\"Deep\\nNeural\\nNetwork\",fontsize=20)        \n",
    "    for m in range(9):\n",
    "        plt.text(4.1, 3.1-0.9*m, f\"Cell {m+1}, \\\n",
    "        Pr(win)={history[stage][1].get(str(m+1),0):.4f}\", fontsize=20, color=\"r\")  \n",
    "   \n",
    "\n",
    "    plt.savefig(f\"files/ch11/ttt_stage{stage*2}step2.png\") \n",
    "    \n",
    "    # highlight the best action\n",
    "    ax.add_patch(Rectangle((4,3.85-bests[stage]*0.9),\n",
    "                           3.5, 0.5,facecolor = 'b',alpha=0.5))     \n",
    "    plt.savefig(f\"files/ch11/ttt_stage{stage*2}step3.png\")     \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a728554",
   "metadata": {},
   "source": [
    "The above script highlights the decision making proces of Player X. For example, if you open the file ttt_stage4step3.png, you'll see the following picture.\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_stage4step3.png\" /> It shows the probabilities of Player X winning the game with each hypothetical move. In particular, the proability is 100% if Player X chooses Cell 9. The cell is highlighted in blue, and that is also the move made by Player X as a result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f96398",
   "metadata": {},
   "source": [
    "Next, we'll combine the pictures into an animation to show the decision-making process of Player X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd048656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "frames=[]\n",
    "\n",
    "for stage in [0, 2, 4]:\n",
    "    for step in [1,2,3]:\n",
    "        im = Image.open(f\"files/ch11/ttt_stage{stage}step{step}.png\")\n",
    "        f1=np.asarray(im)\n",
    "        frames.append(f1)  \n",
    "imageio.mimsave('files/ch11/ttt_DL_probs.gif', frames, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada070e0",
   "metadata": {},
   "source": [
    "If you open the file ttt_DL_probs.gif, you'll see the animation as follows.\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_DL_probs.gif\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "### 5.4. Animate Game Board Positions and the Decision Making\n",
    "Next, we'll combine the game board positions and the decision making process of Player X in each stage of the game. On the left of the screen, we'll draw the game board. On the right of the screen, we'll draw the probabilities of Player X winning the game for each hypothetical move. We'll animate this step by step until the game ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef028842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlliu2\\AppData\\Local\\Temp\\ipykernel_15976\\3535676410.py:7: UserWarning: Attempting to set identical left == right == -5 results in singular transformations; automatically expanding.\n",
      "  ax.set_xlim(-5,-5)\n",
      "C:\\Users\\hlliu2\\AppData\\Local\\Temp\\ipykernel_15976\\3535676410.py:8: UserWarning: Attempting to set identical bottom == top == -5 results in singular transformations; automatically expanding.\n",
      "  ax.set_ylim(-5,-5)\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    im = Image.open(f\"files/ch11/ttt_step{i}.ps\")\n",
    "    fig, ax=plt.subplots(figsize=(10,10), dpi=200)\n",
    "    newax = fig.add_axes([0,0,1,1])\n",
    "    newax.imshow(im)\n",
    "    newax.axis('off')\n",
    "    ax.set_xlim(-5,-5)\n",
    "    ax.set_ylim(-5,-5)\n",
    "    plt.axis(\"off\")\n",
    "    #plt.grid()\n",
    "    plt.savefig(f\"files/ch11/ttt_step{i}plt.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "frames=[]\n",
    "\n",
    "for stage in [0, 2, 4]:\n",
    "    for step in [1,2,3]:\n",
    "        im = Image.open(f\"files/ch11/ttt_step{stage}plt.png\")\n",
    "        f0=np.asarray(im)\n",
    "        im = Image.open(f\"files/ch11/ttt_stage{stage}step{step}.png\")\n",
    "        f1=np.asarray(im)\n",
    "        fs = np.concatenate([f0,f1],axis=1)\n",
    "        frames.append(fs)\n",
    "        if step==0:\n",
    "            frames.append(fs)            \n",
    "im = Image.open(\"files/ch11/ttt_step5plt.png\")\n",
    "f0=np.asarray(im)\n",
    "im = Image.open(\"files/ch11/ttt_stage4step1.png\")\n",
    "f1=np.asarray(im)\n",
    "fs = np.concatenate([f0,f1],axis=1)\n",
    "frames.append(fs)\n",
    "frames.append(fs)\n",
    "\n",
    "imageio.mimsave('files/ch11/ttt_DL_steps.gif', frames, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd34819",
   "metadata": {},
   "source": [
    "If you open the gif file, you'll see the following animation:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/ttt_DL_steps.gif\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87d61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5ab9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
