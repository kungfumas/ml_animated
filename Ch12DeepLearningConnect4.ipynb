{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 12: Deep Learning Game Strategies: Connect Four\n",
    "\n",
    "In this chapter, you’ll combine what you have learned in Chapters 5 to 10 to design deep learning game strategies for the Connect Four game. You'll first create a game environment for Connect Four with all the features and methods of a typical OpenAI Gym game environment. The game environment also has a graphical interface. \n",
    "\n",
    "You'll use similated games as input data to feed into a deep neural network. After the model is trained, you'll use it to play games. At each step of the game, you'll look at all possible next moves. The model predicts the probability of winning the game with each hypothetical move. You'll pick the move with the highest probability of winning the game.\n",
    "\n",
    "Finally, you'll animate the decision making process. You'll use the deep learning game strategy to play a full game. At each step, the animation will show the game board on the left, and the probability of winning for each next move on the right. The best move will be highlighted, as follows:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/conn_DL_steps.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 12}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 12 in a subfolder /files/ch12. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch12\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. Create the Connect Four Game Environment\n",
    "We'll create a Connect Four game environment, using the ***turtle*** library to draw game boards. We’ll create all the features and methods that a typical OpenAI Gym environment has. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "### 1.1. Use A Python Class to Represent the Environment\n",
    "We’ll create a Python class to represent the Connect Four game environment. The class will have various attributes, variables, and methods to replicate those in a typical OpenAI Gym game environment. \n",
    "\n",
    "#### Attributes\n",
    "Specifically, our self-made Connect Four game environment will have the following attributes:\n",
    " \n",
    "*\taction_space: an attribute that provides the space of all actions that can be taken by the agent. The action space will have seven values, 1 to 7. This represents the 7 columns a player can drop discs in.\n",
    "*\tobservation_space: an attribute that provides the list of all possible states in the environment. We'll use a numpy array with 7 rows and 6 columns to represent the 42 cells on a game board.\n",
    "*\tstate: an attribute indicating which state the agent is currently in. Each of the 42 cells can take values -1 (occupied by player Yellow), 0 (empty), or 1 (occupied by player Red).\n",
    "*\taction: an attribute indicating the action taken by the agent. The action is a number between 1 and 7.\n",
    "*\treward: an attribute indicating the reward to the agent because of the action taken by the agent. The reward is 0 in each step, unless a player has won the game, in which case the winner has a reward of 1 and the loser a reward of -1. \n",
    "*\tdone: an attribute indicating whether the game has ended. This happens when one player wins or if the game is tied.\n",
    "*\tinfo: an attribute that provides information about the game. We'll set it as an empty string \"\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9996e",
   "metadata": {},
   "source": [
    "#### Methods\n",
    "Our self-made Connect Four game environment will have a few methods as well:\n",
    " \n",
    "*\treset() is a method to set the game environment to the initial (that is, the starting) state. All cells on the board will be empty.\n",
    "*\trender() is a method showing the current state of the environment graphically.\n",
    "*\tstep() is a method that returns the new state, the reward, the value of *done* variable, and the varibale *info* based on the action taken by the agent.\n",
    "*\tsample() is a method to randomly choose an action from the action space.\n",
    "*\tclose() is a method to end the game environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "### 1.2. Create A Local Module for the Connect Four Game\n",
    "We'll create a local module for the Connect Four game and place it inside the local package for this book: the package ***utils*** that we have created in Chapter 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "Now let's code in a self-made Connect Four game environment using a Python class. Save the script in the cell below as *Connect4_env.py* in the folder *utils* you created in Chapter 10. Alternatively, you can download it from my GitHub repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b900fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle as t\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define an action_space helper class\n",
    "class action_space:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def sample(self):\n",
    "        num = np.random.choice(range(self.n))\n",
    "        # covert 0-6 to 1-7 to avoid confusion \n",
    "        action = 1+num\n",
    "        return action\n",
    "    \n",
    "# Define an obervation_space helper class    \n",
    "class observation_space:\n",
    "    def __init__(self, row, col):\n",
    "        self.shape = (row, col)\n",
    "\n",
    "class conn():\n",
    "  \n",
    "    def __init__(self): \n",
    "        # use the helper action_space class\n",
    "        self.action_space=action_space(7)\n",
    "        # use the helper observation_space class\n",
    "        self.observation_space=observation_space(7,6)\n",
    "        self.info=\"\"   \n",
    "        # The x-coordinates of the center of the 7 columns\n",
    "        self.xs = [-300,-200,-100,0,100,200,300]\n",
    "        # The y-coordinates of the center of the 6 rows\n",
    "        self.ys = [-250,-150,-50,50,150,250]  \n",
    "        self.showboard=False  \n",
    "        self.game_piece=None \n",
    "            \n",
    "    def reset(self):  \n",
    "        # The X player moves first\n",
    "        self.turn = \"red\"\n",
    "        # Create a list of valid moves\n",
    "        self.validinputs = [1,2,3,4,5,6,7]\n",
    "        # Create a list of lists to track game pieces\n",
    "        self.occupied = [list(),list(),list(),list(),list(),list(),list()]\n",
    "        # Tracking the state\n",
    "        self.state=np.array([[0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0],\n",
    "                            [0,0,0,0,0,0]])\n",
    "        self.done=False\n",
    "        self.reward=0     \n",
    "        return self.state        \n",
    "        \n",
    "    # step() function: place piece on board and update state\n",
    "    def step(self, inp):\n",
    "        # Remember the current game piece\n",
    "        self.game_piece=[inp-1, len(self.occupied[inp-1]), self.turn]        \n",
    "        # update the state: red is 1 and yellow is -1\n",
    "        self.state[inp-1][len(self.occupied[inp-1])]=2*(self.turn==\"red\")-1       \n",
    "        # Add the move to the occupied list \n",
    "        self.occupied[inp-1].append(self.turn)\n",
    "\n",
    "        # Update the list of valid moves\n",
    "        if len(self.occupied[inp-1]) == 6 and inp in self.validinputs:\n",
    "            self.validinputs.remove(inp)  \n",
    "        # check if the player has won the game\n",
    "        if self.win_game(inp) == True:\n",
    "            self.done=True\n",
    "            # reward is 1 if red won; -1 if yellow won\n",
    "            self.reward=2*(self.turn==\"red\")-1\n",
    "            self.validinputs=[]\n",
    "        # If all cellls are occupied and no winner, it's a tie\n",
    "        elif len(self.validinputs) == 0:\n",
    "            self.done=True\n",
    "            self.reward=0\n",
    "        else:\n",
    "            # Give the turn to the other player\n",
    "            if self.turn == \"red\":\n",
    "                self.turn = \"yellow\"\n",
    "            else:\n",
    "                self.turn = \"red\"             \n",
    "        return self.state, self.reward, self.done, self.info\n",
    "                     \n",
    "    # Determine if a player has won the game\n",
    "    # Define a horizontal4() function to check connecting 4 horizontally\n",
    "    def horizontal4(self, x, y):\n",
    "        win = False\n",
    "        for dif in (-3, -2, -1, 0):\n",
    "            try:\n",
    "                if self.occupied[x+dif][y] == self.turn\\\n",
    "                and self.occupied[x+dif+1][y] == self.turn\\\n",
    "                and self.occupied[x+dif+2][y] == self.turn\\\n",
    "                and self.occupied[x+dif+3][y] == self.turn\\\n",
    "                and  x+dif >= 0:\n",
    "                    win = True            \n",
    "            except IndexError:\n",
    "                pass\n",
    "        return win         \n",
    "    # Define a vertical4() function to check connecting 4 vertically\n",
    "    def vertical4(self, x, y):\n",
    "        win = False\n",
    "        try:\n",
    "            if self.occupied[x][y] == self.turn\\\n",
    "            and self.occupied[x][y-1] == self.turn\\\n",
    "            and self.occupied[x][y-2] == self.turn\\\n",
    "            and self.occupied[x][y-3] == self.turn\\\n",
    "            and y-3 >= 0:\n",
    "                win = True     \n",
    "        except IndexError:\n",
    "            pass\n",
    "        return win   \n",
    "    # Define a forward4() function to check connecting 4 diagonally in / shape\n",
    "    def forward4(self, x, y):\n",
    "        win = False\n",
    "        for dif in (-3, -2, -1, 0):\n",
    "            try:\n",
    "                if self.occupied[x+dif][y+dif] == self.turn\\\n",
    "                and self.occupied[x+dif+1][y+dif+1] == self.turn\\\n",
    "                and self.occupied[x+dif+2][y+dif+2] == self.turn\\\n",
    "                and self.occupied[x+dif+3][y+dif+3] == self.turn\\\n",
    "                and x+dif >=  0 and y+dif >= 0:\n",
    "                    win = True            \n",
    "            except IndexError:\n",
    "                pass\n",
    "        return win     \n",
    "    # Define a back4() function to check connecting 4 diagonally in \\ shape\n",
    "    def back4(self, x, y):\n",
    "        win = False\n",
    "        for dif in (-3, -2, -1, 0):\n",
    "            try:\n",
    "                if self.occupied[x+dif][y-dif] == self.turn\\\n",
    "                and self.occupied[x+dif+1][y-dif-1] == self.turn\\\n",
    "                and self.occupied[x+dif+2][y-dif-2] == self.turn\\\n",
    "                and self.occupied[x+dif+3][y-dif-3] == self.turn\\\n",
    "                and x+dif >=  0 and y-dif-3 >= 0:\n",
    "                    win = True            \n",
    "            except IndexError:\n",
    "                pass\n",
    "        return win         \n",
    "    \n",
    "    # Define a win_game() function to check if someone wins the game\n",
    "    def win_game(self, inp):\n",
    "        win = False\n",
    "        x = inp-1\n",
    "        y = len(self.occupied[inp-1])-1\n",
    "        # Check all winning possibilities\n",
    "        if self.vertical4(x,y)==True:\n",
    "            win = True\n",
    "        if self.horizontal4(x,y)==True:\n",
    "            win = True\n",
    "        if self.forward4(x,y)==True:\n",
    "            win = True\n",
    "        if self.back4(x,y)==True:\n",
    "            win = True\n",
    "        return win\n",
    "\n",
    "    def display_board(self):\n",
    "        # Set up the screen\n",
    "        try:\n",
    "            t.setup(730,680, 10, 70)\n",
    "        except:\n",
    "            t.setup(730,680, 10, 70)\n",
    "        t.hideturtle()\n",
    "        t.tracer(False)\n",
    "        t.title(\"Connect Four in Turtle Graphics\")\n",
    "        # Draw frame\n",
    "        t.pensize(5)\n",
    "        t.up()\n",
    "        t.goto(-350,-300)\n",
    "        t.down()\n",
    "        t.begin_fill()\n",
    "        t.color(\"black\", \"blue\")\n",
    "        t.forward(700)\n",
    "        t.left(90)\n",
    "        t.forward(600)\n",
    "        t.left(90)\n",
    "        t.forward(700)\n",
    "        t.left(90)\n",
    "        t.forward(600)\n",
    "        t.left(90)\n",
    "        t.end_fill()\n",
    "        t.up()\n",
    "        # Write column numbers on the board\n",
    "        colnum = 1\n",
    "        for x in range(-300, 350, 100):\n",
    "            t.goto(x,300)\n",
    "            t.write(colnum,font = ('Arial',20,'normal'))\n",
    "            t.goto(x,-330)\n",
    "            t.write(colnum,font = ('Arial',20,'normal'))\n",
    "            colnum +=  1          \n",
    "        # Show white cells\n",
    "        for col in range(7):\n",
    "            for row in range(6):\n",
    "                t.up()\n",
    "                t.goto(self.xs[col],self.ys[row])\n",
    "                t.dot(80,\"white\") \n",
    "        t.update()                  \n",
    "        # Create a second turtle to show disc falling\n",
    "        self.fall = t.Turtle()\n",
    "        self.fall.up()\n",
    "        self.fall.hideturtle()\n",
    "        \n",
    "    def render(self):\n",
    "        if self.showboard==False:\n",
    "            self.display_board()\n",
    "            self.showboard=True\n",
    "\n",
    "        if self.game_piece is not None:\n",
    "            # Show the disc fall from the top\n",
    "            col, row, c = self.game_piece\n",
    "            if row<7:\n",
    "                for i in range(6,row+1,-1):\n",
    "                    self.fall.goto(self.xs[col],self.ys[i-1])\n",
    "                    self.fall.dot(80,c)\n",
    "                    t.update()\n",
    "                    time.sleep(0.05)\n",
    "                    self.fall.clear()\n",
    "            # Go to the cell and place a dot of the player's color\n",
    "            t.up()\n",
    "            t.goto(self.xs[col],self.ys[row])\n",
    "            t.dot(80,c)            \n",
    "            t.update() \n",
    "        \n",
    "    def close(self):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            t.bye()\n",
    "        except t.Terminator:\n",
    "            print('exit turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05110e83",
   "metadata": {},
   "source": [
    "If you run the above cell, nothing will happen. The class simply creates a game environment. We need to initiate the game environment and start playing using Python programs, just as you do with an OpenAI Gym game environment. We'll do that in the next subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "### 1.3. Verify the Custom-Made Game Environment\n",
    "Next, we'll check the attributes and methods of the self-made game environment and make sure it has all the elements that are provided by a typical OpenAI Gym game environment. \n",
    "\n",
    "First we'll initiate the game environment and show the game board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a847d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Connect4_env import conn\n",
    "\n",
    "env = conn()\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6bd0a",
   "metadata": {},
   "source": [
    "You should see a separate turtle window, with a game board as follows: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/conn_start.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977029b",
   "metadata": {},
   "source": [
    "If you want to close the game board window, use the *close()* method, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "439f093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "Next, we'll check the attributes of the environment such as the observation space and action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of possible actions are 7\n",
      "the following are ten sample actions\n",
      "7\n",
      "1\n",
      "4\n",
      "1\n",
      "7\n",
      "2\n",
      "4\n",
      "6\n",
      "4\n",
      "6\n",
      "the shape of the observation space is (7, 6)\n"
     ]
    }
   ],
   "source": [
    "env=conn()\n",
    "# check the action space\n",
    "number_actions = env.action_space.n\n",
    "print(\"the number of possible actions are\", number_actions)\n",
    "# sample the action space ten times\n",
    "print(\"the following are ten sample actions\")\n",
    "for i in range(10):\n",
    "   print(env.action_space.sample())\n",
    "# check the shape of the observation space\n",
    "print(\"the shape of the observation space is\", env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The meanings of the actions in this game are as follows\n",
    "* 1: Placing a game piece in column 1\n",
    "* 2: Placing a game piece in column 2\n",
    "* ...\n",
    "* 7: Placing a game piece in column 7\n",
    "\n",
    "\n",
    "The state space is a matrix with 7 columns and 6 rows: \n",
    "* 0 means the cell is empty; \n",
    "* -1 means the cell is occupied by the yellow player; \n",
    "* 1 means the cell is occupied by the red player."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f879a",
   "metadata": {},
   "source": [
    "## 2. Play Games in the Connect Four Environment\n",
    "Next, we'll play games in the custom-made environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83ac2f",
   "metadata": {},
   "source": [
    "### 2.1. Play a full game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5af973",
   "metadata": {},
   "source": [
    "Here we'll play a full game, by randomly choosing an action from the action space each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from utils.Connect4_env import conn\n",
    "\n",
    "# Initiate the game environment\n",
    "env = conn()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "# Play a full game manually\n",
    "while True:\n",
    "    print(f\"the current state is \\n{np.array(state).T[::-1]}\")    \n",
    "    action = random.choice(env.validinputs)\n",
    "    time.sleep(1)\n",
    "    print(f\"Player red has chosen action={action}\")    \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"the current state is \\n{np.array(new_state).T[::-1]}\")\n",
    "        if reward==1:\n",
    "            print(f\"Player red has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    print(f\"the current state is \\n{np.array(new_state).T[::-1]}\")    \n",
    "    action = random.choice(env.validinputs)\n",
    "    time.sleep(1)\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    new_new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"the current state is \\n{np.array(new_new_state).T[::-1]}\")\n",
    "        if reward==-1:\n",
    "            print(f\"Player yellow has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    else: \n",
    "        # play next round\n",
    "        state=new_new_state\n",
    "    \n",
    "env.close()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "Note that the outcome is different each time you run it because the actions are randomly chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064889bb",
   "metadata": {},
   "source": [
    "### 2.2. Play the Game Manually\n",
    "Next, you’ll learn how to manually interact with the Connect Four game. You'll use the key board to enter a number between 1 and 7. The following lines of code show you how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aac17214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a number between 1 and 7\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "Player red, what's your move?4\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "Player yellow has chosen action=5\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0]]\n",
      "Player red, what's your move?4\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1 -1  0  0]]\n",
      "Player yellow has chosen action=7\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1 -1  0 -1]]\n",
      "Player red, what's your move?4\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1 -1  0 -1]]\n",
      "Player yellow has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1 -1  0 -1]]\n",
      "Player red, what's your move?4\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1 -1  0 -1]]\n",
      "Player red has won!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from utils.Connect4_env import conn\n",
    "\n",
    "# Initiate the game environment\n",
    "env = conn()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "print('enter a number between 1 and 7')\n",
    "# Play a full game manually\n",
    "while True:\n",
    "    print(f\"the current state is \\n{np.array(state).T[::-1]}\")    \n",
    "    action = int(input(\"Player red, what's your move?\"))\n",
    "    time.sleep(1)\n",
    "    print(f\"Player red has chosen action={action}\")    \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"the current state is \\n{np.array(new_state).T[::-1]}\")\n",
    "        if reward==1:\n",
    "            print(f\"Player red has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    print(f\"the current state is \\n{np.array(new_state).T[::-1]}\")    \n",
    "    action = random.choice(env.validinputs)\n",
    "    time.sleep(1)\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    new_new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"the current state is \\n{np.array(new_new_state).T[::-1]}\")\n",
    "        if reward==-1:\n",
    "            print(f\"Player yellow has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    else: \n",
    "        # play next round\n",
    "        state=new_new_state\n",
    "    \n",
    "env.close()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604c977",
   "metadata": {},
   "source": [
    "I am the red player, and I have won by connecting four pieces vertically in column 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "## 3. Train the Deep Learning Game Stratey\n",
    "In this section, you’ll learn how to use a deep neural network to train intelligent game strategies for Connect Four. In particular, you’ll use the convolutional neural network that you used in image classification to train the game strategy. By treating the game board as a two-dimensional graph instead of a one-dimensional vector, you’ll greatly improve the intelligence of your game strategies.\n",
    "\n",
    "You’ll learn how to prepare data to train the model; how to interpret the predictions from the model; how to use the prediction to play games; and how to check the efficacy of your strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e0e1d",
   "metadata": {},
   "source": [
    "### 3.1. A Summary of the Deep Learning Game Strategy\n",
    "Here is a summary of what we’ll do to train the game strategy:\n",
    "\n",
    "1.\tWe’ll let two computer players automatically play a game with random moves, and record the whole game history. The game history will contain all the game board positions from the very first move to the very last move.\n",
    "2.\tWe then associate each board position with a game outcome (win, tie, or lose). The game board position is similar to features X in our image classification problem, and the outcome is similar to labels y in our classification problem.\n",
    "3.\tWe’ll simulate 100,000 games. By using the histories of the games and the corresponding outcomes as Xs and ys, we feed the data into a Deep Neural Networks model. After the training is done, we have a trained model.\n",
    "4.\tWe can now use the trained model to play a game. At each move of the game, we look at all possible next moves, and feed the hypothetical game board into the pretained model. The model will tell you the probabilities of win, lose, and tie.\n",
    "5.\tYou select the move that the model predicts with the highest chance of winning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c150fb",
   "metadata": {},
   "source": [
    "### 3.2. Simulate Games\n",
    "You’ll learn how to generate data to train the DNN. The logic is as follows: you’ll generate 100,000 games in which both players use random moves. You’ll then record the board positions of all intermediate steps and the eventual outcomes of each board position (win, lose, or tie). \n",
    "\n",
    "First, let's simulate one game. The code in the cell below accomplishes that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b436040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Connect4_env import conn\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "\n",
    "# Define the one_game() function\n",
    "def one_game():\n",
    "    history = []\n",
    "    state=env.reset()   \n",
    "    while True:   \n",
    "        action = random.choice(env.validinputs)  \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        history.append(np.array(new_state).reshape(7,6))\n",
    "        if done:\n",
    "            break\n",
    "    return history, reward\n",
    "\n",
    "# Simulate one game and print out results\n",
    "history, outcome = one_game()\n",
    "pprint(history)\n",
    "pprint(outcome)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc4aac",
   "metadata": {},
   "source": [
    "Note here we have converted the game board to a 7 by 6 array so it's easy for you to see the positions of the game pieces. \n",
    "\n",
    "Now let's simulate 100,000 games and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c123bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the game 100000 times and record all games\n",
    "results = []        \n",
    "for x in range(100000):\n",
    "    history, outcome = one_game()\n",
    "    # Note here I associate each board with the game outcome\n",
    "    for board in history:\n",
    "        results.append((outcome, board))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab485c",
   "metadata": {},
   "source": [
    "Now let's save the data on your computer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3e9e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,\n",
      "  array([[0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [-1, -1,  1,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  1, -1,  0,  0,  0],\n",
      "       [-1, -1,  1,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]]))]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the simulation data on your computer\n",
    "with open('files/ch12/games_conn100K.p', 'wb') as fp:\n",
    "    pickle.dump(results,fp)\n",
    "# read the data and print out the first 10 observations       \n",
    "with open('files/ch12/games_conn100K.p', 'rb') as fp:\n",
    "    games = pickle.load(fp)\n",
    "pprint(games[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fb6b9",
   "metadata": {},
   "source": [
    "Each observation has two values. The first is the outcome, in the form of -1, 0, or 1. The second is the game board position as a 7 by 6 numpy array. The data seem to have been stored correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e344b",
   "metadata": {},
   "source": [
    "We now have the data we need. You’ll learn how to train the model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a0ec8",
   "metadata": {},
   "source": [
    "### 3.3. Train Your Connect Four Game Strategy Using A Deep Neural Network\n",
    "The following neural network trains the game strategy using the data you just created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "      \n",
    "with open('files/ch12/games_conn100K.p', 'rb') as fp:\n",
    "    games=pickle.load(fp)\n",
    "\n",
    "boards = []\n",
    "outcomes = []\n",
    "for game in games:\n",
    "    boards.append(game[1])\n",
    "    outcomes.append(game[0])\n",
    "\n",
    "X = np.array(boards).reshape((-1, 7, 6, 1))\n",
    "# one_hot encoder, three outcomes: -1, 0, and 1\n",
    "y = to_categorical(outcomes, num_classes=3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=128, kernel_size=(4, 4),padding=\"same\", \n",
    "                 activation=\"relu\", input_shape=(7,6,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 100 epochs\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "model.save('files/ch12/trained_conn100K.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac255acf",
   "metadata": {},
   "source": [
    "The model is now trained. Let's test how good it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14805aac",
   "metadata": {},
   "source": [
    "## 4. Use the Trained Model to Play Games\n",
    "Next, we’ll use the trained model to play a game. \n",
    "\n",
    "The red player will use the best move from the trained model. The yellow player will randomly select a move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b277b0",
   "metadata": {},
   "source": [
    "### 4.1. The Best Move Based on the Trained Deep Neural Network\n",
    "First, we'll define a *best_move()* function for player X. The function takes a board position as its first argument, and a list of possible next moves as its second argument. We also need the list *occupied* to calculate which row the falling piece will land.\n",
    "\n",
    "The function will go over each move hypothetically, and use the trained deep neural network to predict the probability of the red player winning the game. The function returns the move with the highest chance of winning.\n",
    "\n",
    "We define a best_move() function for the computer to find best moves. \n",
    "What the computer does is as follows:\n",
    "1.\tLook at the current board.\n",
    "2.\tLook at all possible next moves, and add each move to the current board to form a hypothetical board\n",
    "3.\tUse the pretained model to predict the chance of winning with the hypothetical board\n",
    "4.\tChoose the move that produces the highest chance of winning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe399e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move(board, valids, occupied):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(valids)==1:\n",
    "        return valids[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome = -2;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically to predict outcome\n",
    "    for col in valids:\n",
    "        tooccupy=deepcopy(board)\n",
    "        row = 1+len(occupied[col-1])\n",
    "        tooccupy[col-1][row-1]=1\n",
    "        prediction=reload.predict(np.array(tooccupy).reshape((-1, 7, 6, 1)),verbose=0)\n",
    "        p_win=prediction[0][1]\n",
    "        if p_win>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = p_win\n",
    "            # Update the best move\n",
    "            bestmove = col\n",
    "    return bestmove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b5a45",
   "metadata": {},
   "source": [
    "Now let's use the *best_move()* function to choose moves for the red player and play a game. The yellow player picks random moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1675dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a move in the form of 1 to 7\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "Player yellow has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "Player yellow has chosen action=5\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1 -1  0  0]]\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1 -1  0  0]]\n",
      "Player yellow has chosen action=6\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1 -1 -1  0]]\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1 -1 -1  0]]\n",
      "Player red has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.Connect4_env import conn\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# You can either use your own trained model, or the one I provided in GitHub\n",
    "#reload = tf.keras.models.load_model('files/ch12/trained_conn100K.h5')\n",
    "reload = tf.keras.models.load_model('files/ch12/trained_conn_model_padding.h5')\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset()   \n",
    "env.render()\n",
    "\n",
    "print(\"enter a move in the form of 1 to 7\")\n",
    "\n",
    "# Play a full game manually\n",
    "while True:\n",
    "    print(f\"the current state is \\n{np.array(state).T[::-1]}\") \n",
    "    # Use the best_move() function to select the next move\n",
    "    action = best_move(state, env.validinputs, env.occupied)\n",
    "    print(f\"Player red has chosen action={action}\")    \n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"the current state is \\n{np.array(new_state).T[::-1]}\") \n",
    "        if reward==1:\n",
    "            print(f\"Player red has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    print(f\"the current state is \\n{np.array(new_state).T[::-1]}\")    \n",
    "    action = random.choice(env.validinputs)\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    new_new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(f\"the current state is \\n{np.array(new_new_state).T[::-1]}\") \n",
    "        if reward==-1:\n",
    "            print(f\"Player yellow has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    else: \n",
    "        # play next round\n",
    "        state=new_new_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04d76fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c267a",
   "metadata": {},
   "source": [
    "The computer player will look at each possible next move, and add that move to the current board to form a hypothetical board. We feed the hypothetical board into the model to make predictions. The prediction will have three values: the probability of tying, player 1 winning, and player 2 winning. The computer will choose the move with the highest probability of the red player winning the game. \n",
    "\n",
    "Here is one example of the eventual outcome:\n",
    "\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/conn_win.png\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe35955",
   "metadata": {},
   "source": [
    "### 4.2. Test the Efficacy of the DNN Model\n",
    "Next, we’ll test how often the DNN trained game strategy wins against a player who makes random moves. \n",
    "The following script does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af89be3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of winning games is 1000\n",
      "the number of tying games is 0\n",
      "the number of losing games is 0\n"
     ]
    }
   ],
   "source": [
    "from utils.Connect4_env import conn\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#reload = tf.keras.models.load_model('files/ch12/trained_conn10K.h5')\n",
    "reload = tf.keras.models.load_model('files/ch12/trained_conn_model_padding.h5')\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "\n",
    "def test_one_conn():\n",
    "    state=env.reset()   \n",
    "    while True:\n",
    "        action = best_move(state, env.validinputs, env.occupied)  \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "        action = random.choice(env.validinputs)\n",
    "        new_new_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "        else: \n",
    "            # play next round\n",
    "            state=new_new_state\n",
    "    return reward    \n",
    "\n",
    "#repeat the game 1000 times and record all game outcomes\n",
    "results=[]        \n",
    "for x in range(1000):\n",
    "    result=test_one_conn()\n",
    "    results.append(result)    \n",
    "\n",
    "#print out the number of winning games\n",
    "print(\"the number of winning games is\", results.count(1))\n",
    "\n",
    "#print out the number of tying games\n",
    "print(\"the number of tying games is\", results.count(0))\n",
    "\n",
    "#print out the number of losing games\n",
    "print(\"the number of losing games is\", results.count(-1))                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "The trained model has won all 1000 games. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "## 5. Animate the Deep Learning Process\n",
    "In this section, we'll create an animation to show how the agent makes a decision by getting the best move from the trained deep neural network at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2dd43",
   "metadata": {},
   "source": [
    "### 5.1. Print Out Probabilities of Winning for Each Next Move\n",
    "In each stage of the game, we'll first draw the game board on the left of the screen. The red player will look at all possible next moves and use the trained deep neural network to predict the probability of winning for each hypothetical next move. We'll draw the probabilities on the right. Finally, we'll highlight the action with the highest probability of winng. The action is red player's next move. We'll repeat this step by step until the game ends. \n",
    "\n",
    "This animation will let us look under the hood and understand how deep learning can help us design intelligent game strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0df744",
   "metadata": {},
   "source": [
    "The next script will play a full game and record the game board and the winning probabilities in each step of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d85158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "Player yellow has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "Player yellow has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "Player yellow has chosen action=1\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]\n",
      " [-1  0 -1  1  0  0  0]]\n",
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]\n",
      " [-1  0 -1  1  0  0  0]]\n",
      "Player red has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.Connect4_env import conn\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import turtle as t\n",
    "\n",
    "reload = tf.keras.models.load_model('files/ch12/trained_conn_model_padding.h5')\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset()  \n",
    "step=0\n",
    "try:\n",
    "    ts=t.getscreen() \n",
    "except t.Terminator:\n",
    "    ts=t.getscreen()\n",
    "t.hideturtle()\n",
    "env.render()\n",
    "ts.getcanvas().postscript(file=f\"files/ch12/conn_step{step}.ps\")\n",
    "\n",
    "# Create a list to record game history\n",
    "history=[]\n",
    "\n",
    "def best_move(board, valids, occupied):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(valids)==1:\n",
    "        return valids[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome = -2;\n",
    "    bestmove=None \n",
    "    # record winning probabilities for all hypothetical moves\n",
    "    p_wins={}    \n",
    "    #go through all possible moves hypothetically to predict outcome\n",
    "    for col in valids:\n",
    "        tooccupy=deepcopy(board)\n",
    "        row = 1+len(occupied[col-1])\n",
    "        tooccupy[col-1][row-1]=1\n",
    "        prediction=reload.predict(np.array(tooccupy).reshape((-1, 7, 6, 1)),verbose=0)\n",
    "        p_win=prediction[0][1]\n",
    "        p_wins[col]=p_win\n",
    "        if p_win>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = p_win\n",
    "            # Update the best move\n",
    "            bestmove = col\n",
    "    return bestmove, p_wins\n",
    "\n",
    "# Play a full game \n",
    "while True:\n",
    "    print(f\"the current state is \\n{np.array(state).T[::-1]}\")  \n",
    "    bestmove,p_wins=best_move(state, env.validinputs, env.occupied)\n",
    "    action=bestmove\n",
    "    print(f\"Player red has chosen action={action}\")  \n",
    "    old_state=deepcopy(state)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    history.append([old_state,p_wins,action,deepcopy(new_state),done])\n",
    "    env.render()\n",
    "    step += 1      \n",
    "    ts.getcanvas().postscript(file=f\"files/ch12/conn_step{step}.ps\")\n",
    "    if done:\n",
    "        print(f\"the current state is \\n{np.array(new_state).T[::-1]}\") \n",
    "        if reward==1:\n",
    "            print(f\"Player red has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    print(f\"the current state is \\n{np.array(new_state).T[::-1]}\")   \n",
    "    action = random.choice(env.validinputs)\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    new_new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    step += 1      \n",
    "    ts.getcanvas().postscript(file=f\"files/ch12/conn_step{step}.ps\")\n",
    "    if done:\n",
    "        print(f\"the current state is \\n{np.array(new_new_state).T[::-1]}\") \n",
    "        if reward==-1:\n",
    "            print(f\"Player yellow has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break\n",
    "    else: \n",
    "        # play next round\n",
    "        state=new_new_state\n",
    "env.close()    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3a48561",
   "metadata": {},
   "source": [
    "Before making the first move, the red player has seven hypothetical next moves: 1 to 7. The trained neural network tells us what's the probabiltiy of winning the game for each hypothetical move. We can print out the seven probabilities by using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431d5f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the red player chooses action 1, the probability of winning is 0.3281.\n",
      "If the red player chooses action 2, the probability of winning is 0.3718.\n",
      "If the red player chooses action 3, the probability of winning is 0.4599.\n",
      "If the red player chooses action 4, the probability of winning is 0.4793.\n",
      "If the red player chooses action 5, the probability of winning is 0.4599.\n",
      "If the red player chooses action 6, the probability of winning is 0.4495.\n",
      "If the red player chooses action 7, the probability of winning is 0.4096.\n"
     ]
    }
   ],
   "source": [
    "p_wins_step0=history[0][1]\n",
    "for key, value in p_wins_step0.items():\n",
    "    print(f\"If the red player chooses action {key}, the probability of winning is {value:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ffe33",
   "metadata": {},
   "source": [
    "The above results show that the probability of the red player winning the game is the highest, at 47.93%, if action 4 is taken. That's why the red player chooses column 4 in the first move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb762289",
   "metadata": {},
   "source": [
    "You can also print out the the probability of the red player winning the game in the next rounds, but I'll leave that for you to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a27e46",
   "metadata": {},
   "source": [
    "Let's save the game history data for later use. Run the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51cbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the game history on your computer\n",
    "with open('files/ch12/conn_game_history.p','wb') as fp:\n",
    "    pickle.dump((history, step),fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a0fe6",
   "metadata": {},
   "source": [
    "### 5.2. Animate the Whole Game\n",
    "Next, you'll combine the pictures created in the last subsection into an animation. As a result, you'll see the game board step by step for the whole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4deaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "frames=[]\n",
    "for i in range(step+1):\n",
    "    im = Image.open(f\"files/ch12/conn_step{i}.ps\")\n",
    "    frame=np.asarray(im)\n",
    "    frames.append(frame) \n",
    "imageio.mimsave(\"files/ch12/conn_steps.gif\", frames, fps=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea21ada",
   "metadata": {},
   "source": [
    "If you open the file conn_steps.gif, you'll see the following: \n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/conn_steps.gif\"/>\n",
    "\n",
    "The animation shows the game board at each stage of the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e9512",
   "metadata": {},
   "source": [
    "### 5.3. Animate the Decision Making\n",
    "Next, we'll animate the decision making process of the red player in each stage of the game. We'll draw the probabilities of the red player winning the game for each hypothetical move. We'll highlight the move with the highest probability of winning the game. We'll animate this step by step until the game ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2df61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# load up the history data\n",
    "history, num_step = pickle.load(open('files/ch12/conn_game_history.p', 'rb'))\n",
    "# remember the best moves \n",
    "bests = []\n",
    "for item in history:\n",
    "    bests.append(item[2])\n",
    "# Generate pictures\n",
    "for stage in range(len(history)):\n",
    "    fig = plt.figure(figsize=(10,6), dpi=200)\n",
    "    ax = fig.add_subplot(111) \n",
    "    ax.set_xlim(0,10)\n",
    "    ax.set_ylim(-2.5, 3.5)\n",
    "    #plt.grid()\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"files/ch12/conn_stage{stage*2}step1.png\") \n",
    "    xys = [[(4,-2.3),(2,0)],           \n",
    "       [(4,-1.4),(2,0)],\n",
    "       [(4,-0.5),(2,0)],\n",
    "       [(4,0.4),(2,0)],\n",
    "       [(4,1.3),(2,0)],\n",
    "       [(4,2.2),(2,0)],\n",
    "       [(4,3.1),(2,0)]]\n",
    "    for xy in xys:\n",
    "        ax.annotate(\"\",xy=xy[0],xytext=xy[1],\n",
    "        arrowprops=dict(arrowstyle = '->', color = 'g', linewidth = 2))  \n",
    "    # add rectangle to plot\n",
    "    ax.add_patch(Rectangle((0,-0.6), 2, 1.3,\n",
    "                     facecolor = 'b',alpha=0.1))\n",
    "    plt.text(0.2,-0.5,\"Deep\\nNeural\\nNetwork\",fontsize=20)        \n",
    "    for m in range(7):\n",
    "        plt.text(4.1, 3.1-0.9*m, f\"Column {m+1}, \\\n",
    "        Pr(win)={history[stage][1].get(m+1,0):.4f}\", fontsize=20, color=\"r\")  \n",
    "   \n",
    "\n",
    "    plt.savefig(f\"files/ch12/conn_stage{stage*2}step2.png\") \n",
    "    \n",
    "    # highlight the best action\n",
    "    ax.add_patch(Rectangle((4,3.85-bests[stage]*0.9),\n",
    "                           6, 0.5,facecolor = 'b',alpha=0.5))     \n",
    "    plt.savefig(f\"files/ch12/conn_stage{stage*2}step3.png\")     \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a728554",
   "metadata": {},
   "source": [
    "The above script highlights the decision making proces of red player. For example, if you open the file conn_stage0step3.png, you'll see the following picture.\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/conn_stage0step3.png\" /> It shows the probabilities of the red player winning the game with each hypothetical move. In particular, the proability is 47.93% if the red player chooses Column 4. The choice is highlighted in blue, and that is also the move made by the red player as a result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f96398",
   "metadata": {},
   "source": [
    "Next, we'll combine the pictures into an animation to show the decision-making process of the red player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd048656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "frames=[]\n",
    "\n",
    "for stage in range(len(history)):\n",
    "    for step in [1,2,3]:\n",
    "        im = Image.open(f\"files/ch12/conn_stage{stage*2}step{step}.png\")\n",
    "        f1=np.asarray(im)\n",
    "        frames.append(f1)  \n",
    "imageio.mimsave('files/ch12/conn_DL_probs.gif', frames, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada070e0",
   "metadata": {},
   "source": [
    "If you open the file conn_DL_probs.gif, you'll see the animation as follows.\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/conn_DL_probs.gif\" /> Note that your results will likely be different from mine due to the random nature of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "### 5.4. Animate Game Board Positions and the Decision Making\n",
    "Next, we'll combine the game board positions and the decision making process of the red player in each stage of the game. On the left of the screen, we'll draw the game board. On the right of the screen, we'll draw the probabilities of red player winning the game for each hypothetical move. We'll animate this step by step until the game ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ce2579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hlliu2\\AppData\\Local\\Temp\\ipykernel_20284\\1315791436.py:7: UserWarning: Attempting to set identical left == right == -3 results in singular transformations; automatically expanding.\n",
      "  ax.set_xlim(-3,-3)\n",
      "C:\\Users\\hlliu2\\AppData\\Local\\Temp\\ipykernel_20284\\1315791436.py:8: UserWarning: Attempting to set identical bottom == top == -3 results in singular transformations; automatically expanding.\n",
      "  ax.set_ylim(-3,-3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_step+1):\n",
    "    im = Image.open(f\"files/ch12/conn_step{i}.ps\")\n",
    "    fig, ax=plt.subplots(figsize=(6,6), dpi=200)\n",
    "    newax = fig.add_axes([0,0,1,1])\n",
    "    newax.imshow(im)\n",
    "    newax.axis('off')\n",
    "    ax.set_xlim(-3,-3)\n",
    "    ax.set_ylim(-3,-3)\n",
    "    plt.axis(\"off\")\n",
    "    #plt.grid()\n",
    "    plt.savefig(f\"files/ch12/conn_step{i}plt.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "frames=[]\n",
    "\n",
    "for stage in range(len(history)):\n",
    "    for step in [1,2,3]:\n",
    "        im = Image.open(f\"files/ch12/conn_step{stage*2}plt.png\")\n",
    "        f0=np.asarray(im)\n",
    "        im = Image.open(f\"files/ch12/conn_stage{stage*2}step{step}.png\")\n",
    "        f1=np.asarray(im)\n",
    "        fs = np.concatenate([f0,f1],axis=1)\n",
    "        frames.append(fs)\n",
    "        if step==0:\n",
    "            frames.append(fs)            \n",
    "im = Image.open(f\"files/ch12/conn_step{num_step}plt.png\")\n",
    "f0=np.asarray(im)\n",
    "im = Image.open(\"files/ch12/conn_stage0step1.png\")\n",
    "f1=np.asarray(im)\n",
    "fs = np.concatenate([f0,f1],axis=1)\n",
    "frames.append(fs)\n",
    "frames.append(fs)\n",
    "\n",
    "imageio.mimsave('files/ch12/conn_DL_steps.gif', frames, fps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd34819",
   "metadata": {},
   "source": [
    "If you open the gif file, you'll see the following animation:\n",
    "<img src=\"https://gattonweb.uky.edu/faculty/lium/ml/conn_DL_steps.gif\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
